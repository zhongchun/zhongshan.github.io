<!DOCTYPE HTML>
<html lang="zh-CN">
<head>
    

<head><meta name="generator" content="Hexo 3.8.0">
    <meta charset="utf-8">
    <meta name="keywords" content="Notes-深度学习入门之反向传播算法, Machine Learning, Big Data">
    <meta name="description" content="上一章中，我们介绍了神经网络的学习，并通过数值微分计算了神经网络的权重参数的梯度（严格来说，是损失函数关于权重参数的梯度）。数值微分虽然简单，也容易实现，但缺点是计算上比较费时间。本章我们将学习一个能够高效计算权重参数的梯度的方法——误差反">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <title>Notes-深度学习入门之反向传播算法 | BerMaker</title>
    <link rel="icon" type="image/png" href="/favicon.png">

    <link rel="stylesheet" type="text/css" href="/libs/awesome/css/font-awesome.min.css">
    <link rel="stylesheet" type="text/css" href="/libs/materialize/css/materialize.min.css">
    <link rel="stylesheet" type="text/css" href="/libs/aos/aos.css">
    <link rel="stylesheet" type="text/css" href="/libs/animate/animate.min.css">
    <link rel="stylesheet" type="text/css" href="/libs/lightGallery/css/lightgallery.min.css">
    <link rel="stylesheet" type="text/css" href="/css/matery.css">
    <link rel="stylesheet" type="text/css" href="/css/my.css">

    <script src="/libs/jquery/jquery-2.2.0.min.js"></script>
<link rel="stylesheet" href="/css/prism-tomorrow.css" type="text/css"></head>

</head>

<body>

<header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="container">
            <div class="nav-wrapper">
                <div class="brand-logo">
                    <a href="/" class="waves-effect waves-light">
                        
                        <img src="/medias/logo.png" class="logo-img hide-on-small-only">
                        
                        <span class="logo-span">BerMaker</span>
                    </a>
                </div>
                

<a href="#" data-activates="mobile-nav" class="button-collapse"><i class="fa fa-navicon"></i></a>
<ul class="right">
    
    <li class="hide-on-med-and-down">
        <a href="/" class="waves-effect waves-light">
            
            <i class="fa fa-home"></i>
            
            <span>首页</span>
        </a>
    </li>
    
    <li class="hide-on-med-and-down">
        <a href="/tags" class="waves-effect waves-light">
            
            <i class="fa fa-tags"></i>
            
            <span>标签</span>
        </a>
    </li>
    
    <li class="hide-on-med-and-down">
        <a href="/categories" class="waves-effect waves-light">
            
            <i class="fa fa-bookmark"></i>
            
            <span>分类</span>
        </a>
    </li>
    
    <li class="hide-on-med-and-down">
        <a href="/archives" class="waves-effect waves-light">
            
            <i class="fa fa-archive"></i>
            
            <span>归档</span>
        </a>
    </li>
    
    <li class="hide-on-med-and-down">
        <a href="/about" class="waves-effect waves-light">
            
            <i class="fa fa-user-circle-o"></i>
            
            <span>关于</span>
        </a>
    </li>
    
    <li class="hide-on-med-and-down">
        <a href="/friends" class="waves-effect waves-light">
            
            <i class="fa fa-address-book"></i>
            
            <span>友情链接</span>
        </a>
    </li>
    
    <li>
        <a id="toggleSearch" class="waves-effect waves-light">
            <i id="searchIcon" class="mdi-action-search" title="搜索"></i>
        </a>
    </li>

</ul>

<div class="side-nav" id="mobile-nav">

    <div class="mobile-head bg-color">
        
        <img src="/medias/logo.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">BerMaker</div>
        <div class="logo-desc">
            
            If not now, when? If not you, who?
            
        </div>
    </div>

    

    <ul class="menu-list mobile-menu-list">
        
        <li>
            <a href="/" class="waves-effect waves-light">
                
                <i class="fa fa-fw fa-home"></i>
                
                首页
            </a>
        </li>
        
        <li>
            <a href="/tags" class="waves-effect waves-light">
                
                <i class="fa fa-fw fa-tags"></i>
                
                标签
            </a>
        </li>
        
        <li>
            <a href="/categories" class="waves-effect waves-light">
                
                <i class="fa fa-fw fa-bookmark"></i>
                
                分类
            </a>
        </li>
        
        <li>
            <a href="/archives" class="waves-effect waves-light">
                
                <i class="fa fa-fw fa-archive"></i>
                
                归档
            </a>
        </li>
        
        <li>
            <a href="/about" class="waves-effect waves-light">
                
                <i class="fa fa-fw fa-user-circle-o"></i>
                
                关于
            </a>
        </li>
        
        <li>
            <a href="/friends" class="waves-effect waves-light">
                
                <i class="fa fa-fw fa-address-book"></i>
                
                友情链接
            </a>
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/zhongchun" class="waves-effect waves-light" target="_blank">
                <i class="fa fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>

    <div class="social-link">
    <a href="https://github.com/zhongchun" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fa fa-github"></i>
    </a>



    <a href="mailto:495571751@qq.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fa fa-envelope-open"></i>
    </a>



    <a href="/atom.xml" class="tooltipped" target="_blank" data-tooltip="RSS 订阅" data-position="top" data-delay="50">
        <i class="fa fa-rss"></i>
    </a>

</div>
</div>

            </div>
        </div>

        
        <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/zhongchun" class="github-corner tooltipped hide-on-med-and-down" target="_blank" data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewbox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"/>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"/>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"/>
    </svg>
</a>
        
    </nav>
</header>





<div class="bg-cover post-cover" style="background-image: url('/images/bp.png')">
    <div class="container">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <div class="description center-align post-title">
                        Notes-深度学习入门之反向传播算法
                    </div>
                </div>
            </div>
        </div>
    </div>
</div>



<main class="post-container content">

    
    <link rel="stylesheet" href="/libs/tocbot/tocbot.css">
<style>
    #articleContent h1,
    #articleContent h2,
    #articleContent h3,
    #articleContent h4,
    #articleContent h5,
    #articleContent h6 {
        padding-top: 76px;
        margin-top: -76px;
    }

    #articleContent h1 {
        line-height: 3.5rem;
    }

    #articleContent h2 {
        line-height: 3.2rem;
    }

    #articleContent h3 {
        line-height: 2.8rem;
    }

    #articleContent h4 {
        line-height: 2.5rem;
    }

    #articleContent h5 {
        line-height: 2.2rem;
    }

    #articleContent h6 {
        line-height: 1.9rem;
    }

    #articleContent :focus {
        outline: none;
    }

    .toc-fixed {
        position: fixed;
        top: 64px;
    }

    .toc-widget {
        padding-left: 20px;
    }

    .toc-widget .toc-title {
        margin: 35px 0 15px 0;
        padding-left: 17px;
        font-size: 1.5rem;
        font-weight: bold;
        line-height: 1.5rem;
    }

    .toc-widget ol {
        padding: 0;
        list-style: none;
    }

    #toc-content ol {
        padding-left: 10px;
    }

    #toc-content ol li {
        padding-left: 10px;
    }

    #toc-content .toc-link:hover {
        color: #42b983;
        font-weight: 700;
        text-decoration: underline;
    }

    #toc-content .toc-link::before {
        background-color: transparent;
        max-height: 25px;
    }

    #toc-content .is-active-link {
        color: #42b983;
    }

    #toc-content .is-active-link::before {
        background-color: #42b983;
    }
</style>
<div class="row">
    <div class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/tags/Notes/" target="_blank">
                                <span class="chip bg-color">Notes</span>
                            </a>
                        
                            <a href="/tags/Deep-Learning/" target="_blank">
                                <span class="chip bg-color">Deep Learning</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fa fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/categories/Deep-Learning/" class="post-category" target="_blank">
                                Deep Learning
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                <div class="post-date info-break-policy">
                    <i class="fa fa-calendar-minus-o fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2018-11-11
                </div>

                
                    
                    <div class="info-break-policy">
                        <i class="fa fa-file-word-o fa-fw"></i>文章字数:&nbsp;&nbsp;
                        5.7k
                    </div>
                    

                    
                    <div class="info-break-policy">
                        <i class="fa fa-clock-o fa-fw"></i>阅读时长:&nbsp;&nbsp;
                        20 分
                    </div>
                    
                
				
				
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="fa fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">
        <div class="card-content article-card-content">
            <div id="articleContent">
                <p>上一章中，我们介绍了神经网络的学习，并通过数值微分计算了神经网络的权重参数的梯度（严格来说，是损失函数关于权重参数的梯度）。数值微分虽然简单，也容易实现，但<strong>缺点</strong>是计算上比较费时间。本章我们将学习一个能够高效计算权重参数的梯度的方法——误差反向传播法。</p>
<p>要正确理解误差反向传播法，我个人认为有两种方法：一种是<strong>基于数学式</strong>；另一种是<strong>基于计算图</strong>（computational graph）。前者是比较常见的方法，机器学习相关的图书中多数都是以数学式为中心展开论述的。因为这种方法严密且简洁，所以确实非常合理，但如果一上来就围绕数学式进行探讨，会忽略一些根本的东西，止步于式子的罗列。因此，本章希望大家通过计算图，直观地理解误差反向传播法。然后，再结合实际的代码加深理解，相信大家一定会有种“原来如此！”的感觉。</p>
<h2 id="计算图"><a href="#计算图" class="headerlink" title="计算图"></a>计算图</h2><p>计算图将计算过程用图形表示出来。这里说的图形是数据结构图，通过多个节点和边表示（连接节点的直线称为“边”）。为了让大家熟悉计算图，本节先用计算图解一些简单的问题。从这些简单的问题开始，逐步深入，最终抵达误差反向传播法。</p>
<h3 id="用计算图求解"><a href="#用计算图求解" class="headerlink" title="用计算图求解"></a>用计算图求解</h3><p><strong>问题 1</strong>：太郎在超市买了 2 个 100 日元一个的苹果，消费税是 10%，请计算支付金额。</p>
<p><img src="https://ws3.sinaimg.cn/large/006tKfTcly1g076nlaz88j30rs08idh5.jpg" alt="059"></p>
<p><strong>图 5-2　基于计算图求解的问题 1 的答案：“苹果的个数”和“消费税”作为变量标在○外面</strong></p>
<p>用计算图解题的情况下，需要按如下流程进行：</p>
<ul>
<li>构建计算图</li>
<li>在计算图上，从左向右进行计算</li>
</ul>
<p>这里的第 2 歩“从左向右进行计算”是一种正方向上的传播，简称为正向传播（forward propagation）。正向传播是从计算图出发点到结束点的传播。既然有正向传播这个名称，当然也可以考虑反向（从图上看的话，就是从右向左）的传播。实际上，这种传播称为反向传播（backward propagation）。反向传播将在接下来的导数计算中发挥重要作用。</p>
<h3 id="局部计算"><a href="#局部计算" class="headerlink" title="局部计算"></a>局部计算</h3><p>计算图可以集中精力于局部计算。无论全局的计算有多么复杂，各个步骤所要做的就是对象节点的局部计算。虽然局部计算非常简单，但是通过传递它的计算结果，可以获得全局的复杂计算的结果。</p>
<p>计算图将复杂的计算分割成简单的局部计算，和流水线作业一样，将局部计算的结果传递给下一个节点。在将复杂的计算分解成简单的计算这一点上与汽车的组装有相似之处。</p>
<h3 id="为何用计算图解题"><a href="#为何用计算图解题" class="headerlink" title="为何用计算图解题"></a>为何用计算图解题</h3><p>计算图到底有什么优点：</p>
<ul>
<li>局部计算：无论全局是多么复杂的计算，都可以通过局部计算使各个节点致力于简单的计算，从而简化问题</li>
<li>利用计算图可以将中间的计算结果全部保存起来</li>
<li>使用计算图最大的原因是，可以通过反向传播高效计算导数</li>
</ul>
<p>综上，计算图的优点是，可以通过正向传播和反向传播高效地计算各个变量的导数值。</p>
<p>问题 1 中，我们计算了购买 2 个苹果时加上消费税最终需要支付的金额。这里，假设我们想知道苹果价格的上涨会在多大程度上影响最终的支付金额，即求“支付金额关于苹果的价格的导数”。</p>
<p><img src="https://ws1.sinaimg.cn/large/006tKfTcly1g076o6b09qj30rs08pgn4.jpg" alt="062"></p>
<p><strong>图 5-5　基于反向传播的导数的传递</strong></p>
<p>如图 5-5 所示，反向传播使用与正方向相反的箭头（粗线）表示。反向传播传递“局部导数”，将导数的值写在箭头的下方。在这个例子中，反向传播从右向左传递导数的值（1 → 1.1 → 2.2）。从这个结果中可知，“支付金额关于苹果的价格的导数”的值是 2.2。这意味着，如果苹果的价格上涨 1 日元，最终的支付金额会增加 2.2 日元（严格地讲，如果苹果的价格增加某个微小值，则最终的支付金额将增加那个微小值的 2.2 倍）。</p>
<p>这里只求了关于苹果的价格的导数，不过“支付金额关于消费税的导数”“支付金额关于苹果的个数的导数”等也都可以用同样的方式算出来。并且，计算中途求得的导数的结果（中间传递的导数）可以被共享，从而可以高效地计算多个导数。综上，计算图的优点是，可以通过正向传播和反向传播高效地计算各个变量的导数值。</p>
<h2 id="链式法则"><a href="#链式法则" class="headerlink" title="链式法则"></a>链式法则</h2><p>前面介绍的计算图的正向传播将计算结果正向（从左到右）传递，其计算过程是我们日常接触的计算过程，所以感觉上可能比较自然。而反向传播将局部导数向正方向的反方向（从右到左）传递，一开始可能会让人感到困惑。传递这个局部导数的原理，是基于<strong>链式法则</strong>（chain rule）的。本节将介绍链式法则，并阐明它是如何对应计算图上的反向传播的。</p>
<h3 id="计算图的反向传播"><a href="#计算图的反向传播" class="headerlink" title="计算图的反向传播"></a>计算图的反向传播</h3><p>计算图的反向传播：沿着与正方向相反的方向，乘上局部导数。</p>
<p>这就是反向传播的计算顺序。通过这样的计算，可以高效地求出导数的值，这是反向传播的要点。那么这是如何实现的呢？我们可以从链式法则的原理进行解释。下面我们就来介绍链式法则。</p>
<h3 id="什么是链式法则"><a href="#什么是链式法则" class="headerlink" title="什么是链式法则"></a>什么是链式法则</h3><p>介绍链式法则时，我们需要先从<strong>复合函数</strong>说起。复合函数是由多个函数构成的函数。</p>
<p>链式法则是关于<strong>复合函数</strong>的导数的性质：</p>
<pre class=" language-text"><code class="language-text">如果某个函数由复合函数表示，则该复合函数的导数可以用构成复合函数的各个函数的导数的乘积表示。
</code></pre>
<h3 id="链式法则和计算图"><a href="#链式法则和计算图" class="headerlink" title="链式法则和计算图"></a>链式法则和计算图</h3><p>计算图：沿着与正方向相反的方向，乘上局部导数后传递</p>
<h2 id="反向传播"><a href="#反向传播" class="headerlink" title="反向传播"></a>反向传播</h2><p>上一节介绍了计算图的反向传播是基于链式法则成立的。本节将以“+”和“×”等运算为例，介绍反向传播的结构。</p>
<h3 id="加法节点的反向传播"><a href="#加法节点的反向传播" class="headerlink" title="加法节点的反向传播"></a>加法节点的反向传播</h3><p>加法节点的反向传播将上游的值原封不动地输出到下游。</p>
<h3 id="乘法节点的反向传播"><a href="#乘法节点的反向传播" class="headerlink" title="乘法节点的反向传播"></a>乘法节点的反向传播</h3><p><img src="https://ws1.sinaimg.cn/large/006tKfTcly1g0776h7kmej30rs0ba0tt.jpg" alt="070"></p>
<p><strong>图 5-12　乘法的反向传播：左图是正向传播，右图是反向传播</strong></p>
<p>乘法的反向传播会将上游的值乘以正向传播时的输入信号的“翻转值”后传递给下游。翻转值表示一种翻转关系，如图 5-12 所示，正向传播时信号是 x 的话，反向传播时则是 y；正向传播时信号是 y 的话，反向传播时则是 x。</p>
<p>法的反向传播只是将上游的值传给下游，并不需要正向传播的输入信号。但是，乘法的反向传播需要正向传播时的输入信号值。因此，实现乘法节点的反向传播时，要保存正向传播的输入信号。</p>
<h3 id="苹果的例子"><a href="#苹果的例子" class="headerlink" title="苹果的例子"></a>苹果的例子</h3><p>再来思考一下本章最开始举的购买苹果的例子（2 个苹果和消费税）。这里要解的问题是苹果的价格、苹果的个数、消费税这 3 个变量各自如何影响最终支付的金额。这个问题相当于求“支付金额关于苹果的价格的导数”“支付金额关于苹果的个数的导数”“支付金额关于消费税的导数”。用计算图的反向传播来解的话，求解过程如图 5-14 所示。</p>
<p><img src="https://ws4.sinaimg.cn/large/006tKfTcly1g0777ihyp8j30rs0a4wgh.jpg" alt="072"></p>
<p><strong>图 5-14　购买苹果的反向传播的例子</strong></p>
<h2 id="简单层的实现"><a href="#简单层的实现" class="headerlink" title="简单层的实现"></a>简单层的实现</h2><p>本节将用 Python 实现前面的购买苹果的例子。这里，我们把要实现的计算图的乘法节点称为“乘法层”（<code>MulLayer</code>），加法节点称为“加法层”（<code>AddLayer</code>）。</p>
<pre class=" language-text"><code class="language-text">下一节，我们将把构建神经网络的“层”实现为一个类。这里所说的“层”是神经网络中功能的单位。比如，负责 sigmoid 函数的 Sigmoid、负责矩阵乘积的 Affine 等，都以层为单位进行实现。因此，这里也以层为单位来实现乘法节点和加法节点。
</code></pre>
<h3 id="乘法层的实现"><a href="#乘法层的实现" class="headerlink" title="乘法层的实现"></a>乘法层的实现</h3><p>层的实现中有两个共通的方法（接口）<code>forward()</code> 和 <code>backward()</code>。<code>forward()</code> 对应正向传播，<code>backward()</code> 对应反向传播。</p>
<p>现在来实现乘法层。乘法层作为 <code>MulLayer</code> 类，其实现过程如下所示</p>
<pre class=" language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">MulLayer</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        self<span class="token punctuation">.</span>x <span class="token operator">=</span> None
        self<span class="token punctuation">.</span>y <span class="token operator">=</span> None

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">,</span> y<span class="token punctuation">)</span><span class="token punctuation">:</span>
        self<span class="token punctuation">.</span>x <span class="token operator">=</span> x
        self<span class="token punctuation">.</span>y <span class="token operator">=</span> y
        out <span class="token operator">=</span> x <span class="token operator">*</span> y

        <span class="token keyword">return</span> out

    <span class="token keyword">def</span> <span class="token function">backward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> dout<span class="token punctuation">)</span><span class="token punctuation">:</span>
        dx <span class="token operator">=</span> dout <span class="token operator">*</span> self<span class="token punctuation">.</span>y <span class="token comment" spellcheck="true"># 翻转x和y</span>
        dy <span class="token operator">=</span> dout <span class="token operator">*</span> self<span class="token punctuation">.</span>x

        <span class="token keyword">return</span> dx<span class="token punctuation">,</span> dy
</code></pre>
<h3 id="加法层的实现"><a href="#加法层的实现" class="headerlink" title="加法层的实现"></a>加法层的实现</h3><pre class=" language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">AddLayer</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        <span class="token keyword">pass</span>

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">,</span> y<span class="token punctuation">)</span><span class="token punctuation">:</span>
        out <span class="token operator">=</span> x <span class="token operator">+</span> y
        <span class="token keyword">return</span> out

    <span class="token keyword">def</span> <span class="token function">backward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> dout<span class="token punctuation">)</span><span class="token punctuation">:</span>
        dx <span class="token operator">=</span> dout <span class="token operator">*</span> <span class="token number">1</span>
        dy <span class="token operator">=</span> dout <span class="token operator">*</span> <span class="token number">1</span>
        <span class="token keyword">return</span> dx<span class="token punctuation">,</span> dy
</code></pre>
<h2 id="激活函数层的实现"><a href="#激活函数层的实现" class="headerlink" title="激活函数层的实现"></a>激活函数层的实现</h2><p>现在，我们将计算图的思路应用到神经网络中。这里，我们把构成神经网络的层实现为一个类。先来实现激活函数的 <code>ReLU</code> 层和 <code>Sigmoid</code> 层。</p>
<h3 id="ReLU-层"><a href="#ReLU-层" class="headerlink" title="ReLU 层"></a>ReLU 层</h3><p>激活函数 ReLU（Rectified Linear Unit）由下式（5.7）表示。</p>
<p><img src="https://ws2.sinaimg.cn/large/006tKfTcly1g077miwnvrj30x50680sw.jpg" alt="188"></p>
<p>通过式（5.7），可以求出 <em>y</em> 关于 <em>x</em> 的导数，如式（5.8）所示。</p>
<p><img src="https://ws2.sinaimg.cn/large/006tKfTcly1g077mllfkqj30xp06aq36.jpg" alt="189"></p>
<p>如果正向传播时的输入 x 大于 0，则反向传播会将上游的值原封不动地传给下游。反过来，如果正向传播时的 x 小于等于 0，则反向传播中传给下游的信号将停在此处。</p>
<p><img src="https://ws4.sinaimg.cn/large/006tKfTcly1g077quqdbaj30rs06o3zb.jpg" alt="076"></p>
<p><strong>图 5-18　ReLU 层的计算图</strong></p>
<pre class=" language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">Relu</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        self<span class="token punctuation">.</span>mask <span class="token operator">=</span> None

    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
        self<span class="token punctuation">.</span>mask <span class="token operator">=</span> <span class="token punctuation">(</span>x <span class="token operator">&lt;=</span> <span class="token number">0</span><span class="token punctuation">)</span>
        out <span class="token operator">=</span> x<span class="token punctuation">.</span>copy<span class="token punctuation">(</span><span class="token punctuation">)</span>
        out<span class="token punctuation">[</span>self<span class="token punctuation">.</span>mask<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">0</span>

        <span class="token keyword">return</span> out

    <span class="token keyword">def</span> <span class="token function">backward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> dout<span class="token punctuation">)</span><span class="token punctuation">:</span>
        dout<span class="token punctuation">[</span>self<span class="token punctuation">.</span>mask<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">0</span>
        dx <span class="token operator">=</span> dout

        <span class="token keyword">return</span> dx
</code></pre>
<pre class=" language-text"><code class="language-text">ReLU 层的作用就像电路中的开关一样。正向传播时，有电流通过的话，就将开关设为 ON；没有电流通过的话，就将开关设为 OFF。反向传播时，开关为 ON 的话，电流会直接通过；开关为 OFF 的话，则不会有电流通过。
</code></pre>
<h3 id="Sigmoid-层"><a href="#Sigmoid-层" class="headerlink" title="Sigmoid 层"></a>Sigmoid 层</h3><p>sigmoid 函数由式（5.9）表示。</p>
<p><img src="https://ws4.sinaimg.cn/large/006tKfTcly1g077r6uudtg30770150mq.gif" alt="gif"></p>
<p>用计算图表示式（5.9）的话，则如图 5-19 所示。</p>
<p><img src="https://ws2.sinaimg.cn/large/006tKfTcly1g077rplchoj30rs06hmxx.jpg" alt="077"></p>
<p>Sigmoid层的计算图如图5-20所示：</p>
<p><img src="/Users/yuzhongchun/Downloads/081.png" alt="081"></p>
<p><strong>图 5-20　Sigmoid 层的计算图</strong></p>
<p>最终，Sigmoid的计算图（简洁版）如5-21所示：</p>
<p><img src="https://note.youdao.com/yws/api/personal/file/WEBb80b06e14402edc2713c0a8786695e53?method=download&amp;shareKey=4bb2fbbed6e9ffafe47accefb1c087be" alt=""></p>
<p><strong>图 5-21　Sigmoid 层的计算图（简洁版）</strong></p>
<p>图 5-20 的计算图和简洁版的图 5-21 的计算图的计算结果是相同的，但是，简洁版的计算图可以省略反向传播中的计算过程，因此计算效率更高。此外，通过对节点进行集约化，可以不用在意 Sigmoid 层中琐碎的细节，而只需要专注它的输入和输出，这一点也很重要。</p>
<p>另外，可以进一步整理为：</p>
<p><img src="https://note.youdao.com/yws/api/personal/file/WEB060508652aac5da164d3394bc9777b6b?method=download&amp;shareKey=21050fc18d504494022c4ee20a31df92" alt=""></p>
<p>因此，图 5-21 所表示的 Sigmoid 层的反向传播，只根据正向传播的输出就能计算出来。</p>
<p><img src="https://note.youdao.com/yws/api/personal/file/WEB876fcd0a1642c652c8fd6052f57937f9?method=download&amp;shareKey=91e837041cddb8e988337a8c9ec7e0c7" alt=""></p>
<p><strong>图 5-22　Sigmoid 层的计算图：可以根据正向传播的输出 y 计算反向传播</strong></p>
<h2 id="Affine-Softmax-层的实现"><a href="#Affine-Softmax-层的实现" class="headerlink" title="Affine/Softmax 层的实现"></a>Affine/Softmax 层的实现</h2><h3 id="Affine-层"><a href="#Affine-层" class="headerlink" title="Affine 层"></a>Affine 层</h3><pre class=" language-text"><code class="language-text">神经网络的正向传播中进行的矩阵的乘积运算在几何学领域被称为“仿射变换”{1[几何中，仿射变换包括一次线性变换和一次平移，分别对应神经网络的加权和运算与加偏置运算。——译者注]}。因此，这里将进行仿射变换的处理实现为“Affine 层”。
</code></pre>
<p>现在将这里进行的求矩阵的乘积与偏置的和的运算用计算图表示出来。将乘积运算用“dot”节点表示的话，则 <code>np.dot(X, W) + B</code> 的运算可用图 5-24 所示的计算图表示出来。另外，在各个变量的上方标记了它们的形状（比如，计算图上显示了 <strong>X</strong> 的形状为 (2,)，<strong>X</strong> · <strong>W</strong> 的形状为 (3,) 等）。</p>
<p><img src="https://ws4.sinaimg.cn/large/006tKfTcly1g08mxaad97j30rs0htdho.jpg" alt="085"></p>
<p><strong>图 5-24　Affine 层的计算图（注意变量是矩阵，各个变量的上方标记了该变量的形状）</strong></p>
<p>图 5-24 是比较简单的计算图，不过要注意 <strong>X</strong>、<strong>W</strong>、<strong>B</strong> 是矩阵（多维数组）。之前我们见到的计算图中各个节点间流动的是标量，而这个例子中各个节点间传播的是矩阵。</p>
<p>现在我们来考虑图 5-24 的计算图的反向传播。以矩阵为对象的反向传播，按矩阵的各个元素进行计算时，步骤和以标量为对象的计算图相同。实际写一下的话，可以得到下式（这里省略了式（5.13）的推导过程）。</p>
<p><img src="https://ws3.sinaimg.cn/large/006tKfTcly1g08n1dxv5mj30m806n74a.jpg" alt="5_13"></p>
<p>现在，我们根据式（5.13），尝试写出计算图的反向传播，如图 5-25 所示。</p>
<p><img src="https://ws1.sinaimg.cn/large/006tKfTcly1g08n9c2zaxj30rs0er0vl.jpg" alt="086"></p>
<p><strong>图 5-25　Affine 层的反向传播：注意变量是多维数组。反向传播时各个变量的下方标记了该变量的形状</strong></p>
<p><img src="https://ws4.sinaimg.cn/large/006tKfTcly1g08nbzvqu3j30rs0a9gnb.jpg" alt="087"></p>
<p><strong>图 5-26　矩阵的乘积（“dot”节点）的反向传播可以通过组建使矩阵对应维度的元素个数一致的乘积运算而推导出来</strong></p>
<h3 id="批版本的的-Affine-层"><a href="#批版本的的-Affine-层" class="headerlink" title="批版本的的 Affine 层"></a>批版本的的 Affine 层</h3><p>前面介绍的 Affine层的输入 <strong>X</strong> 是以单个数据为对象的。现在我们考虑 <em>N</em> 个数据一起进行正向传播的情况，也就是批版本的 Affine层。</p>
<p>先给出批版本的 Affine层的计算图，如图 5-27 所示。</p>
<p><img src="https://ws3.sinaimg.cn/large/006tKfTcly1g08ndvs01sj30rs0fy427.jpg" alt="088"></p>
<p><strong>图 5-27　批版本的 Affine 层的计算图</strong></p>
<h3 id="Softmax-with-Loss-层"><a href="#Softmax-with-Loss-层" class="headerlink" title="Softmax-with-Loss 层"></a>Softmax-with-Loss 层</h3><p>最后介绍一下输出层的 softmax 函数。前面我们提到过，softmax 函数会将输入值正规化之后再输出。比如手写数字识别时，Softmax 层的输出如图：<br><img src="https://note.youdao.com/yws/api/personal/file/WEB286e1df6cedf604becb8a12da92ca472?method=download&amp;shareKey=2a9b77c4b9d45dfe61c7096a1437f802" alt=""></p>
<p><strong>图 5-28</strong> 输入图像通过 Affine 层和 ReLU 层进行转换，10 个输入通过 Softmax 层进行正规化。在这个例子中，“0”的得分是 5.3，这个值经过 Softmax 层转换为 0.008（0.8%）；“2”的得分是 10.1，被转换为 0.991（99.1%）</p>
<p>在图 5-28 中，Softmax 层将输入值正规化（将输出值的和调整为 1）之后再输出。另外，因为手写数字识别要进行 10 类分类，所以向Softmax 层的输入也有 10 个。</p>
<pre class=" language-text"><code class="language-text">神经网络中进行的处理有推理（inference）和学习两个阶段。神经网络的推理通常不使用 Softmax 层。比如，用图 5-28 的网络进行推理时，会将最后一个 Affine 层的输出作为识别结果。神经网络中未被正规化的输出结果（图 5-28 中 Softmax 层前面的 Affine 层的输出）有时被称为“得分”。也就是说，当神经网络的推理只需要给出一个答案的情况下，因为此时只对得分最大值感兴趣，所以不需要 Softmax 层。不过，神经网络的学习阶段则需要 Softmax 层。
</code></pre>
<p>下面来实现 Softmax 层。考虑到这里也包含作为损失函数的交叉熵误差（cross entropy error），所以称为“Softmax-with-Loss 层”。Softmax-with-Loss 层（Softmax 函数和交叉熵误差）的计算图如图 5-29 所示。</p>
<p><img src="https://ws1.sinaimg.cn/large/006tKfTcly1g08nuk2t8dj30rs096whk.jpg" alt="090"></p>
<p><strong>图 5-29　Softmax-with-Loss 层的计算图</strong></p>
<p>图 5-29 的计算图可以简化成图 5-30。</p>
<p>图 5-30 的计算图中，softmax 函数记为 Softmax 层，交叉熵误差记为 Cross Entropy Error 层。这里假设要进行 3 类分类，从前面的层接收 3 个输入（得分）。如图 5-30 所示，Softmax 层将输入(<em>a</em>1, <em>a</em>2, <em>a</em>3) 正规化，输出(<em>y</em>1, <em>y</em>2, <em>y</em>3)。Cross Entropy Error 层接收 Softmax 的输出(<em>y</em>1, <em>y</em>2, <em>y</em>3) 和教师标签(<em>t</em>1, <em>t</em>2, <em>t</em>3)，从这些数据中输出损失 <em>L</em>。</p>
<p><img src="https://ws1.sinaimg.cn/large/006tKfTcly1g08nvl0yrfj30rs0hv0tv.jpg" alt="091"></p>
<p><strong>图 5-30　“简易版”的 Softmax-with-Loss 层的计算图</strong></p>
<p>图 5-30 中要注意的是反向传播的结果。Softmax 层的反向传播得到了(<em>y</em>1-<em>t</em>1, <em>y</em>2-<em>t</em>2, <em>y</em>3-<em>t</em>3)这样“漂亮”的结果。由于(<em>y</em>1, <em>y</em>2, <em>y</em>3)是 Softmax 层的输出，(<em>t</em>1, <em>t</em>2, <em>t</em>3)是监督数据，所以(<em>y</em>1-<em>t</em>1, <em>y</em>2-<em>t</em>2, <em>y</em>3-<em>t</em>3)是 Softmax 层的输出和教师标签的差分。神经网络的反向传播会把这个差分表示的误差传递给前面的层，这是神经网络学习中的重要性质。</p>
<p>神经网络学习的目的就是通过调整权重参数，使神经网络的输出（Softmax 的输出）接近教师标签。因此，必须将神经网络的输出与教师标签的误差高效地传递给前面的层。刚刚的(y1-t1, y2-t2, y3-t3)正是 Softmax 层的输出与教师标签的差，直截了当地表示了当前神经网络的输出与教师标签的误差。</p>
<pre class=" language-text"><code class="language-text">使用交叉熵误差作为 softmax 函数的损失函数后，反向传播得到(y1-t1, y2-t2, y3-t3)这样“漂亮”的结果。实际上，这样“漂亮”的结果并不是偶然的，而是为了得到这样的结果，特意设计了交叉熵误差函数。回归问题中输出层使用“恒等函数”，损失函数使用“平方和误差”，也是出于同样的理由（3.5 节）。也就是说，使用“平方和误差”作为“恒等函数”的损失函数，反向传播才能得到(y1-t1, y2-t2, y3-t3)这样“漂亮”的结果。
</code></pre>
<h2 id="误差反向传播法的实现"><a href="#误差反向传播法的实现" class="headerlink" title="误差反向传播法的实现"></a>误差反向传播法的实现</h2><p>通过像组装乐高积木一样组装上一节中实现的层，可以构建神经网络。本节我们将通过组装已经实现的层来构建神经网络。</p>
<h3 id="神经网络学习的全貌图"><a href="#神经网络学习的全貌图" class="headerlink" title="神经网络学习的全貌图"></a>神经网络学习的全貌图</h3><p>在进行具体的实现之前，我们再来确认一下神经网络学习的全貌图。神经网络学习的步骤如下所示。</p>
<pre><code>前提
    神经网络中有合适的权重和偏置，调整权重和偏置以便拟合训练数据的过程称为学习。神经网络的学习分为下面 4 个步骤。

步骤1： mini-batch
    从训练数据中随机选择一部分数据。
步骤2：计算梯度
    计算损失函数关于各个权重参数的梯度。
步骤3：更新参数
    将权重参数沿梯度方向进行微小的更新。
步骤4：重复
    重复步骤 1、步骤 2、步骤 3。
</code></pre><h3 id="对应误差反向传播法的神经网络的实现"><a href="#对应误差反向传播法的神经网络的实现" class="headerlink" title="对应误差反向传播法的神经网络的实现"></a>对应误差反向传播法的神经网络的实现</h3><p>像这样通过将神经网络的组成元素以层的方式实现，可以轻松地构建神经网络。这个用层进行模块化的实现具有很大优点。因为想另外构建一个神经网络（比如 5 层、10 层、20 层……的大的神经网络）时，只需像组装乐高积木那样添加必要的层就可以了。之后，通过各个层内部实现的正向传播和反向传播，就可以正确计算进行识别处理或学习所需的梯度。</p>
<h3 id="误差反向传播法的梯度确认"><a href="#误差反向传播法的梯度确认" class="headerlink" title="误差反向传播法的梯度确认"></a>误差反向传播法的梯度确认</h3><p>到目前为止，我们介绍了两种求梯度的方法。一种是基于数值微分的方法，另一种是解析性地求解数学式的方法。后一种方法通过使用误差反向传播法，即使存在大量的参数，也可以高效地计算梯度。因此，后文将不再使用耗费时间的数值微分，而是使用误差反向传播法求梯度。</p>
<p>数值微分的计算很耗费时间，而且如果有误差反向传播法的（正确的）实现的话，就没有必要使用数值微分的实现了。那么数值微分有什么用呢？实际上，在确认误差反向传播法的实现是否正确时，是需要用到数值微分的。</p>
<p>数值微分的优点是实现简单，因此，一般情况下不太容易出错。而误差反向传播法的实现很复杂，容易出错。所以，经常会比较数值微分的结果和误差反向传播法的结果，以确认误差反向传播法的实现是否正确。确认数值微分求出的梯度结果和误差反向传播法求出的结果是否一致（严格地讲，是非常相近）的操作称为梯度确认（gradient check）。</p>
<pre class=" language-text"><code class="language-text">数值微分和误差反向传播法的计算结果之间的误差为 0 是很少见的。这是因为计算机的计算精度有限（比如，32 位浮点数）。受到数值精度的限制，刚才的误差一般不会为 0，但是如果实现正确的话，可以期待这个误差是一个接近 0 的很小的值。如果这个值很大，就说明误差反向传播法的实现存在错误。
</code></pre>
<h3 id="使用误差反向传播法的学习"><a href="#使用误差反向传播法的学习" class="headerlink" title="使用误差反向传播法的学习"></a>使用误差反向传播法的学习</h3><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>本章我们介绍了将计算过程可视化的计算图，并使用计算图，介绍了神经网络中的误差反向传播法，并以层为单位实现了神经网络中的处理。我们学过的层有 ReLU 层、Softmax-with-Loss 层、Affine 层、Softmax 层等，这些层中实现了 forward 和 backward 方法，通过将数据正向和反向地传播，可以高效地计算权重参数的梯度。通过使用层进行模块化，神经网络中可以自由地组装层，轻松构建出自己喜欢的网络。</p>
<ul>
<li>通过使用计算图，可以直观地把握计算过程。</li>
<li>计算图的节点是由局部计算构成的。局部计算构成全局计算。</li>
<li>计算图的正向传播进行一般的计算。通过计算图的反向传播，可以计算各个节点的导数。</li>
<li>通过将神经网络的组成元素实现为层，可以高效地计算梯度（反向传播法）。</li>
<li>通过比较数值微分和误差反向传播法的结果，可以确认误差反向传播法的实现是否正确（梯度确认）。</li>
</ul>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol>
<li><a href="http://www.ituring.com.cn/book/1921" target="_blank" rel="noopener">深度学习入门：基于Python的理论与实现</a></li>
</ol>

            </div>
            <hr>

            
            <style>
    #reward {
        margin: 40px 0;
        text-align: center;
    }

    #reward .reward-link {
        font-size: 1.88rem;
    }

    #reward .btn-floating:hover {
        box-shadow: 0 6px 12px rgba(0, 0, 0, 0.2), 0 5px 15px rgba(0, 0, 0, 0.2);
    }

    #rewardModal {
        width: 320px;
        height: 350px;
    }

    #rewardModal .reward-title {
        margin: 15px auto;
        padding-bottom: 5px;
    }

    #rewardModal .modal-content {
        padding: 10px;
    }

    #rewardModal .close {
        position: absolute;
        right: 15px;
        top: 15px;
        color: rgba(0, 0, 0, 0.5);
        font-size: 1.3rem;
        line-height: 20px;
        cursor: pointer;
    }

    #rewardModal .reward-tabs {
        margin: 0 auto;
        width: 210px;
    }

    .reward-tabs .tabs {
        height: 38px;
        margin: 10px auto;
        padding-left: 0;
    }

    .reward-tabs .tabs .tab {
        height: 38px;
        line-height: 38px;
    }

    .reward-tabs .tab a {
        color: #fff;
        background-color: #ccc;
    }

    .reward-tabs .tab a:hover {
        color: #fff;
    }

    .reward-tabs .wechat-tab .active {
        color: #fff;
        background-color: #22AB38;
    }

    .reward-tabs .alipay-tab .active {
        color: #fff;
        background-color: #019FE8;
    }

    .reward-tabs .reward-img {
        width: 210px;
        height: 210px;
    }
</style>

<div id="reward">
    <a class="reward-link btn-floating btn-large waves-effect waves-light red">赏</a>

    <!-- Modal Structure -->
    <div id="rewardModal" class="modal">
        <div class="modal-content">
            <a class="close"><i class="fa fa-close"></i></a>
            <h4 class="reward-title">你的赏识是我前进的动力</h4>
            <div class="reward-content">
                <div class="reward-tabs">
                    <ul class="tabs">
                        <li class="tab wechat-tab waves-effect waves-light"><a class="active" href="#wechat">微信</a></li>
                        <li class="tab alipay-tab waves-effect waves-light"><a href="#alipay">支付宝</a></li>
                    </ul>
                    <div id="wechat">
                        <img src="/medias/reward/wechat.jpg" class="reward-img" alt="微信打赏二维码">
                    </div>
                    <div id="alipay">
                        <img src="/medias/reward/alipay.jpg" class="reward-img" alt="支付宝打赏二维码">
                    </div>
                </div>
            </div>
        </div>
    </div>
</div>

<script>
    $(function () {
        $('#reward .reward-link').on('click', function () {
            $('#rewardModal').openModal();
        });

        $('#rewardModal .close').on('click', function () {
            $('#rewardModal').closeModal();
        });
    });
</script>
            

            <link rel="stylesheet" type="text/css" href="/libs/share/css/share.min.css">

<div id="article-share">
    
    <div class="social-share" data-disabled="qzone" data-wechat-qrcode-helper="<p>微信里点“发现”->“扫一扫”二维码便可查看分享。</p>"></div>
    
</div>

<script src="/libs/share/js/social-share.min.js"></script>

            <div class="reprint">
                <p>
                    <span class="reprint-tip">
                        <i class="fa fa-exclamation-circle"></i>&nbsp;&nbsp;转载请注明:
                    </span>
                    <a href="http://yuzhongchun.com" class="b-link-green">BerMaker</a>
                    <i class="fa fa-angle-right fa-lg fa-fw text-color"></i>
                    <a href="/2018/11/11/29-dl-backward-propagation-algorithm-notes/" class="b-link-green">Notes-深度学习入门之反向传播算法</a>
                </p>
            </div>
        </div>
    </div>

    
        <link rel="stylesheet" href="/libs/gitalk/gitalk.css">
<link rel="stylesheet" href="/css/my-gitalk.css">

<div class="card gitalk-card" data-aos="fade-up">
    <div id="gitalk-container" class="card-content"></div>
</div>

<script src="/libs/gitalk/gitalk.min.js"></script>
<script>
    let gitalk = new Gitalk({
        clientID: '21ea087c6104923ba888',
        clientSecret: '560f45f0aefc21d726f86ee331bb04fc8660bcf6',
        repo: 'gitalk',
        owner: 'zhongchun',
        admin: "zhongchun",
        id: '2018-11-11T14-35-32',
        distractionFreeMode: false  // Facebook-like distraction free mode
    });

    gitalk.render('gitalk-container');
</script>
    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fa fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/2018/11/16/30-dl-learning-skills-notes/">
                    <div class="card-image">
                        
                        <img src="/images/learning-skills.jpg" class="responsive-img" alt="Notes-深度学习入门之与学习相关的技巧">
                        
                        <span class="card-title">Notes-深度学习入门之与学习相关的技巧</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary">本章将介绍神经网络的学习中的一些重要观点，主题涉及寻找最优权重参数的最优化方法、权重参数的初始值、超参数的设定方法等。此外，为了应对过拟合，本章还将介绍权值衰减、Dropout 等正则化方法，并进行实现。最后将对近年来众多研究中使用的 Ba</div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="fa fa-clock-o fa-fw icon-date"></i>2018-11-16
                        </span>
                        <span class="publish-author">
                            
                            <i class="fa fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/categories/Deep-Learning/" class="post-category" target="_blank">
                                    Deep Learning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/tags/Notes/" target="_blank">
                        <span class="chip bg-color">Notes</span>
                    </a>
                    
                    <a href="/tags/Deep-Learning/" target="_blank">
                        <span class="chip bg-color">Deep Learning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fa fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/2018/11/10/31-machine-learning-guide/">
                    <div class="card-image">
                        
                        <img src="/images/machine-learning-woman.jpg" class="responsive-img" alt="Machine Learning Guide">
                        
                        <span class="card-title">Machine Learning Guide</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary">Machine learning is a subset of artificial intelligence in the field of computer science that often uses statistical tec</div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="fa fa-clock-o fa-fw icon-date"></i>2018-11-10
                            </span>
                        <span class="publish-author">
                            
                            <i class="fa fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/categories/Machine-Learning/" class="post-category" target="_blank">
                                    Machine Learning
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/tags/Machine-Learning/" target="_blank">
                        <span class="chip bg-color">Machine Learning</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>
</div>


<script>
    $('#articleContent').on('copy', function (e) {
        // IE8 or earlier browser is 'undefined'
        if (typeof window.getSelection === 'undefined') return;

        var selection = window.getSelection();
        // if the selection is short let's not annoy our users.
        if (('' + selection).length < Number.parseInt('120')) {
            return;
        }

        // create a div outside of the visible area and fill it with the selected text.
        var bodyElement = document.getElementsByTagName('body')[0];
        var newdiv = document.createElement('div');
        newdiv.style.position = 'absolute';
        newdiv.style.left = '-99999px';
        bodyElement.appendChild(newdiv);
        newdiv.appendChild(selection.getRangeAt(0).cloneContents());

        // we need a <pre> tag workaround.
        // otherwise the text inside "pre" loses all the line breaks!
        if (selection.getRangeAt(0).commonAncestorContainer.nodeName === 'PRE') {
            newdiv.innerHTML = "<pre>" + newdiv.innerHTML + "</pre>";
        }

        var url = document.location.href;
        newdiv.innerHTML += '<br />'
            + '来源: BerMaker<br />'
            + '作者: BerMaker<br />'
            + '链接: <a href="' + url + '">' + url + '</a><br />'
            + '本文章著作权归作者所有，任何形式的转载都请注明出处。';

        selection.selectAllChildren(newdiv);
        window.setTimeout(function () {bodyElement.removeChild(newdiv);}, 200);
    });
</script>

    </div>
    <div class="col l3 hide-on-med-and-down">
        <div class="toc-widget">
            <div class="toc-title"><i class="fa fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<script src="/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingSelector: 'h2, h3, h4'
        });

        // modify the toc link href to support Chinese.
        let i = 0;
        let tocHeading = 'toc-heading-';
        $('#toc-content a').each(function () {
            $(this).attr('href', '#' + tocHeading + (++i));
        });

        // modify the heading title id to support Chinese.
        i = 0;
        $('#articleContent').children('h2, h3, h4').each(function () {
            $(this).attr('id', tocHeading + (++i));
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });
    });
</script>
    

</main>


<footer class="page-footer bg-color">
    <div class="container row center-align">
        <div class="col s12 m8 l8 copy-right">
            本站由&copy;<a href="https://blinkfox.github.io/" target="_blank">Blinkfox</a>基于
            <a href="https://hexo.io/" target="_blank">Hexo</a> 的
            <a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">hexo-theme-matery</a>主题搭建.

            
                &nbsp;<i class="fa fa-area-chart"></i>&nbsp;站点总字数:&nbsp;
                <span class="white-color">78.3k</span>
            

            
			
                <br>
                
                <span id="busuanzi_container_site_pv">
                    <i class="fa fa-heart-o"></i>
                    本站总访问量 <span id="busuanzi_value_site_pv" class="white-color"></span>
                </span>
                
                
                <span id="busuanzi_container_site_uv">
                    <i class="fa fa-users"></i>
                    次,&nbsp;访客数 <span id="busuanzi_value_site_uv" class="white-color"></span> 人.
                </span>
                
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/zhongchun" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fa fa-github"></i>
    </a>



    <a href="mailto:495571751@qq.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fa fa-envelope-open"></i>
    </a>



    <a href="/atom.xml" class="tooltipped" target="_blank" data-tooltip="RSS 订阅" data-position="top" data-delay="50">
        <i class="fa fa-rss"></i>
    </a>

</div>
    </div>
</footer>

<div class="progress-bar"></div>


<!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fa fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字" class="search-input" autofocus>
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script src="/js/search.js"></script>
<script type="text/javascript">
    searchFunc("/" + "search.xml", 'searchInput', 'searchResult');
</script>
<!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fa fa-angle-up"></i>
    </a>
</div>


<script src="/libs/materialize/js/materialize.min.js"></script>
<script src="/libs/masonry/masonry.pkgd.min.js"></script>
<script src="/libs/aos/aos.js"></script>
<script src="/libs/scrollprogress/scrollProgress.min.js"></script>
<script src="/libs/lightGallery/js/lightgallery-all.min.js"></script>
<script src="/js/matery.js"></script>

<!-- Global site tag (gtag.js) - Google Analytics -->



    <script src="/libs/others/clicklove.js"></script>


    <script async src="/libs/others/busuanzi.pure.mini.js"></script>


</body>
</html>