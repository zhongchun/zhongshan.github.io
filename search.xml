<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Ray 环境搭建和示例</title>
      <link href="/2019/06/23/2-ray-env/"/>
      <url>/2019/06/23/2-ray-env/</url>
      
        <content type="html"><![CDATA[<h1 id="Ray-环境搭建和示例"><a href="#Ray-环境搭建和示例" class="headerlink" title="Ray 环境搭建和示例"></a>Ray 环境搭建和示例</h1><h2 id="Ray-环境设置"><a href="#Ray-环境设置" class="headerlink" title="Ray 环境设置"></a>Ray 环境设置</h2><p>本次实验采用 2 台 Mac，系统 macOS Mojave Version 10.14.3。</p><h3 id="安装-Python-3"><a href="#安装-Python-3" class="headerlink" title="安装 Python 3"></a>安装 Python 3</h3><pre class=" language-python"><code class="language-python">brew install python</code></pre><h3 id="安装-Python-虚拟环境"><a href="#安装-Python-虚拟环境" class="headerlink" title="安装 Python 虚拟环境"></a>安装 Python 虚拟环境</h3><pre class=" language-python"><code class="language-python">pip3 install virtualenvcd <span class="token operator">~</span><span class="token operator">/</span>Toolsvirtualenv <span class="token operator">-</span>p <span class="token operator">/</span>usr<span class="token operator">/</span>local<span class="token operator">/</span>bin<span class="token operator">/</span>python3 git_ray_envsource <span class="token punctuation">.</span><span class="token operator">/</span>git_ray<span class="token operator">/</span>env<span class="token operator">/</span>bin<span class="token operator">/</span>active    </code></pre><p>然后，查看虚拟环境 Python 的版本</p><pre class=" language-python"><code class="language-python">$ python <span class="token operator">-</span>VPython <span class="token number">3.7</span><span class="token punctuation">.</span><span class="token number">3</span></code></pre><p>注意：确保两台机器 Python 版本一致。</p><h3 id="安装-Ray"><a href="#安装-Ray" class="headerlink" title="安装 Ray"></a>安装 Ray</h3><pre class=" language-python"><code class="language-python">pip install ray</code></pre><p>Ray 的版本是 0.7.0</p><h3 id="以-Cluster-模式运行-Ray"><a href="#以-Cluster-模式运行-Ray" class="headerlink" title="以 Cluster 模式运行 Ray"></a>以 Cluster 模式运行 Ray</h3><p>两台机器IP分别为：<code>192.168.1.6</code> 和 <code>192.168.1.9</code></p><p><code>192.168.1.6</code> 作为 Head，另一台作为 Node。</p><p>在 Head 机器上执行：</p><pre class=" language-shell"><code class="language-shell">ray start --head --redis-port=6379</code></pre><p>启动输入如下：</p><pre class=" language-tex"><code class="language-tex">2019-06-23 15:04:18,601    INFO scripts.py:289 -- Using IP address 192.168.1.6 for this node.2019-06-23 15:04:18,602    INFO node.py:497 -- Process STDOUT and STDERR is being redirected to /tmp/ray/session_2019-06-23_15-04-18_601521_88785/logs.2019-06-23 15:04:18,710    INFO services.py:409 -- Waiting for redis server at 127.0.0.1:6379 to respond...2019-06-23 15:04:18,835    INFO services.py:409 -- Waiting for redis server at 127.0.0.1:25445 to respond...2019-06-23 15:04:18,840    INFO services.py:806 -- Starting Redis shard with 3.44 GB max memory.2019-06-23 15:04:18,855    INFO node.py:511 -- Process STDOUT and STDERR is being redirected to /tmp/ray/session_2019-06-23_15-04-18_601521_88785/logs.2019-06-23 15:04:18,856    INFO services.py:1441 -- Starting the Plasma object store with 5.15 GB memory using /tmp.2019-06-23 15:04:18,877    INFO scripts.py:319 --Started Ray on this node. You can add additional nodes to the cluster by calling    ray start --redis-address 192.168.1.6:6379from the node you wish to add. You can connect a driver to the cluster from Python by running    import ray    ray.init(redis_address="192.168.1.6:6379")If you have trouble connecting from a different machine, check that your firewall is configured properly. If you wish to terminate the processes that have been started, run    ray stop</code></pre><p>在 Node 机器上执行：</p><pre class=" language-shell"><code class="language-shell">ray start --redis-address=192.168.1.6:6379</code></pre><p>启动输入如下：</p><pre><code>2019-06-23 15:08:37,474    INFO services.py:409 -- Waiting for redis server at 192.168.1.6:6379 to respond...2019-06-23 15:08:37,503    INFO scripts.py:363 -- Using IP address 192.168.1.9 for this node.2019-06-23 15:08:37,551    INFO node.py:511 -- Process STDOUT and STDERR is being redirected to /tmp/ray/session_2019-06-23_15-04-18_601521_88785/logs.2019-06-23 15:08:37,551    INFO services.py:1441 -- Starting the Plasma object store with 5.15 GB memory using /tmp.2019-06-23 15:08:37,566    INFO scripts.py:371 --Started Ray on this node. If you wish to terminate the processes that have been started, run    ray stop</code></pre><h2 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h2><p>hello_world.py</p><pre class=" language-python"><code class="language-python"><span class="token keyword">import</span> rayray<span class="token punctuation">.</span>init<span class="token punctuation">(</span>redis_address<span class="token operator">=</span><span class="token string">"192.168.1.6:6379"</span><span class="token punctuation">)</span>@ray<span class="token punctuation">.</span>remote<span class="token keyword">def</span> <span class="token function">hello</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">return</span> <span class="token string">"Hello"</span>@ray<span class="token punctuation">.</span>remote<span class="token keyword">def</span> <span class="token function">world</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">return</span> <span class="token string">"world!"</span>@ray<span class="token punctuation">.</span>remote<span class="token keyword">def</span> <span class="token function">hello_world</span><span class="token punctuation">(</span>a<span class="token punctuation">,</span> b<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">return</span> a <span class="token operator">+</span> <span class="token string">" "</span> <span class="token operator">+</span> ba_id <span class="token operator">=</span> hello<span class="token punctuation">.</span>remote<span class="token punctuation">(</span><span class="token punctuation">)</span>b_id <span class="token operator">=</span> world<span class="token punctuation">.</span>remote<span class="token punctuation">(</span><span class="token punctuation">)</span>c_id <span class="token operator">=</span> hello_world<span class="token punctuation">.</span>remote<span class="token punctuation">(</span>a_id<span class="token punctuation">,</span> b_id<span class="token punctuation">)</span>hello <span class="token operator">=</span> ray<span class="token punctuation">.</span>get<span class="token punctuation">(</span>c_id<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>hello<span class="token punctuation">)</span></code></pre><p>运行：python hello_world.py</p><p>输出：</p><pre><code>Hello world!</code></pre><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ol><li>[<a href="https://blog.csdn.net/Uwr44UOuQcNsUQb60zk2/article/details/79029868" target="_blank" rel="noopener">https://blog.csdn.net/Uwr44UOuQcNsUQb60zk2/article/details/79029868</a>)</li></ol>]]></content>
      
      
      <categories>
          
          <category> BigData </category>
          
          <category> Ray </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Ray </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Ray 简介</title>
      <link href="/2019/06/23/1-ray-introduction/"/>
      <url>/2019/06/23/1-ray-introduction/</url>
      
        <content type="html"><![CDATA[<h2 id="Ray"><a href="#Ray" class="headerlink" title="Ray"></a>Ray</h2><p><img src="https://note.youdao.com/yws/api/personal/file/WEBc55822afcb03b9602da52c65b9d9ee24?method=download&amp;shareKey=7bdea00b22b10800810cd10ff920ffd7" alt="Ray Logo"></p><p><a href="https://rise.cs.berkeley.edu/projects/ray/" target="_blank" rel="noopener">Ray</a> 是 <a href="https://rise.cs.berkeley.edu/" target="_blank" rel="noopener">UC Berkeley riseLab</a> (前身是开发 Spark/Mesos 等的AMPLab实验室) 针对机器学习领域开发的一种新的分布式计算框架。</p><p>Ray 是一个面向大规模机器学习和增强学习应用的高性能分布式执行框架。它通过在全局控制存储中抽象系统的控制状态并使所有其他组件保持无状态来实现可伸缩性和容错性。它使用共享内存分布式对象存储，通过共享内存来高效处理大数据，并使用自下而上的分层调度架构来实现低延迟和高吞吐量调度。它使用基于动态任务图和 actor 的轻量级API，以灵活的方式表达各种应用程序。</p><h2 id="Meetup"><a href="#Meetup" class="headerlink" title="Meetup"></a>Meetup</h2><p>2019.06.22 蚂蚁金服在北京举办了一场名为《下一代计算》的meetup，邀请到了Spark和Ray的核心贡献者，加州大学伯克利分校RISE实验室（前身为AMP实验室）的主管教授<a href="https://people.eecs.berkeley.edu/~istoica/" target="_blank" rel="noopener">Ion Stoica</a>。Ion 教授做了一场《Ray: Past, Present and Future》的分享。</p><p>下图是现场的照片：</p><p><img src="https://note.youdao.com/yws/api/personal/file/WEB1384b6082767c6c3c1a39719100e13b4?method=download&amp;shareKey=3028b4e9443300be94671ce9e68a6864" alt="Ion Stoica Ray prentation"></p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ol><li><a href="https://rise.cs.berkeley.edu/projects/ray/" target="_blank" rel="noopener">rise lab</a></li><li><a href="https://github.com/ray-project/ray" target="_blank" rel="noopener">Ray GitHub</a></li><li><a href="https://ray.readthedocs.io/en/latest/index.html" target="_blank" rel="noopener">Ray Docs</a></li><li><a href="https://arxiv.org/abs/1712.05889" target="_blank" rel="noopener"><strong>Ray 论文：Ray: A Distributed Framework for Emerging AI Applications</strong></a></li><li><a href="https://arxiv.org/abs/1703.03924" target="_blank" rel="noopener"><strong>Ray 论文：Real-Time Machine Learning: The Missing Pieces</strong></a></li><li><a href="https://www.oreilly.com/ideas/ray-a-distributed-execution-framework-for-emerging-ai-applications" target="_blank" rel="noopener">Michael Jordan介 绍 Ray 的视频</a></li><li><a href="https://www.youtube.com/watch?v=Ayc0ca150HI" target="_blank" rel="noopener"><strong>Ray for Reinforcement Learning | Data Council SF ‘19, Robert Nishihara 演讲</strong></a></li><li><a href="https://www.youtube.com/watch?v=oD6mvWpa1J4" target="_blank" rel="noopener"><strong>RISE Camp 2018 03 - Ray: Distributed Execution Framework for Emerging AI, Robert Nishihara 演讲</strong></a></li><li><a href="https://www.youtube.com/watch?v=D_oz7E4v-U0" target="_blank" rel="noopener"><strong>Ray: A Distributed Execution Framework for AI | SciPy 2018 | Robert Nishihara 演讲</strong></a></li><li><a href="https://ray-project.github.io/" target="_blank" rel="noopener">Ray Blog</a></li><li><a href="https://rise.cs.berkeley.edu/blog/pandas-on-ray/" target="_blank" rel="noopener">Ray 作者博客</a></li><li><a href="https://bair.berkeley.edu/blog/2018/01/09/ray/" target="_blank" rel="noopener">Ray: A Distributed System for AI</a></li><li><a href="https://www.jianshu.com/p/a5f8665d84ff" target="_blank" rel="noopener">Ray - 面向增强学习场景的分布式计算框架</a></li><li><a href="https://blog.csdn.net/Uwr44UOuQcNsUQb60zk2/article/details/79029868" target="_blank" rel="noopener">伯克利AI分布式框架Ray，兼容TensorFlow、PyTorch与MXNet</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> BigData </category>
          
          <category> Ray </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Ray </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>不连续00和111问题</title>
      <link href="/2019/03/31/1-interview-00-111/"/>
      <url>/2019/03/31/1-interview-00-111/</url>
      
        <content type="html"><![CDATA[<h2 id="题目"><a href="#题目" class="headerlink" title="题目"></a>题目</h2><pre class=" language-text"><code class="language-text">有一个19位0和1组成的数字串，第一位和最后一位都是0，中间的17位数字可以是0或者1。但不能是两个连续的0或3个连续的1，问这样的19位数字串共有多少组？</code></pre><h2 id="解题方法"><a href="#解题方法" class="headerlink" title="解题方法"></a>解题方法</h2><h3 id="数学方法"><a href="#数学方法" class="headerlink" title="数学方法"></a>数学方法</h3><p>根据题目，除了第一位，后面的18位，只能是 10 或 110 两种连续数字串的排列组合。<br>于是就转变为这样一个数学问题：有18个位置，拿 10 或 110 两种数字串填充满18个位置，共有多少种排列组合？</p><p>共有以下几种方式：</p><ol><li>只拿 10 填充，即有 1 种方式；</li><li>只拿 110 填充，即有 1 种方式；</li><li>拿 3 个 10 和 4 个 110 做排列组合，即有 $C^3_7 = 35$ 种方式；</li><li>拿 6 个 10 和 2 个 110 做排列组合，即有 $C^2_8 = 28$ 种方式</li></ol><p>总计：$1 + 1 + 35 + 28 = 65$ 种</p><h3 id="动态规划"><a href="#动态规划" class="headerlink" title="动态规划"></a>动态规划</h3><p>根据题目，除了第一位，后面的18位，只能由 10 或 110 两种连续数字串的组成。那么，当前的组数正好等于往前2位和往前3位两种组数的和。于是，递归公式如下：</p><p>$f(x+3) = f(x+1) + f(x)$</p><p>利用从低往上的计算方式：</p><pre><code>f(1) = 0;f(2) = 1;f(3) = 1;f(4) = 1;f(5) = 2;f(6) = 2;f(7) = 3;f(8) = 4;f(9) = 5;f(10) = 7;f(11) = 9;f(12) = 12;f(13) = 16;f(14) = 21;f(15) = 28;f(16) = 37;f(17) = 49;f(18) = 65;</code></pre><p>所以，最终的答案是 65 种。</p>]]></content>
      
      
      <categories>
          
          <category> Interview </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Notes </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Java 使用 protobuf</title>
      <link href="/2019/01/31/40-java-protobuf/"/>
      <url>/2019/01/31/40-java-protobuf/</url>
      
        <content type="html"><![CDATA[<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>Protocol buffers are Google’s language-neutral, platform-neutral, extensible mechanism for serializing structured data – think XML, but smaller, faster, and simpler. You define how you want your data to be structured once, then you can use special generated source code to easily write and read your structured data to and from a variety of data streams and using a variety of languages.</p><h2 id="mac-安装-protobuf"><a href="#mac-安装-protobuf" class="headerlink" title="mac 安装 protobuf"></a>mac 安装 protobuf</h2><pre class=" language-shell"><code class="language-shell">brew install protobuf</code></pre><p>安装后：</p><pre class=" language-shell"><code class="language-shell">protoc --version</code></pre><p>输出：</p><pre><code>libprotoc 3.6.1</code></pre><h2 id="定义-proto-文件"><a href="#定义-proto-文件" class="headerlink" title="定义 proto 文件"></a>定义 proto 文件</h2><p>person.proto</p><pre class=" language-proto"><code class="language-proto">syntax = "proto3";option java_package = "com.dragon.bermaker";option java_outer_classname = "PersonModel";message Person {     int32 id = 1;     string name = 2;     string email = 3;}</code></pre><h2 id="生成-Java-文件"><a href="#生成-Java-文件" class="headerlink" title="生成 Java 文件"></a>生成 Java 文件</h2><p>执行：</p><pre class=" language-shell"><code class="language-shell">protoc -I=$SRC_DIR --java_out=$DST_DIR $SRC_DIR/person.proto</code></pre><h2 id="测试-protobuf"><a href="#测试-protobuf" class="headerlink" title="测试 protobuf"></a>测试 protobuf</h2><p>maven pom 文件添加依赖：</p><pre class=" language-xml"><code class="language-xml"><span class="token comment" spellcheck="true">&lt;!-- protobuf --></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>dependency</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>groupId</span><span class="token punctuation">></span></span>com.google.protobuf<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>groupId</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>artifactId</span><span class="token punctuation">></span></span>protobuf-java<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>artifactId</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>version</span><span class="token punctuation">></span></span>${protobuf.version}<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>version</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>dependency</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>dependency</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>groupId</span><span class="token punctuation">></span></span>com.google.protobuf<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>groupId</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>artifactId</span><span class="token punctuation">></span></span>protobuf-java-util<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>artifactId</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>version</span><span class="token punctuation">></span></span>${protobuf.version}<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>version</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>dependency</span><span class="token punctuation">></span></span></code></pre><p>测试代码：</p><pre class=" language-java"><code class="language-java"><span class="token comment" spellcheck="true">/* * Copyright (C) 2019 xxx, Inc. All Rights Reserved. */</span><span class="token keyword">package</span> com<span class="token punctuation">.</span>dragon<span class="token punctuation">.</span>bermaker<span class="token punctuation">.</span>bean<span class="token punctuation">;</span><span class="token keyword">import</span> org<span class="token punctuation">.</span>junit<span class="token punctuation">.</span>Test<span class="token punctuation">;</span><span class="token keyword">import</span> com<span class="token punctuation">.</span>google<span class="token punctuation">.</span>protobuf<span class="token punctuation">.</span>InvalidProtocolBufferException<span class="token punctuation">;</span><span class="token keyword">import</span> com<span class="token punctuation">.</span>google<span class="token punctuation">.</span>protobuf<span class="token punctuation">.</span>util<span class="token punctuation">.</span>JsonFormat<span class="token punctuation">;</span><span class="token comment" spellcheck="true">/** * @ClassName: PersonModelTest * @Project: dragon-wing * @Description: TODO * @Author: bermaker * @Date: 2019/1/30 5:49 PM * @Version: 1.0 */</span><span class="token keyword">public</span> <span class="token keyword">class</span> <span class="token class-name">PersonModelTest</span> <span class="token punctuation">{</span>    <span class="token annotation punctuation">@Test</span>    <span class="token keyword">public</span> <span class="token keyword">void</span> <span class="token function">testProtobuf</span><span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>        PersonModel<span class="token punctuation">.</span>Person<span class="token punctuation">.</span>Builder builder <span class="token operator">=</span> PersonModel<span class="token punctuation">.</span>Person<span class="token punctuation">.</span><span class="token function">newBuilder</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        builder<span class="token punctuation">.</span><span class="token function">setId</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        builder<span class="token punctuation">.</span><span class="token function">setName</span><span class="token punctuation">(</span><span class="token string">"tiantian"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        builder<span class="token punctuation">.</span><span class="token function">setEmail</span><span class="token punctuation">(</span><span class="token string">"tiantian@qq.com"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        PersonModel<span class="token punctuation">.</span>Person person <span class="token operator">=</span> builder<span class="token punctuation">.</span><span class="token function">build</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        System<span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span><span class="token string">"Before: "</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        System<span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span>person<span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token keyword">byte</span><span class="token punctuation">[</span><span class="token punctuation">]</span> buff <span class="token operator">=</span> person<span class="token punctuation">.</span><span class="token function">toByteArray</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        System<span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span><span class="token string">"======================================================================="</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token keyword">for</span> <span class="token punctuation">(</span><span class="token keyword">byte</span> b <span class="token operator">:</span> buff<span class="token punctuation">)</span> <span class="token punctuation">{</span>            System<span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">print</span><span class="token punctuation">(</span>b<span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token punctuation">}</span>        System<span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        System<span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span><span class="token string">"Buf length="</span> <span class="token operator">+</span> buff<span class="token punctuation">.</span>length<span class="token punctuation">)</span><span class="token punctuation">;</span>        System<span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span><span class="token string">"======================================================================="</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        PersonModel<span class="token punctuation">.</span>Person personParsed <span class="token operator">=</span> null<span class="token punctuation">;</span>        <span class="token keyword">try</span> <span class="token punctuation">{</span>            personParsed <span class="token operator">=</span> PersonModel<span class="token punctuation">.</span>Person<span class="token punctuation">.</span><span class="token function">parseFrom</span><span class="token punctuation">(</span>buff<span class="token punctuation">)</span><span class="token punctuation">;</span>            System<span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span><span class="token string">"After: "</span><span class="token punctuation">)</span><span class="token punctuation">;</span>            System<span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span>personParsed<span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token punctuation">}</span> <span class="token keyword">catch</span> <span class="token punctuation">(</span><span class="token class-name">InvalidProtocolBufferException</span> e<span class="token punctuation">)</span> <span class="token punctuation">{</span>            e<span class="token punctuation">.</span><span class="token function">printStackTrace</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token punctuation">}</span>        <span class="token comment" spellcheck="true">// 转换成json</span>        String json <span class="token operator">=</span> <span class="token string">""</span><span class="token punctuation">;</span>        <span class="token keyword">try</span> <span class="token punctuation">{</span>            json <span class="token operator">=</span> JsonFormat<span class="token punctuation">.</span><span class="token function">printer</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">print</span><span class="token punctuation">(</span>builder<span class="token punctuation">)</span><span class="token punctuation">;</span>            System<span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span>json<span class="token punctuation">.</span><span class="token function">toString</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span>            System<span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span><span class="token string">"Json length="</span> <span class="token operator">+</span> json<span class="token punctuation">.</span><span class="token function">getBytes</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>length<span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token punctuation">}</span> <span class="token keyword">catch</span> <span class="token punctuation">(</span><span class="token class-name">InvalidProtocolBufferException</span> e<span class="token punctuation">)</span> <span class="token punctuation">{</span>            e<span class="token punctuation">.</span><span class="token function">printStackTrace</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token punctuation">}</span>    <span class="token punctuation">}</span><span class="token punctuation">}</span></code></pre><p>输出：</p><pre><code>Before: id: 1name: &quot;tiantian&quot;email: &quot;tiantian@qq.com&quot;=======================================================================811881161059711011610597110261811610597110116105971106498971051001174699111109Buf length=32=======================================================================After: id: 1name: &quot;tiantian&quot;email: &quot;tiantian@qq.com&quot;{  &quot;id&quot;: 1,  &quot;name&quot;: &quot;tiantian&quot;,  &quot;email&quot;: &quot;tiantian@qq.com&quot;}Json length=68</code></pre>]]></content>
      
      
      <categories>
          
          <category> Java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Java </tag>
            
            <tag> Protobuf </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hands-on-ML Classification</title>
      <link href="/2018/12/09/39-classification-hands-on-machine-learning-notes/"/>
      <url>/2018/12/09/39-classification-hands-on-machine-learning-notes/</url>
      
        <content type="html"><![CDATA[<p>In Chapter 1 we mentioned that the most common supervised learning tasks are regression (predicting values) and classification (predicting classes). In Chapter 2 we explored a regression task, predicting housing values, using various algorithms such as Linear Regression, Decision Trees, and Random Forests (which will be explained in further detail in later chapters). Now we will turn our attention to classification systems.</p><h2 id="MNIST"><a href="#MNIST" class="headerlink" title="MNIST"></a>MNIST</h2><p>In this chapter, we will be using the MNIST dataset, which is a set of 70,000 small images of digits handwritten by high school students and employees of the US Cen‐ sus Bureau. Each image is labeled with the digit it represents. This set has been stud‐ ied so much that it is often called the “Hello World” of Machine Learning: whenever people come up with a new classification algorithm, they are curious to see how it will perform on MNIST. Whenever someone learns Machine Learning, sooner or later they tackle MNIST.</p><p><img src="https://ws4.sinaimg.cn/large/006tKfTcly1g0gi50w5vnj30u00u0mzu.jpg" alt="A few digits from the MNIST dataset"></p><h2 id="Training-a-Binary-Classifier"><a href="#Training-a-Binary-Classifier" class="headerlink" title="Training a Binary Classifier"></a>Training a Binary Classifier</h2><p>Let’s simplify the problem for now and only try to identify one digit—for example, the number 5. This “5-detector” will be an example of a binary classifier, capable of distinguishing between just two classes, 5 and not-5. Let’s create the target vectors for this classification task:</p><pre class=" language-python"><code class="language-python">y_train_5 <span class="token operator">=</span> <span class="token punctuation">(</span>y_train <span class="token operator">==</span> <span class="token number">5</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># True for all 5s, False for all other digits.</span>y_test_5 <span class="token operator">=</span> <span class="token punctuation">(</span>y_test <span class="token operator">==</span> <span class="token number">5</span><span class="token punctuation">)</span></code></pre><h2 id="Performance-Measures"><a href="#Performance-Measures" class="headerlink" title="Performance Measures"></a>Performance Measures</h2><p>Evaluating a classifier is often significantly trickier than evaluating a regressor, so we will spend a large part of this chapter on this topic. There are many performance measures available, so grab another coffee and get ready to learn many new concepts and acronyms!</p><h3 id="Measuring-Accuracy-Using-Cross-Validation"><a href="#Measuring-Accuracy-Using-Cross-Validation" class="headerlink" title="Measuring Accuracy Using Cross-Validation"></a>Measuring Accuracy Using Cross-Validation</h3><p>A good way to evaluate a model is to use cross-validation, just as you did in Chapter 2.</p><p>This demonstrates why accuracy is generally not the preferred performance measure for classifiers, especially when you are dealing with skewed datasets (i.e., when some classes are much more frequent than others).</p><h3 id="Confusion-Matrix"><a href="#Confusion-Matrix" class="headerlink" title="Confusion Matrix"></a>Confusion Matrix</h3><p>A much better way to evaluate the performance of a classifier is to look at the confu‐ sion matrix. The general idea is to count the number of times instances of class A are classified as class B. For example, to know the number of times the classifier confused images of 5s with 3s, you would look in the 5th row and 3rd column of the confusion matrix.</p><p><img src="https://ws4.sinaimg.cn/large/006tKfTcly1g0gi8pa6szj310i0jcjvr.jpg" alt=""></p><h3 id="Precision-and-Recall"><a href="#Precision-and-Recall" class="headerlink" title="Precision and Recall"></a>Precision and Recall</h3><p>$precision=\frac{TP}{TP+FP}​$</p><p>$recall=\frac{TP}{TP+FN}$</p><p>$F_1=\frac{2}{\frac{1}{precision}+\frac{1}{recall}}​$</p><h3 id="Precision-Recall-Tradeoff"><a href="#Precision-Recall-Tradeoff" class="headerlink" title="Precision/Recall Tradeoff"></a>Precision/Recall Tradeoff</h3><p><img src="https://ws1.sinaimg.cn/large/006tKfTcly1g0gicmdbkbj310c0d8gox.jpg" alt=""></p><p><img src="https://ws3.sinaimg.cn/large/006tKfTcly1g0gid0crvdj31o00u0dir.jpg" alt="Precision and recall versus the decision threshold"></p><p><img src="https://ws2.sinaimg.cn/large/006tKfTcly1g0gidxl67mj31400u0tas.jpg" alt="Precision versus recall"></p><h3 id="The-ROC-Curve"><a href="#The-ROC-Curve" class="headerlink" title="The ROC Curve"></a>The ROC Curve</h3><p><img src="https://ws4.sinaimg.cn/large/006tKfTcly1g0gif03ifdj31400u0whu.jpg" alt="ROC curve"></p><h2 id="Multiclass-Classification"><a href="#Multiclass-Classification" class="headerlink" title="Multiclass Classification"></a>Multiclass Classification</h2><p>Whereas binary classifiers distinguish between two classes, multiclass classifiers (also called multinomial classifiers) can distinguish between more than two classes.</p><p>Some algorithms (such as Random Forest classifiers or naive Bayes classifiers) are capable of handling multiple classes directly. Others (such as Support Vector Machine classifiers or Linear classifiers) are strictly binary classifiers. However, there are vari‐ ous strategies that you can use to perform multiclass classification using multiple binary classifiers.</p><ul><li>OvA: one-versus-all</li><li>OvO: one-versus-one</li></ul><p>Some algorithms (such as Support Vector Machine classifiers) scale poorly with the size of the training set, so for these algorithms OvO is preferred since it is faster to train many classifiers on small training sets than training few classifiers on large training sets. For most binary classification algorithms, however, OvA is preferred.</p><h2 id="Error-Analysis"><a href="#Error-Analysis" class="headerlink" title="Error Analysis"></a>Error Analysis</h2><p>Of course, if this were a real project, you would follow the steps in your Machine Learning project checklist (see Appendix B): exploring data preparation options, try‐ ing out multiple models, shortlisting the best ones and fine-tuning their hyperpara‐ meters using GridSearchCV, and automating as much as possible, as you did in the previous chapter. Here, we will assume that you have found a promising model and you want to find ways to improve it. One way to do this is to analyze the types of errors it makes.</p><p><img src="https://ws1.sinaimg.cn/large/006tKfTcly1g0gigwracxj30u00u0q3c.jpg" alt="Confusion matrix"><img src="https://ws3.sinaimg.cn/large/006tKfTcly1g0gihb8gabj30u00u0t94.jpg" alt="Confusion matrix errors"></p><h2 id="Multilabel-Classification"><a href="#Multilabel-Classification" class="headerlink" title="Multilabel Classification"></a>Multilabel Classification</h2><p>Until now each instance has always been assigned to just one class. In some cases you may want your classifier to output multiple classes for each instance. For example, consider a facerecognition classifier: what should it do if it recognizes several people on the same picture? Of course it should attach one label per person it recognizes. Say the classifier has been trained to recognize three faces, Alice, Bob, and Charlie; then when it is shown a picture of Alice and Charlie, it should output [1, 0, 1] (meaning “Alice yes, Bob no, Charlie yes”). Such a classification system that outputs multiple binary labels is called a multilabel classification system.</p><h2 id="Multioutput-Classification"><a href="#Multioutput-Classification" class="headerlink" title="Multioutput Classification"></a>Multioutput Classification</h2><p>The last type of classification task we are going to discuss here is called multioutput- multiclass classification (or simply multioutput classification). It is simply a generaliza‐ tion of multilabel classification where each label can be multiclass (i.e., it can have more than two possible values).</p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol><li><a href="http://shop.oreilly.com/product/0636920052289.do" target="_blank" rel="noopener">Hands-On Machine Learning with Scikit-Learn and TensorFlow</a></li><li><a href="https://www.oreilly.com/catalog/errata.csp?isbn=0636920052289" target="_blank" rel="noopener">Errata for Hands-On Machine Learning with Scikit-Learn and TensorFlow</a></li><li><a href="https://github.com/ageron/handson-ml" target="_blank" rel="noopener">GitHub: Hands-On Machine Learnging With Scikit-Learn and TensorFlow</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Notes </tag>
            
            <tag> Machine Learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hands-on-ML End-to-End Machine Learning Project</title>
      <link href="/2018/12/08/38-end-to-end-machine-learning-project-hands-on-mahcine-learning-notes/"/>
      <url>/2018/12/08/38-end-to-end-machine-learning-project-hands-on-mahcine-learning-notes/</url>
      
        <content type="html"><![CDATA[<h1 id="End-to-End-Machine-Learning-Project"><a href="#End-to-End-Machine-Learning-Project" class="headerlink" title="End-to-End Machine Learning Project"></a>End-to-End Machine Learning Project</h1><p>[TOC]</p><p>In this chapter, you will go through an example project end to end, pretending to be a recently hired data scientist in a real estate company.1 Here are the main steps you will go through:</p><pre class=" language-text"><code class="language-text">1. Look at the big picture.2. Get the data.3. Discover and visualize the data to gain insights.4. Prepare the data for Machine Learning algorithms.5. Select a model and train it.6. Fine-tune your model.7. Present your solution.8. Launch, monitor, and maintain your system.</code></pre><h2 id="Working-with-Real-Data"><a href="#Working-with-Real-Data" class="headerlink" title="Working with Real Data"></a>Working with Real Data</h2><p>When you are learning about Machine Learning it is best to actually experiment with real-world data, not just artificial datasets. Fortunately, there are thousands of open datasets to choose from, ranging across all sorts of domains. Here are a few places you can look to get data:</p><ul><li><p>Popular open data repositories:</p><ul><li><p><a href="http://archive.ics.uci.edu/ml/" target="_blank" rel="noopener">UC Irvine Machine Learning Repository</a></p></li><li><p><a href="https://www.kaggle.com/datasets" target="_blank" rel="noopener">Kaggle datasets</a></p></li><li><p><a href="http://aws.amazon.com/fr/datasets/" target="_blank" rel="noopener">Amazon’s AWS datasets</a></p></li></ul></li><li><p>Meta portals (they list open data repositories):</p><ul><li><a href="http://dataportals.org/" target="_blank" rel="noopener">http://dataportals.org/</a></li><li><a href="http://opendatamonitor.eu/" target="_blank" rel="noopener">http://opendatamonitor.eu/</a></li><li><a href="http://quandl.com/" target="_blank" rel="noopener">http://quandl.com/</a></li></ul></li><li>Other pages listing many popular open data repositories:<ul><li><a href="https://goo.gl/SJHN2k" target="_blank" rel="noopener">Wikipedia’s list of Machine Learning datasets</a></li><li><a href="http://goo.gl/zDR78y" target="_blank" rel="noopener">Quora.com question</a></li><li><a href="https://www.reddit.com/r/datasets" target="_blank" rel="noopener">Datasets subreddit</a></li></ul></li></ul><p>In this chapter we chose the California Housing Prices dataset from the StatLib repos‐ itory2 (see Figure 2-1). This dataset was based on data from the 1990 California cen‐ sus. It is not exactly recent (you could still afford a nice house in the Bay Area at the time), but it has many qualities for learning, so we will pretend it is recent data. We also added a categorical attribute and removed a few features for teaching purposes.</p><p><img src="https://ws4.sinaimg.cn/large/006tKfTcly1g0ghdvv3kbj310i0lkqes.jpg" alt="California hoursing prices"></p><h2 id="Look-at-the-Big-Picture"><a href="#Look-at-the-Big-Picture" class="headerlink" title="Look at the Big Picture"></a>Look at the Big Picture</h2><p>Since you are a well-organized data scientist, the first thing you do is to pull out your Machine Learning project checklist. You can start with the one in Appendix B; it should work reasonably well for most Machine Learning projects but make sure to adapt it to your needs. In this chapter we will go through many checklist items, but we will also skip a few, either because they are self- explanatory or because they will be discussed in later chapters.</p><h3 id="Frame-the-Problem"><a href="#Frame-the-Problem" class="headerlink" title="Frame the Problem"></a>Frame the Problem</h3><p>The first question to ask your boss is what exactly is the business objective; building a model is probably not the end goal. How does the company expect to use and benefit from this model? This is important because it will determine how you frame the problem, what algorithms you will select, what performance measure you will use to evaluate your model, and how much effort you should spend tweaking it.</p><h3 id="Select-a-Performance-Measure"><a href="#Select-a-Performance-Measure" class="headerlink" title="Select a Performance Measure"></a>Select a Performance Measure</h3><p>Your next step is to select a performance measure. A typical performance measure for regression problems is the Root Mean Square Error (RMSE). It measures the standard deviation4 of the errors the system makes in its predictions. For example, an RMSE equal to 50,000 means that about 68% of the system’s predictions fall within $50,000 of the actual value, and about 95% of the predictions fall within $100,000 of the actual value.</p><p>###Check the Assumptions</p><p>Lastly, it is good practice to list and verify the assumptions that were made so far (by you or others); this can catch serious issues early on. For example, the district prices that your system outputs are going to be fed into a downstream Machine Learning system, and we assume that these prices are going to be used as such. But what if the downstream system actually converts the prices into categories (e.g., “cheap,” “medium,” or “expensive”) and then uses those categories instead of the prices them‐ selves? In this case, getting the price perfectly right is not important at all; your sys‐ tem just needs to get the category right. If that’s so, then the problem should have been framed as a classification task, not a regression task. You don’t want to find this out after working on a regression system for months.</p><p>Fortunately, after talking with the team in charge of the downstream system, you are confident that they do indeed need the actual prices, not just categories. Great! You’re all set, the lights are green, and you can start coding now!</p><h2 id="Get-the-Data"><a href="#Get-the-Data" class="headerlink" title="Get the Data"></a>Get the Data</h2><p>###Create the Workspace</p><p>First you will need to have Python installed. It is probably already installed on your system. If not, you can get it at <a href="https://www.python.org/" target="_blank" rel="noopener">https://www.python.org/</a>.</p><h3 id="Download-the-Data"><a href="#Download-the-Data" class="headerlink" title="Download the Data"></a>Download the Data</h3><p>In typical environments your data would be available in a relational database (or some other common datastore) and spread across multiple tables/documents/files.</p><h3 id="Take-a-Quick-Look-at-the-Data-Structure"><a href="#Take-a-Quick-Look-at-the-Data-Structure" class="headerlink" title="Take a Quick Look at the Data Structure"></a>Take a Quick Look at the Data Structure</h3><p><img src="https://ws2.sinaimg.cn/large/006tKfTcly1g0ghm3we46j31400u0alu.jpg" alt="Attribute histogram"></p><h3 id="Create-a-Test-Set"><a href="#Create-a-Test-Set" class="headerlink" title="Create a Test Set"></a>Create a Test Set</h3><p>It may sound strange to voluntarily set aside part of the data at this stage. After all, you have only taken a quick glance at the data, and surely you should learn a whole lot more about it before you decide what algorithms to use, right? This is true, but your brain is an amazing pattern detection system, which means that it is highly prone to overfitting: if you look at the test set, you may stumble upon some seemingly interesting pattern in the test data that leads you to select a particular kind of Machine Learning model. When you estimate the generalization error using the test set, your estimate will be too optimistic and you will launch a system that will not perform as well as expected. This is called data snooping bias.</p><h2 id="Discover-and-Visualize-the-Data-to-Gain-Insights"><a href="#Discover-and-Visualize-the-Data-to-Gain-Insights" class="headerlink" title="Discover and Visualize the Data to Gain Insights"></a>Discover and Visualize the Data to Gain Insights</h2><h3 id="Visualizing-Geographical-Data"><a href="#Visualizing-Geographical-Data" class="headerlink" title="Visualizing Geographical Data"></a>Visualizing Geographical Data</h3><p><img src="https://ws1.sinaimg.cn/large/006tKfTcly1g0ghkz5mxaj316v0u07wh.jpg" alt="California Housing Orices Plot"></p><h3 id="Looking-for-Correlations"><a href="#Looking-for-Correlations" class="headerlink" title="Looking for Correlations"></a>Looking for Correlations</h3><p><img src="https://ws3.sinaimg.cn/large/006tKfTcly1g0ghmosg9fj310i0jg7bf.jpg" alt="Standard correlation coefficient of various datasets"></p><p>The correlation coefficient only measures linear correlations (“if x goes up, then y generally goes up/down”). It may completely miss out on nonlinear relationships (e.g., “if x is close to zero then y gen‐ erally goes up”). Note how all the plots of the bottom row have a correlation coefficient equal to zero despite the fact that their axes are clearly not independent: these are examples of nonlinear rela‐ tionships. Also, the second row shows examples where the correlation coefficient is equal to 1 or –1; notice that this has nothing to do with the slope. For example, your height in inches has a correla‐ tion coefficient of 1 with your height in feet or in nanometers.</p><h3 id="Experimenting-with-Attribute-Combinations"><a href="#Experimenting-with-Attribute-Combinations" class="headerlink" title="Experimenting with Attribute Combinations"></a>Experimenting with Attribute Combinations</h3><p>Hopefully the previous sections gave you an idea of a few ways you can explore the data and gain insights. You identified a few data quirks that you may want to clean up before feeding the data to a Machine Learning algorithm, and you found interesting correlations between attributes, in particular with the target attribute. You also noticed that some attributes have a tail-heavy distribution, so you may want to trans‐ form them (e.g., by computing their logarithm). Of course, your mileage will vary considerably with each project, but the general ideas are similar.</p><p>This round of exploration does not have to be absolutely thorough; the point is to start off on the right foot and quickly gain insights that will help you get a first rea‐ sonably good prototype. But this is an iterative process: once you get a prototype up and running, you can analyze its output to gain more insights and come back to this exploration step.</p><h2 id="Prepare-the-Data-for-Machine-Learning-Algorithms"><a href="#Prepare-the-Data-for-Machine-Learning-Algorithms" class="headerlink" title="Prepare the Data for Machine Learning Algorithms"></a>Prepare the Data for Machine Learning Algorithms</h2><p>###Data Cleaning</p><p>Most Machine Learning algorithms cannot work with missing features, so let’s create a few functions to take care of them. You noticed earlier that the total_bedrooms attribute has some missing values, so let’s fix this. You have three options:</p><ul><li><p>Get rid of the corresponding districts.</p></li><li><p>Get rid of the whole attribute.</p></li><li><p>Set the values to some value (zero, the mean, the median, etc.).</p></li></ul><h3 id="Handling-Text-and-Categorical-Attributes"><a href="#Handling-Text-and-Categorical-Attributes" class="headerlink" title="Handling Text and Categorical Attributes"></a>Handling Text and Categorical Attributes</h3><p>Earlier we left out the categorical attribute ocean_proximity because it is a text attribute so we cannot compute its median. Most Machine Learning algorithms pre‐ fer to work with numbers anyway, so let’s convert these text labels to numbers.</p><h3 id="Custom-Transformers"><a href="#Custom-Transformers" class="headerlink" title="Custom Transformers"></a>Custom Transformers</h3><p>Although Scikit-Learn provides many useful transformers, you will need to write your own for tasks such as custom cleanup operations or combining specific attributes. You will want your transformer to work seamlessly with Scikit-Learn func‐ tionalities (such as pipelines), and since Scikit-Learn relies on duck typing (not inher‐ itance), all you need is to create a class and implement three methods: fit() (returning self), transform(), and fit_transform().</p><h3 id="Feature-Scaling"><a href="#Feature-Scaling" class="headerlink" title="Feature Scaling"></a>Feature Scaling</h3><p>One of the most important transformations you need to apply to your data is feature scaling. With few exceptions, Machine Learning algorithms don’t perform well when the input numerical attributes have very different scales. This is the case for the hous‐ ing data: the total number of rooms ranges from about 6 to 39,320, while the median incomes only range from 0 to 15. Note that scaling the target values is generally not required.</p><p>There are two common ways to get all attributes to have the same scale: min-max scaling and standardization.</p><p>Min-max scaling (many people call this normalization) is quite simple: values are shifted and rescaled so that they end up ranging from 0 to 1. We do this by subtracting the min value and dividing by the max minus the min. Scikit-Learn provides a transformer called MinMaxScaler for this. It has a feature_range hyperparameter that lets you change the range if you don’t want 0–1 for some reason.</p><p>Standardization is quite different: first it subtracts the mean value (so standardized values always have a zero mean), and then it divides by the variance so that the resulting distribution has unit variance. Unlike min-max scaling, standardization does not bound values to a specific range, which may be a problem for some algorithms (e.g., neural networks often expect an input value ranging from 0 to 1). However, standardization is much less affected by outliers. For example, suppose a district had a median income equal to 100 (by mistake). Min-max scaling would then crush all the other values from 0–15 down to 0–0.15, whereas standardization would not be much affec‐ ted. Scikit-Learn provides a transformer called StandardScaler for standardization.</p><h3 id="Transformation-Pipelines"><a href="#Transformation-Pipelines" class="headerlink" title="Transformation Pipelines"></a>Transformation Pipelines</h3><p>As you can see, there are many data transformation steps that need to be executed in the right order. Fortunately, Scikit-Learn provides the Pipeline class to help with such sequences of transformations.</p><h2 id="Select-and-Train-a-Model"><a href="#Select-and-Train-a-Model" class="headerlink" title="Select and Train a Model"></a>Select and Train a Model</h2><p>At last! You framed the problem, you got the data and explored it, you sampled a training set and a test set, and you wrote transformation pipelines to clean up and prepare your data for Machine Learning algorithms automatically. You are now ready to select and train a Machine Learning model.</p><h3 id="Training-and-Evaluating-on-the-Training-Set"><a href="#Training-and-Evaluating-on-the-Training-Set" class="headerlink" title="Training and Evaluating on the Training Set"></a>Training and Evaluating on the Training Set</h3><p>Could this model really be absolutely perfect? Of course, it is much more likely that the model has badly overfit the data. How can you be sure? As we saw earlier, you don’t want to touch the test set until you are ready to launch a model you are confident about, so you need to use part of the training set for train‐ ing, and part for model validation.</p><h3 id="Better-Evaluation-Using-Cross-Validation"><a href="#Better-Evaluation-Using-Cross-Validation" class="headerlink" title="Better Evaluation Using Cross-Validation"></a>Better Evaluation Using Cross-Validation</h3><p>One way to evaluate the Decision Tree model would be to use the train_test_split function to split the training set into a smaller training set and a validation set, then train your models against the smaller training set and evaluate them against the vali‐ dation set. It’s a bit of work, but nothing too difficult and it would work fairly well.</p><h2 id="Fine-Tune-Your-Model"><a href="#Fine-Tune-Your-Model" class="headerlink" title="Fine-Tune Your Model"></a>Fine-Tune Your Model</h2><p>Let’s assume that you now have a shortlist of promising models. You now need to fine-tune them.</p><h3 id="Grid-Search"><a href="#Grid-Search" class="headerlink" title="Grid Search"></a>Grid Search</h3><p>One way to do that would be to fiddle with the hyperparameters manually, until you find a great combination of hyperparameter values. This would be very tedious work, and you may not have time to explore many combinations.</p><h3 id="Randomized-Search"><a href="#Randomized-Search" class="headerlink" title="Randomized Search"></a>Randomized Search</h3><p>The grid search approach is fine when you are exploring relatively few combinations, like in the previous example, but when the hyperparameter search space is large, it is often preferable to use RandomizedSearchCV instead. This class can be used in much the same way as the GridSearchCV class, but instead of trying out all possible combi‐ nations, it evaluates a given number of random combinations by selecting a random value for each hyperparameter at every iteration. This approach has two main benefits:</p><ul><li><p>If you let the randomized search run for, say, 1,000 iterations, this approach will explore 1,000 different values for each hyperparameter (instead of just a few values per hyperparameter with the grid search approach).</p></li><li><p>You have more control over the computing budget you want to allocate to hyperparameter search, simply by setting the number of iterations.</p></li></ul><h3 id="Ensemble-Methods"><a href="#Ensemble-Methods" class="headerlink" title="Ensemble Methods"></a>Ensemble Methods</h3><p>Another way to fine-tune your system is to try to combine the models that perform best. The group (or “ensemble”) will often perform better than the best individual model (just like Random Forests perform better than the individual Decision Trees they rely on), especially if the individual models make very different types of errors. We will cover this topic in more detail in Chapter 7.</p><h3 id="Analyze-the-Best-Models-and-Their-Errors"><a href="#Analyze-the-Best-Models-and-Their-Errors" class="headerlink" title="Analyze the Best Models and Their Errors"></a>Analyze the Best Models and Their Errors</h3><p>You will often gain good insights on the problem by inspecting the best models.</p><h3 id="Evaluate-Your-System-on-the-Test-Set"><a href="#Evaluate-Your-System-on-the-Test-Set" class="headerlink" title="Evaluate Your System on the Test Set"></a>Evaluate Your System on the Test Set</h3><p>After tweaking your models for a while, you eventually have a system that performs sufficiently well.</p><h2 id="Launch-Monitor-and-Maintain-Your-System"><a href="#Launch-Monitor-and-Maintain-Your-System" class="headerlink" title="Launch, Monitor, and Maintain Your System"></a>Launch, Monitor, and Maintain Your System</h2><p>Perfect, you got approval to launch! You need to get your solution ready for production, in particular by plugging the production input data sources into your system and writing tests.</p><p>You also need to write monitoring code to check your system’s live performance at regular intervals and trigger alerts when it drops. This is important to catch not only sudden breakage, but also performance degradation. This is quite common because models tend to “rot” as data evolves over time, unless the models are regularly trained on fresh data.</p><p>Evaluating your system’s performance will require sampling the system’s predictions and evaluating them. This will generally require a human analysis. These analysts may be field experts, or workers on a crowdsourcing platform (such as Amazon Mechanical Turk or CrowdFlower). Either way, you need to plug the human evalua‐ tion pipeline into your system.</p><p>You should also make sure you evaluate the system’s input data quality. Sometimes performance will degrade slightly because of a poor quality signal (e.g., a malfunctioning sensor sending random values, or another team’s output becoming stale), but it may take a while before your system’s performance degrades enough to trigger an alert. If you monitor your system’s inputs, you may catch this earlier. Monitoring the inputs is particularly important for online learning systems.</p><p>Finally, you will generally want to train your models on a regular basis using fresh data. You should automate this process as much as possible. If you don’t, you are very likely to refresh your model only every six months (at best), and your system’s performance may fluctuate severely over time. If your system is an online learning system, you should make sure you save snapshots of its state at regular intervals so you can easily roll back to a previously working state.</p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol><li><a href="http://shop.oreilly.com/product/0636920052289.do" target="_blank" rel="noopener">Hands-On Machine Learning with Scikit-Learn and TensorFlow</a></li><li><a href="https://www.oreilly.com/catalog/errata.csp?isbn=0636920052289" target="_blank" rel="noopener">Errata for Hands-On Machine Learning with Scikit-Learn and TensorFlow</a></li><li><a href="https://github.com/ageron/handson-ml" target="_blank" rel="noopener">GitHub: Hands-On Machine Learnging With Scikit-Learn and TensorFlow</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Notes </tag>
            
            <tag> Machine Learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hands-on-ML The Machine Learning Landscape</title>
      <link href="/2018/12/02/37-the-machine-learning-landscape-hands-on-machine-leanring-notes/"/>
      <url>/2018/12/02/37-the-machine-learning-landscape-hands-on-machine-leanring-notes/</url>
      
        <content type="html"><![CDATA[<p>Where does Machine Learning start and where does it end? What exactly does it mean for a machine to learn something? </p><h2 id="What-is-Machine-Learning"><a href="#What-is-Machine-Learning" class="headerlink" title="What is Machine Learning"></a>What is Machine Learning</h2><p>Machine Learning is the science (and art) of programming computers so they can learn from data.</p><p>Here is a slightly more general definition:</p><pre><code>    [Machine Learning is the] field of study that gives computers the ability to learn     without being explicitly programmed.        —Arthur Samuel, 1959</code></pre><p>And a more engineering-oriented one:</p><pre><code>    A computer program is said to learn from experience E with respect to some task T     and some performance measure P, if its performance on T, as measured by P, improves     with experience E.        —Tom Mitchell, 1997</code></pre><h2 id="Why-use-Machine-Learning"><a href="#Why-use-Machine-Learning" class="headerlink" title="Why use Machine Learning"></a>Why use Machine Learning</h2><p><img src="https://ws4.sinaimg.cn/large/006tNc79ly1fzn4hjlv1oj31400ja0vn.jpg" alt="Screen Shot 2019-01-29 at 7.55.13 AM"></p><p><img src="https://ws2.sinaimg.cn/large/006tNc79ly1fznthdqu5ej31400k4gph.jpg" alt="Screen Shot 2019-01-29 at 10.20.09 PM"></p><p><img src="https://ws1.sinaimg.cn/large/006tNc79ly1fznti0qg9dj31400fgwie.jpg" alt="Screen Shot 2019-01-29 at 10.20.49 PM"></p><p><img src="https://ws1.sinaimg.cn/large/006tNc79ly1fzntijgqm0j31400je798.jpg" alt="Screen Shot 2019-01-29 at 10.21.19 PM"></p><p>To summarize, Machine Learning is great for:</p><ul><li>Problems for which existing solutions require a lot of hand-tuning or long lists of rules: one Machine Learning algorithm can often simplify code and perform bet ter.</li><li>Complex problems for which there is no good solution at all using a traditional approach: the best Machine Learning techniques can find a solution.</li><li>Fluctuating environments: a Machine Learning system can adapt to new data.</li><li>Getting insights about complex problems and large amounts of data.</li></ul><h2 id="Types-of-Machine-Learning-Systems"><a href="#Types-of-Machine-Learning-Systems" class="headerlink" title="Types of Machine Learning Systems"></a>Types of Machine Learning Systems</h2><p>There are so many different types of Machine Learning systems that it is useful to classify them in broad categories based on:</p><ul><li>Whether or not they are trained with human supervision (supervised, unsuper‐ vised, semisupervised, and Reinforcement Learning)</li><li>Whether or not they can learn incrementally on the fly (online versus batch learning)</li><li>Whether they work by simply comparing new data points to known data points, or instead detect patterns in the training data and build a predictive model, much like scientists do (instance-based versus model-based learning)</li></ul><h3 id="Supervised-Unsupervised-Learning"><a href="#Supervised-Unsupervised-Learning" class="headerlink" title="Supervised/Unsupervised Learning"></a>Supervised/Unsupervised Learning</h3><p>Machine Learning systems can be classified according to the amount and type of supervision they get during training. There are four major categories: supervised learning, unsupervised learning, semisupervised learning, and Reinforcement learning.</p><h4 id="Supervised-learning"><a href="#Supervised-learning" class="headerlink" title="Supervised learning"></a>Supervised learning</h4><p>In supervised learning, the training data you feed to the algorithm includes the desired solutions, called labels (Figure 1-5).</p><p><img src="https://ws2.sinaimg.cn/large/006tNc79ly1fzlz1i60q3j326t0u0n3p.jpg" alt="1548632887379"></p><p>A typical supervised learning task is classification. The spam filter is a good example of this: it is trained with many example emails along with their class (spam or ham), and it must learn how to classify new emails.</p><p>Another typical task is to predict a target numeric value, such as the price of a car, given a set of features (mileage, age, brand, etc.) called predictors. This sort of task is called regression (Figure 1-6). To train the system, you need to give it many examples of cars, including both their predictors and their labels (i.e., their prices).</p><p><img src="https://ws4.sinaimg.cn/large/006tNc79ly1fzn410gk2tj31400mi79c.jpg" alt="Screen Shot 2019-01-29 at 7.39.14 AM"></p><p>Here are some of the most important supervised learning algorithms (covered in this book):</p><ul><li>k-Nearest Neighbors</li><li>Linear Regression</li><li>Logistic Regression</li><li>Support Vector Machines (SVMs)</li><li>Decision Trees and Random Forests</li><li>Neural networks</li></ul><h4 id="Unsupervised-learning"><a href="#Unsupervised-learning" class="headerlink" title="Unsupervised learning"></a>Unsupervised learning</h4><p>In unsupervised learning, as you might guess, the training data is unlabeled (Figure 1-7). The system tries to learn without a teacher.</p><p><img src="https://ws1.sinaimg.cn/large/006tNc79ly1fzn44d9kn8j31400eggq6.jpg" alt="Screen Shot 2019-01-29 at 7.42.42 AM"></p><p>Here are some of the most important unsupervised learning algorithms (we will cover dimensionality reduction):</p><ul><li>Clustering<ul><li>k-Means</li><li>Hierarchical Cluster Analysis (HCA)</li><li>Expectation Maximization</li></ul></li><li><p>Visualization and dimensionality reduction</p><ul><li>Principal Component Analysis (PCA)</li><li>Kernel PCA</li><li>Locally-Linear Embedding (LLE)</li><li>t-distributed Stochastic Neighbor Embedding (t-SNE)</li></ul></li><li><p>Association rule learning</p><ul><li>Apriori</li><li>Eclat</li></ul></li></ul><p><img src="https://ws4.sinaimg.cn/large/006tNc79ly1fzn498dqrtj31400eoq77.jpg" alt="Screen Shot 2019-01-29 at 7.47.23 AM"></p><p><img src="https://ws3.sinaimg.cn/large/006tNc79ly1fzn49w77axj31400qmb29.jpg" alt="Screen Shot 2019-01-29 at 7.48.00 AM"></p><p><img src="https://ws3.sinaimg.cn/large/006tNc79ly1fzn4avh8lrj31400gsdiy.jpg" alt="Screen Shot 2019-01-29 at 7.48.59 AM"></p><h4 id="Semisupervised-learning"><a href="#Semisupervised-learning" class="headerlink" title="Semisupervised learning"></a>Semisupervised learning</h4><p>Some algorithms can deal with partially labeled training data, usually a lot of unla‐ beled data and a little bit of labeled data. This is called semisupervised learning (Figure 1-11).</p><p><img src="https://ws3.sinaimg.cn/large/006tNc79ly1fzn4c4g6qij31400gwn0n.jpg" alt="Screen Shot 2019-01-29 at 7.50.11 AM"></p><p>Most semisupervised learning algorithms are combinations of unsupervised and supervised algorithms. For example, deep belief networks (DBNs) are based on unsu‐ pervised components called restricted Boltzmann machines (RBMs) stacked on top of one another. RBMs are trained sequentially in an unsupervised manner, and then the whole system is fine-tuned using supervised learning techniques.</p><h4 id="Reinforcement-learning"><a href="#Reinforcement-learning" class="headerlink" title="Reinforcement learning"></a>Reinforcement learning</h4><p>Reinforcement Learning is a very different beast. The learning system, called an agent in this context, can observe the environment, select and perform actions, and get rewards in return (or penalties in the form of negative rewards, as in Figure 1-12). It must then learn by itself what is the best strategy, called a policy, to get the most reward over time. A policy defines what action the agent should choose when it is in a given situation.</p><p><img src="https://ws1.sinaimg.cn/large/006tNc79ly1fzn4e9a9ryj31400n67et.jpg" alt="Screen Shot 2019-01-29 at 7.52.13 AM"></p><h3 id="Batch-and-Online-Learning"><a href="#Batch-and-Online-Learning" class="headerlink" title="Batch and Online Learning"></a>Batch and Online Learning</h3><p>Another criterion used to classify Machine Learning systems is whether or not the system can learn incrementally from a stream of incoming data.</p><h4 id="Batch-learning"><a href="#Batch-learning" class="headerlink" title="Batch learning"></a>Batch learning</h4><p>In batch learning, the system is incapable of learning incrementally: it must be trained using all the available data. This will generally take a lot of time and computing resources, so it is typically done offline. First the system is trained, and then it is launched into production and runs without learning anymore; it just applies what it has learned. This is called offline learning.</p><p>If you want a batch learning system to know about new data (such as a new type of spam), you need to train a new version of the system from scratch on the full dataset (not just the new data, but also the old data), then stop the old system and replace it with the new one.</p><p>Fortunately, the whole process of training, evaluating, and launching a Machine Learning system can be automated fairly easily (as shown in Figure 1-3), so even a batch learning system can adapt to change. Simply update the data and train a new version of the system from scratch as often as needed.</p><p>This solution is simple and often works fine, but training using the full set of data can take many hours, so you would typically train a new system only every 24 hours or even just weekly. If your system needs to adapt to rapidly changing data (e.g., to pre‐ dict stock prices), then you need a more reactive solution.</p><p>Also, training on the full set of data requires a lot of computing resources (CPU, memory space, disk space, disk I/O, network I/O, etc.). If you have a lot of data and you automate your system to train from scratch every day, it will end up costing you a lot of money. If the amount of data is huge, it may even be impossible to use a batch learning algorithm.</p><p>Finally, if your system needs to be able to learn autonomously and it has limited resources (e.g., a smartphone application or a rover on Mars), then carrying around large amounts of training data and taking up a lot of resources to train for hours every day is a showstopper.</p><p>Fortunately, a better option in all these cases is to use algorithms that are capable of learning incrementally.</p><h4 id="Online-learning"><a href="#Online-learning" class="headerlink" title="Online learning"></a>Online learning</h4><p>In online learning, you train the system incrementally by feeding it data instances sequentially, either individually or by small groups called mini-batches. Each learning step is fast and cheap, so the system can learn about new data on the fly, as it arrives (see Figure 1-13).</p><p><img src="https://ws3.sinaimg.cn/large/006tNc79ly1fzntnc4k2qj31400iyn0k.jpg" alt="Screen Shot 2019-01-29 at 10.25.56 PM"></p><p>Online learning is great for systems that receive data as a continuous flow (e.g., stock prices) and need to adapt to change rapidly or autonomously.</p><p>Online learning algorithms can also be used to train systems on huge datasets that cannot fit in one machine’s main memory (this is called out-of-core learning). The algorithm loads part of the data, runs a training step on that data, and repeats the process until it has run on all of the data (see Figure 1-14).</p><p>This whole process is usually done offline (i.e., not on the live sys‐ tem), so online learning can be a confusing name. Think of it as incremental learning.</p><p><img src="https://ws3.sinaimg.cn/large/006tNc79ly1fzntr7wdg6j31400km0xd.jpg" alt="Screen Shot 2019-01-29 at 10.29.40 PM"></p><p>One important parameter of online learning systems is how fast they should adapt to changing data: this is called the learning rate. </p><p>A big challenge with online learning is that if bad data is fed to the system, the sys‐ tem’s performance will gradually decline.If we are talking about a live system, your clients will notice. For example, bad data could come from a malfunctioning sensor on a robot, or from someone spamming a search engine to try to rank high in search results. To reduce this risk, you need to monitor your system closely and promptly switch learning off (and possibly revert to a previously working state) if you detect a drop in performance. You may also want to monitor the input data and react to abnormal data (e.g., using an anomaly detection algorithm).</p><h3 id="Instance-Based-Versus-Model-Based-Learning"><a href="#Instance-Based-Versus-Model-Based-Learning" class="headerlink" title="Instance-Based Versus Model-Based Learning"></a>Instance-Based Versus Model-Based Learning</h3><p>One more way to categorize Machine Learning systems is by how they generalize. Most Machine Learning tasks are about making predictions. This means that given a number of training examples, the system needs to be able to generalize to examples it has never seen before. Having a good performance measure on the training data is good, but insufficient; the true goal is to perform well on new instances.</p><p>There are two main approaches to generalization: instance-based learning and model-based learning.</p><h4 id="Instance-based-learning"><a href="#Instance-based-learning" class="headerlink" title="Instance-based learning"></a>Instance-based learning</h4><p>Possibly the most trivial form of learning is simply to learn by heart. If you were to create a spam filter this way, it would just flag all emails that are identical to emails that have already been flagged by users—not the worst solution, but certainly not the best.</p><p>Instead of just flagging emails that are identical to known spam emails, your spam filter could be programmed to also flag emails that are very similar to known spam emails. This requires a measure of similarity between two emails. A (very basic) simi‐ larity measure between two emails could be to count the number of words they have in common. The system would flag an email as spam if it has many words in com‐ mon with a known spam email.</p><p>This is called instance-based learning: the system learns the examples by heart, then generalizes to new cases using a similarity measure (Figure 1-15).</p><p><img src="https://ws1.sinaimg.cn/large/006tNc79ly1fzntunu4uxj31400gk0v6.jpg" alt="Screen Shot 2019-01-29 at 10.32.58 PM"></p><h4 id="Model-based-learning"><a href="#Model-based-learning" class="headerlink" title="Model-based learning"></a>Model-based learning</h4><p>Another way to generalize from a set of examples is to build a model of these exam‐ ples, then use that model to make predictions. This is called model-based learning (Figure 1-16).</p><p><img src="https://ws2.sinaimg.cn/large/006tNc79ly1fzntvcq0wsj31400mmaeb.jpg" alt="Screen Shot 2019-01-29 at 10.33.40 PM"></p><h2 id="Main-Challenges-of-Machine-Learning"><a href="#Main-Challenges-of-Machine-Learning" class="headerlink" title="Main Challenges of Machine Learning"></a>Main Challenges of Machine Learning</h2><p>In short, since your main task is to select a learning algorithm and train it on some data, the two things that can go wrong are “bad algorithm” and “bad data”.</p><h3 id="Insufficient-Quantity-of-Training-data"><a href="#Insufficient-Quantity-of-Training-data" class="headerlink" title="Insufficient Quantity of Training data"></a>Insufficient Quantity of Training data</h3><p>For a toddler to learn what an apple is, all it takes is for you to point to an apple and say “apple” (possibly repeating this procedure a few times). Now the child is able to recognize apples in all sorts of colors and shapes. Genius.</p><p>Machine Learning is not quite there yet; it takes a lot of data for most Machine Learning algorithms to work properly. Even for very simple problems you typically need thousands of examples, and for complex problems such as image or speech recogni‐ tion you may need millions of examples (unless you can reuse parts of an existing model).</p><h3 id="Nonrepresentative-Training-Data"><a href="#Nonrepresentative-Training-Data" class="headerlink" title="Nonrepresentative Training Data"></a>Nonrepresentative Training Data</h3><p>In order to generalize well, it is crucial that your training data be representative of the new cases you want to generalize to. This is true whether you use instance-based learning or model-based learning.</p><p>It is crucial to use a training set that is representative of the cases you want to general‐ ize to. This is often harder than it sounds: if the sample is too small, you will have sampling noise (i.e., nonrepresentative data as a result of chance), but even very large samples can be nonrepresentative if the sampling method is flawed. This is called sampling bias.</p><h3 id="Poor-Quality-Data"><a href="#Poor-Quality-Data" class="headerlink" title="Poor-Quality Data"></a>Poor-Quality Data</h3><p>Obviously, if your training data is full of errors, outliers, and noise (e.g., due to poor- quality measurements), it will make it harder for the system to detect the underlying patterns, so your system is less likely to perform well. It is often well worth the effort to spend time cleaning up your training data. The truth is, most data scientists spend a significant part of their time doing just that. For example:</p><ul><li>If some instances are clearly outliers, it may help to simply discard them or try to fix the errors manually.</li><li>If some instances are missing a few features (e.g., 5% of your customers did not specify their age), you must decide whether you want to ignore this attribute alto‐ gether, ignore these instances, fill in the missing values (e.g., with the median age), or train one model with the feature and one model without it, and so on.</li></ul><h3 id="Irrelevant-Features"><a href="#Irrelevant-Features" class="headerlink" title="Irrelevant Features"></a>Irrelevant Features</h3><p>As the saying goes: garbage in, garbage out. Your system will only be capable of learning if the training data contains enough relevant features and not too many irrelevant ones. A critical part of the success of a Machine Learning project is coming up with a good set of features to train on. This process, called feature engineering, involves:</p><ul><li>Feature selection: selecting the most useful features to train on among existing features.</li><li>Feature extraction: combining existing features to produce a more useful one (as we saw earlier, dimensionality reduction algorithms can help).</li><li>Creating new features by gathering new data.</li></ul><h3 id="Overfitting-the-Training-Data"><a href="#Overfitting-the-Training-Data" class="headerlink" title="Overfitting the Training Data"></a>Overfitting the Training Data</h3><p>Overfitting means that the model performs well on the training data, but it does not generalize well.</p><p>Figure 1-22 shows an example of a high-degree polynomial life satisfaction model that strongly overfits the training data. Even though it performs much better on the training data than the simple linear model, would you really trust its predictions?</p><p><img src="https://ws2.sinaimg.cn/large/006tNc79ly1fznu2fwmxnj31400ggtc8.jpg" alt="Screen Shot 2019-01-29 at 10.40.28 PM"></p><p>Overfitting happens when the model is too complex relative to the amount and noisiness of the training data. The possible solutions are:</p><ul><li>To simplify the model by selecting one with fewer parameters (e.g., a linear model rather than a high-degree polynomial model), by reducing the number of attributes in the training data or by constraining the model</li><li>To gather more training data</li><li>To reduce the noise in the training data (e.g., fix data errors and remove outliers)</li></ul><p>Constraining a model to make it simpler and reduce the risk of overfitting is called regularization.</p><p>The amount of regularization to apply during learning can be controlled by a hyper‐ parameter. A hyperparameter is a parameter of a learning algorithm (not of the model). As such, it is not affected by the learning algorithm itself; it must be set prior to training and remains constant during training. If you set the regularization hyperparameter to a very large value, you will get an almost flat model (a slope close to zero); the learning algorithm will almost certainly not overfit the training data, but it will be less likely to find a good solution. Tuning hyperparameters is an important part of building a Machine Learning system (you will see a detailed example in the next chapter).</p><h3 id="Underfitting-the-Training-Data"><a href="#Underfitting-the-Training-Data" class="headerlink" title="Underfitting the Training Data"></a>Underfitting the Training Data</h3><p>As you might guess, underfitting is the opposite of overfitting: it occurs when your model is too simple to learn the underlying structure of the data. For example, a lin‐ ear model of life satisfaction is prone to underfit; reality is just more complex than the model, so its predictions are bound to be inaccurate, even on the training examples.</p><p>The main options to fix this problem are:</p><ul><li>Selecting a more powerful model, with more parameters</li><li>Feeding better features to the learning algorithm (feature engineering)</li><li>Reducing the constraints on the model (e.g., reducing the regularization hyper‐ parameter)</li></ul><h3 id="Stepping-Back"><a href="#Stepping-Back" class="headerlink" title="Stepping Back"></a>Stepping Back</h3><p>By now you already know a lot about Machine Learning. However, we went through so many concepts that you may be feeling a little lost, so let’s step back and look at the big picture:</p><ul><li>Machine Learning is about making machines get better at some task by learning from data, instead of having to explicitly code rules.</li><li>There are many different types of ML systems: supervised or not, batch or online, instance-based or model-based, and so on.</li><li>In a ML project you gather data in a training set, and you feed the training set to a learning algorithm. If the algorithm is model-based it tunes some parameters to fit the model to the training set (i.e., to make good predictions on the training set itself), and then hopefully it will be able to make good predictions on new cases as well. If the algorithm is instance-based, it just learns the examples by heart and uses a similarity measure to generalize to new instances.</li><li>The system will not perform well if your training set is too small, or if the data is not representative, noisy, or polluted with irrelevant features (garbage in, garbage out). Lastly, your model needs to be neither too simple (in which case it will underfit) nor too complex (in which case it will overfit).</li></ul><h2 id="Testing-and-Validating"><a href="#Testing-and-Validating" class="headerlink" title="Testing and Validating"></a>Testing and Validating</h2><p>The only way to know how well a model will generalize to new cases is to actually try it out on new cases. One way to do that is to put your model in production and moni‐ tor how well it performs. This works well, but if your model is horribly bad, your users will complain—not the best idea.</p><p>A better option is to split your data into two sets: the training set and the test set. As these names imply, you train your model using the training set, and you test it using the test set. The error rate on new cases is called the generalization error (or out-of- sample error), and by evaluating your model on the test set, you get an estimation of this error. This value tells you how well your model will perform on instances it has never seen before.</p><p>If the training error is low (i.e., your model makes few mistakes on the training set) but the generalization error is high, it means that your model is overfitting the train‐ ing data.</p><p>So evaluating a model is simple enough: just use a test set. Now suppose you are hesi‐ tating between two models (say a linear model and a polynomial model): how can you decide? One option is to train both and compare how well they generalize using the test set.</p><p>Now suppose that the linear model generalizes better, but you want to apply some regularization to avoid overfitting. The question is: how do you choose the value of the regularization hyperparameter? One option is to train 100 different models using 100 different values for this hyperparameter. Suppose you find the best hyperparameter value that produces a model with the lowest generalization error, say just 5% error.</p><p>So you launch this model into production, but unfortunately it does not perform as well as expected and produces 15% errors. What just happened?</p><p>The problem is that you measured the generalization error multiple times on the test set, and you adapted the model and hyperparameters to produce the best model for that set. This means that the model is unlikely to perform as well on new data.</p><p>A common solution to this problem is to have a second holdout set called the validation set. You train multiple models with various hyperparameters using the training set, you select the model and hyperparameters that perform best on the validation set, and when you’re happy with your model you run a single final test against the test set to get an estimate of the generalization error.</p><p>To avoid “wasting” too much training data in validation sets, a common technique is to use cross-validation: the training set is split into complementary subsets, and each model is trained against a different combination of these subsets and validated against the remaining parts. Once the model type and hyperparameters have been selected, a final model is trained using these hyperparameters on the full training set, and the generalized error is measured on the test set.</p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol><li><a href="http://shop.oreilly.com/product/0636920052289.do" target="_blank" rel="noopener">Hands-On Machine Learning with Scikit-Learn and TensorFlow</a></li><li><a href="https://www.oreilly.com/catalog/errata.csp?isbn=0636920052289" target="_blank" rel="noopener">Errata for Hands-On Machine Learning with Scikit-Learn and TensorFlow</a></li><li><a href="https://github.com/ageron/handson-ml" target="_blank" rel="noopener">GitHub: Hands-On Machine Learnging With Scikit-Learn and TensorFlow</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Notes </tag>
            
            <tag> Machine Learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Titanic Machine Learning from Disaster</title>
      <link href="/2018/12/01/36-titanic-machine-learning-from-disaster/"/>
      <url>/2018/12/01/36-titanic-machine-learning-from-disaster/</url>
      
        <content type="html"><![CDATA[<p>The sinking of the RMS Titanic is one of the most infamous shipwrecks in history.  On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew.</p><!----><p>This sensational tragedy shocked the international community and led to better safety regulations for ships.</p><h2 id="Competition-Description"><a href="#Competition-Description" class="headerlink" title="Competition Description"></a>Competition Description</h2><p>One of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew. Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.</p><p>In this challenge, we ask you to complete the analysis of what sorts of people were likely to survive. In particular, we ask you to apply the tools of machine learning to predict which passengers survived the tragedy.</p><h2 id="How-to-train-a-model"><a href="#How-to-train-a-model" class="headerlink" title="How to train a model"></a>How to train a model</h2><p>Please see Reference 2, 3, 4.</p><h3 id="Workflow-stages"><a href="#Workflow-stages" class="headerlink" title="Workflow stages"></a>Workflow stages</h3><p><img src="https://note.youdao.com/yws/api/personal/file/WEBaffb53e6c3d55c53eaa1077b924a37e0?method=download&amp;shareKey=3b9c4af797ded9436059752cda5ad6a6" alt="Workflow stages"></p><h3 id="Processing-Steps"><a href="#Processing-Steps" class="headerlink" title="Processing Steps"></a>Processing Steps</h3><p><img src="https://note.youdao.com/yws/api/personal/file/WEB9b051b38eb098212090c73cc0f27f468?method=download&amp;shareKey=7c13bfb3e823446926bdee28f6ea0f91" alt="Predicting the Survival of Titanic Passengers"></p><h2 id="Scripts"><a href="#Scripts" class="headerlink" title="Scripts"></a>Scripts</h2><p><a href="https://github.com/zhongchun/ml_projects/tree/master/titanic_survivor_predictor" target="_blank" rel="noopener">github scripts</a></p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol><li><a href="https://www.kaggle.com/c/titanic" target="_blank" rel="noopener">Kaggle: Titanic_Machine Learning from Disaster</a></li><li><a href="https://www.kaggle.com/chapagain/titanic-solution-a-beginner-s-guide" target="_blank" rel="noopener">Titanic Solution: A Beginner’s Guide</a></li><li><a href="https://towardsdatascience.com/predicting-the-survival-of-titanic-passengers-30870ccc7e8" target="_blank" rel="noopener">Predicting the Survival of Titanic Passengers</a></li><li><a href="https://www.kaggle.com/startupsci/titanic-data-science-solutions" target="_blank" rel="noopener">Titanic Data Science Solutions</a></li><li><a href="https://towardsdatascience.com/how-i-got-a-score-of-82-3-and-ended-up-being-in-top-4-of-kaggles-titanic-dataset-bb2875cee6b5" target="_blank" rel="noopener">How I got a score of 82.3% and ended up being in top 3% of Kaggle’s Titanic Dataset</a></li><li><a href="https://towardsdatascience.com/https-medium-com-janzawadzki-applying-andrew-ngs-1st-deep-neural-network-on-the-titanic-survival-data-set-b77edbc83816" target="_blank" rel="noopener">Applying Andrew Ng’s 1st Deep Neural Network to the Titanic Survival dataset</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Machine Learning </tag>
            
            <tag> Python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hands-on-ML Outline</title>
      <link href="/2018/11/25/35-outline-hands-on-machine-leanring-notes/"/>
      <url>/2018/11/25/35-outline-hands-on-machine-leanring-notes/</url>
      
        <content type="html"><![CDATA[<h2 id="Content"><a href="#Content" class="headerlink" title="Content"></a>Content</h2><p><img src="https://note.youdao.com/yws/api/personal/file/WEB20dfea8121395f64e934d301afbd5caa?method=download&amp;shareKey=0737227383d547e832c1b8be7737c0b2" alt="Hands-on Machine Learning"></p><h2 id="WebSites"><a href="#WebSites" class="headerlink" title="WebSites"></a>WebSites</h2><ol><li><a href="https://www.kaggle.com/" target="_blank" rel="noopener">Kaggle</a></li><li><a href="https://scikit-learn.org/" target="_blank" rel="noopener">Scikit Learn</a></li><li><a href="http://tensorflow.org/" target="_blank" rel="noopener">TensorFlow</a></li><li><a href="https://www.coursera.org/learn/machine-learning/" target="_blank" rel="noopener">Andrew Ng’s ML course on Coursera</a></li><li><a href="https://www.coursera.org/learn/neural-networks" target="_blank" rel="noopener">Geoffrey Hinton’s course on neural networks and Deep Learning</a></li><li><a href="https://www.dataquest.io/" target="_blank" rel="noopener">Data Scientist Online Courses</a></li><li><a href="http://goo.gl/GwtU3A" target="_blank" rel="noopener">Quora ML Blogs</a></li><li><a href="http://deeplearning.net/" target="_blank" rel="noopener">Deep Learning website</a></li></ol><h2 id="Books"><a href="#Books" class="headerlink" title="Books"></a>Books</h2><ol><li><a href="http://shop.oreilly.com/product/0636920033400.do" target="_blank" rel="noopener">Data Science from Scratch</a></li><li><a href="https://dl.acm.org/citation.cfm?id=1571643" target="_blank" rel="noopener">Stephen Marsland, Machine Learning: An Algorithmic Perspective (Chapman and Hall)</a></li><li><a href="http://shop.oreilly.com/product/9781787125933.do" target="_blank" rel="noopener">Sebastian Raschka, Python Machine Learning (Packt Publishing)</a></li><li><a href="https://www.amazon.com/Learning-Data-Yaser-S-Abu-Mostafa/dp/1600490069" target="_blank" rel="noopener">Yaser S. Abu-Mostafa, Malik Magdon-Ismail, and Hsuan-Tien Lin, Learning from Data (AMLBook)</a></li><li><a href="http://aima.cs.berkeley.edu/" target="_blank" rel="noopener">Stuart Russell and Peter Norvig, Artificial Intelligence: A Modern Approach, 3rd Edition (Pearson)</a></li></ol><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol><li><a href="http://shop.oreilly.com/product/0636920052289.do" target="_blank" rel="noopener">Hands-On Machine Learning with Scikit-Learn and TensorFlow</a></li><li><a href="https://www.oreilly.com/catalog/errata.csp?isbn=0636920052289" target="_blank" rel="noopener">Errata for Hands-On Machine Learning with Scikit-Learn and TensorFlow</a></li><li><a href="https://github.com/ageron/handson-ml" target="_blank" rel="noopener">GitHub: Hands-On Machine Learnging With Scikit-Learn and TensorFlow</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Notes </tag>
            
            <tag> Machine Learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>A Small End-to-End Project</title>
      <link href="/2018/11/24/34-a-small-project/"/>
      <url>/2018/11/24/34-a-small-project/</url>
      
        <content type="html"><![CDATA[<p>Books and courses are frustrating. They give you lots of recipes and snippets, but you never get to see how they all fit together.</p><p>When you are applying machine learning to your own datasets, you are working on a project.</p><h2 id="Start"><a href="#Start" class="headerlink" title="Start"></a>Start</h2><p>A machine learning project may not be linear, but it has a number of well known steps:</p><ol><li>Define Problem.</li><li>Prepare Data.</li><li>Evaluate Algorithms.</li><li>Improve Results.</li><li>Present Results.</li></ol><p>The best way to really come to terms with a new platform or tool is to work through a machine learning project end-to-end and cover the key steps. Namely, from loading data, summarizing data, evaluating algorithms and making some predictions.</p><p>If you can do that, you have a template that you can use on dataset after dataset. You can fill in the gaps such as further data preparation and improving result tasks later, once you have more confidence.</p><h2 id="Machine-Learning-in-Python-Step-By-Step-Tutorial"><a href="#Machine-Learning-in-Python-Step-By-Step-Tutorial" class="headerlink" title="Machine Learning in Python: Step-By-Step Tutorial"></a>Machine Learning in Python: Step-By-Step Tutorial</h2><h3 id="Downloading-Installing-and-Starting-Python-SciPy"><a href="#Downloading-Installing-and-Starting-Python-SciPy" class="headerlink" title="Downloading, Installing and Starting Python SciPy"></a>Downloading, Installing and Starting Python SciPy</h3><h4 id="Install-SciPy-Libraries"><a href="#Install-SciPy-Libraries" class="headerlink" title="Install SciPy Libraries"></a>Install SciPy Libraries</h4><p>There are 5 key libraries that you will need to install. Below is a list of the Python SciPy libraries required for this tutorial:</p><ul><li>scipy</li><li>numpy</li><li>matplotlib</li><li>pandas</li><li>sklearn</li></ul><h4 id="Start-Python-and-Check-Versions"><a href="#Start-Python-and-Check-Versions" class="headerlink" title="Start Python and Check Versions"></a>Start Python and Check Versions</h4><pre class=" language-python"><code class="language-python"><span class="token comment" spellcheck="true"># Check the versions of libraries</span><span class="token comment" spellcheck="true"># Python version</span><span class="token keyword">import</span> sys<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'Python: {}'</span><span class="token punctuation">.</span>format<span class="token punctuation">(</span>sys<span class="token punctuation">.</span>version<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># scipy</span><span class="token keyword">import</span> scipy<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'scipy: {}'</span><span class="token punctuation">.</span>format<span class="token punctuation">(</span>scipy<span class="token punctuation">.</span>__version__<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># numpy</span><span class="token keyword">import</span> numpy<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'numpy: {}'</span><span class="token punctuation">.</span>format<span class="token punctuation">(</span>numpy<span class="token punctuation">.</span>__version__<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># matplotlib</span><span class="token keyword">import</span> matplotlib<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'matplotlib: {}'</span><span class="token punctuation">.</span>format<span class="token punctuation">(</span>matplotlib<span class="token punctuation">.</span>__version__<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># pandas</span><span class="token keyword">import</span> pandas<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'pandas: {}'</span><span class="token punctuation">.</span>format<span class="token punctuation">(</span>pandas<span class="token punctuation">.</span>__version__<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># scikit-learn</span><span class="token keyword">import</span> sklearn<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'sklearn: {}'</span><span class="token punctuation">.</span>format<span class="token punctuation">(</span>sklearn<span class="token punctuation">.</span>__version__<span class="token punctuation">)</span><span class="token punctuation">)</span></code></pre><p>Ouput:</p><pre><code>Python: 2.7.13 |Anaconda 4.4.0 (x86_64)| (default, Dec 20 2016, 23:05:08)[GCC 4.2.1 Compatible Apple LLVM 6.0 (clang-600.0.57)]scipy: 0.19.0numpy: 1.12.1matplotlib: 2.0.2pandas: 0.20.1sklearn: 0.18.1</code></pre><h3 id="Load-the-Data"><a href="#Load-the-Data" class="headerlink" title="Load the Data"></a>Load the Data</h3><h4 id="Import-libraries"><a href="#Import-libraries" class="headerlink" title="Import libraries"></a>Import libraries</h4><h4 id="Load-Dataset"><a href="#Load-Dataset" class="headerlink" title="Load Dataset"></a>Load Dataset</h4><h3 id="Summarize-the-Dataset"><a href="#Summarize-the-Dataset" class="headerlink" title="Summarize the Dataset"></a>Summarize the Dataset</h3><p>In this step we are going to take a look at the data a few different ways:</p><ol><li>Dimensions of the dataset.</li><li>Peek at the data itself.</li><li>Statistical summary of all attributes.</li><li>Breakdown of the data by the class variable.</li></ol><h4 id="Dimensions-of-Dataset"><a href="#Dimensions-of-Dataset" class="headerlink" title="Dimensions of Dataset"></a>Dimensions of Dataset</h4><h4 id="Peek-at-the-Data"><a href="#Peek-at-the-Data" class="headerlink" title="Peek at the Data"></a>Peek at the Data</h4><h4 id="Statistical-Summary"><a href="#Statistical-Summary" class="headerlink" title="Statistical Summary"></a>Statistical Summary</h4><h4 id="Class-Distribution"><a href="#Class-Distribution" class="headerlink" title="Class Distribution"></a>Class Distribution</h4><h3 id="Data-Visualization"><a href="#Data-Visualization" class="headerlink" title="Data Visualization"></a>Data Visualization</h3><p>We now have a basic idea about the data. We need to extend that with some visualizations.</p><p>We are going to look at two types of plots:</p><ol><li>Univariate plots to better understand each attribute.</li><li>Multivariate plots to better understand the relationships between attributes.</li></ol><h4 id="Univariate-Plots"><a href="#Univariate-Plots" class="headerlink" title="Univariate Plots"></a>Univariate Plots</h4><h4 id="Multivariate-Plots"><a href="#Multivariate-Plots" class="headerlink" title="Multivariate Plots"></a>Multivariate Plots</h4><h3 id="Evaluate-Some-Algorithms"><a href="#Evaluate-Some-Algorithms" class="headerlink" title="Evaluate Some Algorithms"></a>Evaluate Some Algorithms</h3><p>Now it is time to create some models of the data and estimate their accuracy on unseen data.</p><p>Here is what we are going to cover in this step:</p><ol><li>Separate out a validation dataset.</li><li>Set-up the test harness to use 10-fold cross validation.</li><li>Build 5 different models to predict species from flower measurements</li><li>Select the best model.</li></ol><h4 id="Create-a-Validation-Dataset"><a href="#Create-a-Validation-Dataset" class="headerlink" title="Create a Validation Dataset"></a>Create a Validation Dataset</h4><h4 id="Test-Harness"><a href="#Test-Harness" class="headerlink" title="Test Harness"></a>Test Harness</h4><h4 id="Build-Models"><a href="#Build-Models" class="headerlink" title="Build Models"></a>Build Models</h4><p>Let’s evaluate 6 different algorithms: (Classification)</p><ol><li>Logistic Regression (LR)</li><li>Linear Discriminant Analysis (LDA)</li><li>K-Nearest Neighbors (KNN).</li><li>Classification and Regression Trees (CART).</li><li>Gaussian Naive Bayes (NB).</li><li>Support Vector Machines (SVM).</li></ol><h4 id="Select-Best-Model"><a href="#Select-Best-Model" class="headerlink" title="Select Best Model"></a>Select Best Model</h4><h3 id="Make-Predictions"><a href="#Make-Predictions" class="headerlink" title="Make Predictions"></a>Make Predictions</h3><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol><li><a href="https://machinelearningmastery.com/machine-learning-in-python-step-by-step/" target="_blank" rel="noopener">Your First Machine Learning Project in Python Step-By-Step</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Machine Learning </tag>
            
            <tag> Python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Notes-深度学习入门之深度学习</title>
      <link href="/2018/11/22/33-dl-deep-learning-notes/"/>
      <url>/2018/11/22/33-dl-deep-learning-notes/</url>
      
        <content type="html"><![CDATA[<p>深度学习是加深了层的深度神经网络。基于之前介绍的网络，只需通过叠加层，就可以创建深度网络。本章我们将看一下深度学习的性质、课题和可能性，然后对当前的深度学习进行概括性的说明。</p><h2 id="加深网络"><a href="#加深网络" class="headerlink" title="加深网络"></a>加深网络</h2><p>关于神经网络，我们已经学了很多东西，比如构成神经网络的各种层、学习时的有效技巧、对图像特别有效的 CNN、参数的最优化方法等，这些都是深度学习中的重要技术。本节我们将这些已经学过的技术汇总起来，创建一个深度网络，挑战 MNIST 数据集的手写数字识别。</p><h3 id="向更深的网络出发"><a href="#向更深的网络出发" class="headerlink" title="向更深的网络出发"></a>向更深的网络出发</h3><p>话不多说，这里我们来创建一个如图 8-1 所示的网络结构的 CNN（一个比之前的网络都深的网络）。这个网络参考了下一节要介绍的 VGG。</p><p>如图 8-1 所示，这个网络的层比之前实现的网络都更深。这里使用的卷积层全都是 3 × 3 的小型滤波器，特点是随着层的加深，通道数变大（卷积层的通道数从前面的层开始按顺序以 16、16、32、32、64、64 的方式增加）。此外，如图 8-1 所示，插入了池化层，以逐渐减小中间数据的空间大小；并且，后面的全连接层中使用了 Dropout 层。</p><p><img src="https://ws4.sinaimg.cn/large/006tKfTcly1g09s196pxpj30rs0efq41.jpg" alt="144"></p><p><strong>图 8-1　进行手写数字识别的深度 CNN</strong></p><p>这个网络使用 He 初始值作为权重的初始值，使用 Adam 更新权重参数。把上述内容总结起来，这个网络有如下特点。</p><ul><li>基于 3×3 的小型滤波器的卷积层。</li><li>激活函数是 ReLU。</li><li>全连接层的后面使用 Dropout 层。</li><li>基于 Adam 的最优化。</li><li>使用 He 初始值作为权重初始值。</li></ul><p>从这些特征中可以看出，图 8-1 的网络中使用了多个之前介绍的神经网络技术。现在，我们使用这个网络进行学习。先说一下结论，这个网络的识别精度为 99.38% {1[最终的识别精度有少许偏差，不过在这个网络中，识别精度大体上都会超过 99%。]}，可以说是非常优秀的性能了！</p><h3 id="进一步提高识别精度"><a href="#进一步提高识别精度" class="headerlink" title="进一步提高识别精度"></a>进一步提高识别精度</h3><p>在一个标题为“What is the class of this image ?”的网站 [32] 上，以排行榜的形式刊登了目前为止通过论文等渠道发表的针对各种数据集的方法的识别精度（图 8-3）。</p><p><img src="https://ws1.sinaimg.cn/large/006tKfTcly1g09s5nlvoyj30rs0mstgq.jpg" alt="146"></p><p><strong>图 8-3　针对 MNIST 数据集的各种方法的排行（引自文献 [32]：2016 年 6 月）</strong></p><p>显眼。实际上，排行榜上的前几名大都是基于 CNN 的方法。顺便说一下，截止到 2016 年 6 月，对 MNIST 数据集的最高识别精度是 99.79%（错误识别率为 0.21%），该方法也是以 CNN 为基础的 [33]。不过，它用的 CNN 并不是特别深层的网络（卷积层为 2 层、全连接层为 2 层的网络）。</p><pre class=" language-text"><code class="language-text">对于 MNIST 数据集，层不用特别深就获得了（目前）最高的识别精度。一般认为，这是因为对于手写数字识别这样一个比较简单的任务，没有必要将网络的表现力提高到那么高的程度。因此，可以说加深层的好处并不大。而之后要介绍的大规模的一般物体识别的情况，因为问题复杂，所以加深层对提高识别精度大有裨益。</code></pre><p>参考刚才排行榜中前几名的方法，可以发现进一步提高识别精度的技术和线索。比如，集成学习、学习率衰减、<strong>Data Augmentation</strong>（数据扩充）等都有助于提高识别精度。尤其是 Data Augmentation，虽然方法很简单，但在提高识别精度上效果显著。</p><p>Data Augmentation 基于算法“人为地”扩充输入图像（训练图像）。具体地说，如图 8-4 所示，对于输入图像，通过施加旋转、垂直或水平方向上的移动等微小变化，增加图像的数量。这在数据集的图像数量有限时尤其有效。</p><p><img src="https://ws2.sinaimg.cn/large/006tKfTcly1g09s71ldgbj30rs09140d.jpg" alt="147"></p><p><strong>图 8-4　Data Augmentation 的例子</strong></p><p>除了如图 8-4 所示的变形之外，Data Augmentation 还可以通过其他各种方法扩充图像，比如裁剪图像的“crop 处理”、将图像左右翻转的“flip 处理”{2[flip 处理只在不需要考虑图像对称性的情况下有效。]}等。对于一般的图像，施加亮度等外观上的变化、放大缩小等尺度上的变化也是有效的。不管怎样，通过 Data Augmentation 巧妙地增加训练图像，就可以提高深度学习的识别精度。虽然这个看上去只是一个简单的技巧，不过经常会有很好的效果。这里，我们不进行 Data Augmentation 的实现，不过这个技巧的实现比较简单，有兴趣的读者请自己试一下。</p><h3 id="加深层的动机"><a href="#加深层的动机" class="headerlink" title="加深层的动机"></a>加深层的动机</h3><p>关于加深层的重要性，现状是理论研究还不够透彻。尽管目前相关理论还比较贫乏，但是有几点可以从过往的研究和实验中得以解释（虽然有一些直观）。本节就加深层的重要性，给出一些增补性的数据和说明。</p><p>首先，从以 ILSVRC(<a href="http://www.image-net.org/challenges/LSVRC/" target="_blank" rel="noopener">ImageNet Large Scale Visual Recognition Competition</a>) 为代表的大规模图像识别的比赛结果中可以看出加深层的重要性（详细内容请参考下一节）。这种比赛的结果显示，最近前几名的方法多是基于深度学习的，并且有逐渐加深网络的层的趋势。也就是说，可以看到层越深，识别性能也越高。</p><p>下面我们说一下加深层的好处。其中一个好处就是可以减少网络的参数数量。说得详细一点，就是与没有加深层的网络相比，加深了层的网络可以用更少的参数达到同等水平（或者更强）的表现力。这一点结合卷积运算中的滤波器大小来思考就好理解了。比如，图 8-5 展示了由 5 × 5 的滤波器构成的卷积层。</p><p><img src="https://ws1.sinaimg.cn/large/006tKfTcly1g09sa2n35cj30rs0d2mys.jpg" alt="148"></p><p><strong>图 8-5　5×5 的卷积运算的例子</strong></p><p>这里希望大家考虑一下输出数据的各个节点是从输入数据的哪个区域计算出来的。显然，在图 8-5 的例子中，每个输出节点都是从输入数据的某个 5 × 5 的区域算出来的。接下来我们思考一下图 8-6 中重复两次 3 × 3 的卷积运算的情形。此时，每个输出节点将由中间数据的某个 3 × 3 的区域计算出来。那么，中间数据的 3 × 3 的区域又是由前一个输入数据的哪个区域计算出来的呢？仔细观察图 8-6，可知它对应一个 5 × 5 的区域。也就是说，图 8-6 的输出数据是“观察”了输入数据的某个 5 × 5 的区域后计算出来的。</p><p><img src="https://ws3.sinaimg.cn/large/006tKfTcly1g09sbkicufj30rs08jwg3.jpg" alt="149"></p><p><strong>图 8-6　重复两次 3×3 的卷积层的例子</strong></p><p>一次 5 × 5 的卷积运算的区域可以由两次 3 × 3 的卷积运算抵充。并且，相对于前者的参数数量 25（5 × 5），后者一共是 18（2 × 3 × 3），通过叠加卷积层，参数数量减少了。而且，这个参数数量之差会随着层的加深而变大。比如，重复三次 3 × 3 的卷积运算时，参数的数量总共是 27。而为了用一次卷积运算“观察”与之相同的区域，需要一个 7 × 7 的滤波器，此时的参数数量是 49。</p><pre class=" language-text"><code class="language-text">叠加小型滤波器来加深网络的好处是可以减少参数的数量，扩大感受野（receptive field，给神经元施加变化的某个局部空间区域）。并且，通过叠加层，将 ReLU 等激活函数夹在卷积层的中间，进一步提高了网络的表现力。这是因为向网络添加了基于激活函数的“非线性”表现力，通过非线性函数的叠加，可以表现更加复杂的东西。</code></pre><p>加深层的另一个好处就是使学习更加高效。与没有加深层的网络相比，通过加深层，可以减少学习数据，从而高效地进行学习。为了直观地理解这一点，大家可以回忆一下 7.6 节的内容。7.6 节中介绍了 CNN 的卷积层会分层次地提取信息。具体地说，在前面的卷积层中，神经元会对边缘等简单的形状有响应，随着层的加深，开始对纹理、物体部件等更加复杂的东西有响应。</p><p>我们先牢记这个网络的分层结构，然后考虑一下“狗”的识别问题。要用浅层网络解决这个问题的话，卷积层需要一下子理解很多“狗”的特征。“狗”有各种各样的种类，根据拍摄环境的不同，外观变化也很大。因此，要理解“狗”的特征，需要大量富有差异性的学习数据，而这会导致学习需要花费很多时间。</p><p>不过，通过加深网络，就可以分层次地分解需要学习的问题。因此，各层需要学习的问题就变成了更简单的问题。比如，最开始的层只要专注于学习边缘就好，这样一来，只需用较少的学习数据就可以高效地进行学习。这是为什么呢？因为和印有“狗”的照片相比，包含边缘的图像数量众多，并且边缘的模式比“狗”的模式结构更简单。</p><p>通过加深层，可以分层次地传递信息，这一点也很重要。比如，因为提取了边缘的层的下一层能够使用边缘的信息，所以应该能够高效地学习更加高级的模式。也就是说，通过加深层，可以将各层要学习的问题分解成容易解决的简单问题，从而可以进行高效的学习。</p><p>以上就是对加深层的重要性的増补性说明。不过，这里需要注意的是，近几年的深层化是由大数据、计算能力等即便加深层也能正确地进行学习的新技术和环境支撑的。</p><h2 id="深度学习的小历史"><a href="#深度学习的小历史" class="headerlink" title="深度学习的小历史"></a>深度学习的小历史</h2><p>一般认为，现在深度学习之所以受到大量关注，其契机是 2012 年举办的大规模图像识别大赛 ILSVRC（ImageNet Large Scale Visual Recognition Challenge）。在那年的比赛中，基于深度学习的方法（通称 AlexNet）以压倒性的优势胜出，彻底颠覆了以往的图像识别方法。2012 年深度学习的这场逆袭成为一个转折点，在之后的比赛中，深度学习一直活跃在舞台中央。本节我们以 ILSVRC 这个大规模图像识别比赛为轴，看一下深度学习最近的发展趋势。</p><h3 id="ImageNet"><a href="#ImageNet" class="headerlink" title="ImageNet"></a>ImageNet</h3><p>ImageNet[25] 是拥有超过 100 万张图像的数据集。如图 8-7 所示，它包含了各种各样的图像，并且每张图像都被关联了标签（类别名）。每年都会举办使用这个巨大数据集的 ILSVRC 图像识别大赛。</p><p><img src="https://ws4.sinaimg.cn/large/006tKfTcly1g09sew4z9tj30rs09dan6.jpg" alt="150"></p><p><strong>图 8-7　大规模数据集 ImageNet 的数据例</strong></p><p>ILSVRC 大赛有多个测试项目，其中之一是“类别分类”（classification），在该项目中，会进行 1000 个类别的分类，比试识别精度。我们来看一下最近几年的 ILSVRC 大赛的类别分类项目的结果。图 8-8 中展示了从 2010 年到 2015 年的优胜队伍的成绩。这里，将前 5 类中出现正确解的情况视为“正确”，此时的错误识别率用柱形图来表示。</p><p>图 8-8 中需要注意的是，以 2012 年为界，之后基于深度学习的方法一直居于首位。实际上，我们发现 2012 年的 AlexNet 大幅降低了错误识别率。并且，此后基于深度学习的方法不断在提升识别精度。特别是 2015 年的 ResNet（一个超过 150 层的深度网络）将错误识别率降低到了 3.5%。据说这个结果甚至超过了普通人的识别能力。</p><p><img src="https://note.youdao.com/yws/api/personal/file/WEBf66009af2a301e67fb5143f04fbe564d?method=download&amp;shareKey=088f2466a30bf4125649031a04299957" alt=""><br>图 8-8　ILSCRV 优胜队伍的成绩演变：竖轴是错误识别率，横轴是年份。横轴的括号内是队伍名或者方法名</p><p>ImageNet 挑战赛，举办从2010年到2017年。</p><h3 id="VGG"><a href="#VGG" class="headerlink" title="VGG"></a>VGG</h3><p>VGG 是由卷积层和池化层构成的基础的 CNN。不过，如图 8-9 所示，它的特点在于将有权重的层（卷积层或者全连接层）叠加至 16 层（或者 19 层），具备了深度（根据层的深度，有时也称为“VGG16”或“VGG19”）。</p><p>VGG 中需要注意的地方是，基于 3×3 的小型滤波器的卷积层的运算是连续进行的。如图 8-9 所示，重复进行“卷积层重叠 2 次到 4 次，再通过池化层将大小减半”的处理，最后经由全连接层输出结果。</p><pre class=" language-text"><code class="language-text">VGG 在 2014 年的比赛中最终获得了第 2 名的成绩（下一节介绍的 GoogleNet 是 2014 年的第 1 名）。虽然在性能上不及 GoogleNet，但因为 VGG 结构简单，应用性强，所以很多技术人员都喜欢使用基于 VGG 的网络。</code></pre><p><img src="https://ws1.sinaimg.cn/large/006tKfTcly1g09sjes7wzj30rs0ci0ux.jpg" alt="152"></p><p><strong>图 8-9　VGG</strong></p><h3 id="GoogLeNet"><a href="#GoogLeNet" class="headerlink" title="GoogLeNet"></a>GoogLeNet</h3><p>GoogLeNet 的网络结构如图 8-10 所示。图中的矩形表示卷积层、池化层等。</p><p><img src="https://ws1.sinaimg.cn/large/006tKfTcly1g09skjmfk2j30rs065aci.jpg" alt="153"></p><p><strong>图 8-10　GoogLeNet</strong></p><p>只看图的话，这似乎是一个看上去非常复杂的网络结构，但实际上它基本上和之前介绍的 CNN 结构相同。不过，GoogLeNet 的特征是，网络不仅在纵向上有深度，在横向上也有深度（广度）。</p><p>GoogLeNet 在横向上有“宽度”，这称为“Inception 结构”，以图 8-11 所示的结构为基础。</p><p>如图 8-11 所示，Inception 结构使用了多个大小不同的滤波器（和池化），最后再合并它们的结果。GoogLeNet 的特征就是将这个 Inception 结构用作一个构件（构成元素）。此外，在 GoogLeNet 中，很多地方都使用了大小为 1 × 1 的滤波器的卷积层。这个 1 × 1 的卷积运算通过在通道方向上减小大小，有助于减少参数和实现高速化处理。</p><p><img src="https://ws4.sinaimg.cn/large/006tKfTcly1g09slvuonqj30rs0d3tam.jpg" alt="154"></p><p><strong>图 8-11　GoogLeNet 的 Inception 结构</strong></p><h3 id="ResNet"><a href="#ResNet" class="headerlink" title="ResNet"></a>ResNet</h3><p>ResNet[24] 是微软团队开发的网络。它的特征在于具有比以前的网络更深的结构。</p><p>我们已经知道加深层对于提升性能很重要。但是，在深度学习中，过度加深层的话，很多情况下学习将不能顺利进行，导致最终性能不佳。ResNet 中，为了解决这类问题，导入了“快捷结构”（也称为“捷径”或“小路”）。导入这个快捷结构后，就可以随着层的加深而不断提高性能了（当然，层的加深也是有限度的）。</p><p>如图 8-12 所示，快捷结构横跨（跳过）了输入数据的卷积层，将输入 <code>x</code> 合计到输出。</p><p>图 8-12 中，在连续 2 层的卷积层中，将输入 <em>x</em> 跳着连接至 2 层后的输出。这里的重点是，通过快捷结构，原来的 2 层卷积层的输出${F}(x)$变成了${F}(x)+x$。通过引入这种快捷结构，即使加深层，也能高效地学习。这是因为，通过快捷结构，反向传播时信号可以无衰减地传递。</p><p><img src="https://ws2.sinaimg.cn/large/006tKfTcly1g09sohy3gkj30rs0fp0v1.jpg" alt="155"></p><p><strong>图 8-12　ResNet 的构成要素（引用自文献 [24]）：这里的“weight layer”是指卷积层</strong></p><pre class=" language-text"><code class="language-text">因为快捷结构只是原封不动地传递输入数据，所以反向传播时会将来自上游的梯度原封不动地传向下游。这里的重点是不对来自上游的梯度进行任何处理，将其原封不动地传向下游。因此，基于快捷结构，不用担心梯度会变小（或变大），能够向前一层传递“有意义的梯度”。通过这个快捷结构，之前因为加深层而导致的梯度变小的梯度消失问题就有望得到缓解。</code></pre><p>ResNet 以前面介绍过的 VGG 网络为基础，引入快捷结构以加深层，其结果如图 8-13 所示。</p><p><img src="https://ws1.sinaimg.cn/large/006tKfTcly1g09spjhmazj30rs03umzu.jpg" alt="156"></p><p><strong>图 8-13　ResNet（引用自文献 [24]）：方块对应 3×3 的卷积层，其特征在于引入了横跨层的快捷结构</strong></p><p>如图 8-13 所示，ResNet 通过以 2 个卷积层为间隔跳跃式地连接来加深层。另外，根据实验的结果，即便加深到 150 层以上，识别精度也会持续提高。并且，在 ILSVRC 大赛中，ResNet 的错误识别率为 3.5%（前 5 类中包含正确解这一精度下的错误识别率），令人称奇。</p><pre class=" language-text"><code class="language-text">实践中经常会灵活应用使用 ImageNet 这个巨大的数据集学习到的权重数据，这称为迁移学习，将学习完的权重（的一部分）复制到其他神经网络，进行再学习（fine tuning）。比如，准备一个和 VGG 相同结构的网络，把学习完的权重作为初始值，以新数据集为对象，进行再学习。迁移学习在手头数据集较少时非常有效。</code></pre><h2 id="深度学习的高速化"><a href="#深度学习的高速化" class="headerlink" title="深度学习的高速化"></a>深度学习的高速化</h2><p>随着大数据和网络的大规模化，深度学习需要进行大量的运算。虽然到目前为止，我们都是使用 CPU 进行计算的，但现实是只用 CPU 来应对深度学习无法令人放心。实际上，环视一下周围，大多数深度学习的框架都支持 <strong>GPU</strong>（Graphics Processing Unit），可以高速地处理大量的运算。另外，最近的框架也开始支持多个 GPU 或多台机器上的分布式学习。本节我们将焦点放在深度学习的计算的高速化上，然后逐步展开。深度学习的实现在 8.1 节就结束了，本节要讨论的高速化（支持 GPU 等）并不进行实现。</p><h3 id="需要努力解决的问题"><a href="#需要努力解决的问题" class="headerlink" title="需要努力解决的问题"></a>需要努力解决的问题</h3><p>在介绍深度学习的高速化之前，我们先来看一下深度学习中什么样的处理比较耗时。图 8-14 中以 AlexNet 的 <code>forward</code> 处理为对象，用饼图展示了各层所耗费的时间。</p><p>从图中可知，AlexNex 中，大多数时间都被耗费在卷积层上。实际上，卷积层的处理时间加起来占 GPU 整体的 95%，占 CPU 整体的 89% ！因此，如何高速、高效地进行卷积层中的运算是深度学习的一大课题。虽然图 8-14 是推理时的结果，不过学习时也一样，卷积层中会耗费大量时间。</p><pre class=" language-text"><code class="language-text">正如 7.2 节介绍的那样，卷积层中进行的运算可以追溯至乘积累加运算。因此，深度学习的高速化的主要课题就变成了如何高速、高效地进行大量的乘积累加运算。</code></pre><p><img src="https://ws3.sinaimg.cn/large/006tKfTcly1g09srrzgnpj30rs09kwin.jpg" alt="157"></p><p><strong>图 8-14　AlexNet 的 forward 处理中各层的时间比：左边是使用 GPU 的情况，右边是使用 CPU 的情况。图中的“conv”对应卷积层，“pool”对应池化层，“fc”对应全连接层，“norm”对应正规化层（引用自文献 [26]）</strong></p><h3 id="基于GPU的高速化"><a href="#基于GPU的高速化" class="headerlink" title="基于GPU的高速化"></a>基于GPU的高速化</h3><p>GPU 原本是作为图像专用的显卡使用的，但最近不仅用于图像处理，也用于通用的数值计算。由于 GPU 可以高速地进行并行数值计算，因此 <strong>GPU 计算</strong>的目标就是将这种压倒性的计算能力用于各种用途。所谓 GPU 计算，是指基于 GPU 进行通用的数值计算的操作。</p><p>深度学习中需要进行大量的乘积累加运算（或者大型矩阵的乘积运算）。这种大量的并行运算正是 GPU 所擅长的（反过来说，CPU 比较擅长连续的、复杂的计算）。因此，与使用单个 CPU 相比，使用 GPU 进行深度学习的运算可以达到惊人的高速化。下面我们就来看一下基于 GPU 可以实现多大程度的高速化。图 8-15 是基于 CPU 和 GPU 进行 AlexNet 的学习时分别所需的时间。</p><p>从图中可知，使用 CPU 要花 40 天以上的时间，而使用 GPU 则可以将时间缩短至 6 天。此外，还可以看出，通过使用 cuDNN 这个最优化的库，可以进一步实现高速化。</p><p>GPU 主要由 NVIDIA 和 AMD 两家公司提供。虽然两家的 GPU 都可以用于通用的数值计算，但与深度学习比较“亲近”的是 NVIDIA 的 GPU。实际上，大多数深度学习框架只受益于 NVIDIA 的 GPU。这是因为深度学习的框架中使用了 NVIDIA 提供的 CUDA 这个面向 GPU 计算的综合开发环境。图 8-15 中出现的 cuDNN 是在 CUDA 上运行的库，它里面实现了为深度学习最优化过的函数等。</p><p><img src="https://ws3.sinaimg.cn/large/006tKfTcly1g09ssz0cfpj30rs0iedho.jpg" alt="158"></p><p><strong>图 8-15　使用 CPU 的“16-core Xeon CPU”和 GPU 的“Titan 系列”进行 AlexNet 的学习时分别所需的时间（引用自文献 [27]）</strong></p><pre class=" language-text"><code class="language-text">通过 im2col 可以将卷积层进行的运算转换为大型矩阵的乘积。这个 im2col 方式的实现对 GPU 来说是非常方便的实现方式。这是因为，相比按小规模的单位进行计算，GPU 更擅长计算大规模的汇总好的数据。也就是说，通过基于 im2col 以大型矩阵的乘积的方式汇总计算，更容易发挥出 GPU 的能力。</code></pre><h3 id="分布式学习"><a href="#分布式学习" class="headerlink" title="分布式学习"></a>分布式学习</h3><p>虽然通过 GPU 可以实现深度学习运算的高速化，但即便如此，当网络较深时，学习还是需要几天到几周的时间。并且，前面也说过，深度学习伴随着很多试错。为了创建良好的网络，需要反复进行各种尝试，这样一来就必然会产生尽可能地缩短一次学习所需的时间的要求。于是，将深度学习的学习过程扩展开来的想法（也就是分布式学习）就变得重要起来。</p><p>为了进一步提高深度学习所需的计算的速度，可以考虑在多个 GPU 或者多台机器上进行分布式计算。现在的深度学习框架中，出现了好几个支持多 GPU 或者多机器的分布式学习的框架。其中，Google 的 TensorFlow、微软的 CNTK（Computational Network Toolki）在开发过程中高度重视分布式学习。以大型数据中心的低延迟·高吞吐网络作为支撑，基于这些框架的分布式学习呈现出惊人的效果。</p><p>基于分布式学习，可以达到何种程度的高速化呢？图 8-16 中显示了基于 TensorFlow 的分布式学习的效果。</p><p><img src="https://ws3.sinaimg.cn/large/006tKfTcly1g09su1uq91j30rs0ftdh9.jpg" alt="159"></p><p><strong>图 8-16　基于 TensorFlow 的分布式学习的效果：横轴是 GPU 的个数，纵轴是与单个 GPU 相比时的高速化率（引用自文献 [28]）</strong></p><p>如图 8-16 所示，随着 GPU 个数的增加，学习速度也在提高。实际上，与使用 1 个 GPU 时相比，使用 100 个 GPU（设置在多台机器上，共 100 个）似乎可以实现 56 倍的高速化！这意味着之前花费 7 天的学习只要 3 个小时就能完成，充分说明了分布式学习惊人的效果。</p><p>关于分布式学习，“如何进行分布式计算”是一个非常难的课题。它包含了机器间的通信、数据的同步等多个无法轻易解决的问题。可以将这些难题都交给 TensorFlow 等优秀的框架。这里，我们不讨论分布式学习的细节。关于分布式学习的技术性内容，请参考 TensorFlow 的技术论文（白皮书）等。</p><h3 id="运算精度的位数缩减"><a href="#运算精度的位数缩减" class="headerlink" title="运算精度的位数缩减"></a>运算精度的位数缩减</h3><p>在深度学习的高速化中，除了计算量之外，内存容量、总线带宽等也有可能成为瓶颈。关于内存容量，需要考虑将大量的权重参数或中间数据放在内存中。关于总线带宽，当流经 GPU（或者 CPU）总线的数据超过某个限制时，就会成为瓶颈。考虑到这些情况，我们希望尽可能减少流经网络的数据的位数。</p><p>计算机中为了表示实数，主要使用 64 位或者 32 位的浮点数。通过使用较多的位来表示数字，虽然数值计算时的误差造成的影响变小了，但计算的处理成本、内存使用量却相应地增加了，还给总线带宽带来了负荷。</p><p>关于数值精度（用几位数据表示数值），我们已经知道深度学习并不那么需要数值精度的位数。这是神经网络的一个重要性质。这个性质是基于神经网络的健壮性而产生的。这里所说的健壮性是指，比如，即便输入图像附有一些小的噪声，输出结果也仍然保持不变。可以认为，正是因为有了这个健壮性，流经网络的数据即便有所“劣化”，对输出结果的影响也较小。</p><p>计算机中表示小数时，有 32 位的单精度浮点数和 64 位的双精度浮点数等格式。根据以往的实验结果，在深度学习中，即便是 16 位的<strong>半精度浮点数</strong>（half float），也可以顺利地进行学习 [30]。实际上，NVIDIA 的下一代 GPU 框架 Pascal 也支持半精度浮点数的运算，由此可以认为今后半精度浮点数将被作为标准使用。</p><pre class=" language-text"><code class="language-text">NVIDIA 的 Maxwell GPU 虽然支持半精度浮点数的存储（保存数据的功能），但是运算本身不是用 16 位进行的。下一代的 Pascal 框架，因为运算也是用 16 位进行的，所以只用半精度浮点数进行计算，就有望实现超过上一代 GPU 约 2 倍的高速化。</code></pre><p>以往的深度学习的实现中并没有注意数值的精度，不过 Python 中一般使用 64 位的浮点数。NumPy 中提供了 16 位的半精度浮点数类型（不过，只有 16 位类型的存储，运算本身不用 16 位进行），即便使用 NumPy 的半精度浮点数，识别精度也不会下降。</p><p>关于深度学习的位数缩减，到目前为止已有若干研究。最近有人提出了用 1 位来表示权重和中间数据的 Binarized Neural Networks 方法 [31]。为了实现深度学习的高速化，位数缩减是今后必须关注的一个课题，特别是在面向嵌入式应用程序中使用深度学习时，位数缩减非常重要。</p><h2 id="深度学习的应用案例"><a href="#深度学习的应用案例" class="headerlink" title="深度学习的应用案例"></a>深度学习的应用案例</h2><p>前面，作为使用深度学习的例子，我们主要讨论了手写数字识别的图像类别分类问题（称为“物体识别”）。不过，深度学习并不局限于物体识别，还可以应用于各种各样的问题。此外，在图像、语音、自然语言等各个不同的领域，深度学习都展现了优异的性能。本节将以计算机视觉这个领域为中心，介绍几个深度学习能做的事情（应用）。</p><h3 id="物体检测"><a href="#物体检测" class="headerlink" title="物体检测"></a>物体检测</h3><p>物体检测是从图像中确定物体的位置，并进行分类的问题。如图 8-17 所示，要从图像中确定物体的种类和物体的位置。</p><p><img src="https://ws3.sinaimg.cn/large/006tKfTcly1g09sx1gedhj30rs0dhdyo.jpg" alt="160"></p><p><strong>图 8-17　物体检测的例子</strong></p><p>观察图 8-17 可知，物体检测是比物体识别更难的问题。之前介绍的物体识别是以整个图像为对象的，但是物体检测需要从图像中确定类别的位置，而且还有可能存在多个物体。</p><p>对于这样的物体检测问题，人们提出了多个基于 CNN 的方法。这些方法展示了非常优异的性能，并且证明了在物体检测的问题上，深度学习是非常有效的。</p><p>在使用 CNN 进行物体检测的方法中，有一个叫作 R-CNN[35] 的有名的方法。图 8-18 显示了 R-CNN 的处理流。</p><p><img src="https://ws4.sinaimg.cn/large/006tKfTcly1g09sxu3ogyj30rs07w45a.jpg" alt="161"></p><p><strong>图 8-18　R-CNN 的处理流</strong></p><p>希望大家注意图中的“2.Extract region proposals”（候选区域的提取）和“3.Compute CNN features”（CNN 特征的计算）的处理部分。这里，首先（以某种方法）找出形似物体的区域，然后对提取出的区域应用 CNN 进行分类。R-CNN 中会将图像变形为正方形，或者在分类时使用 SVM（支持向量机），实际的处理流会稍微复杂一些，不过从宏观上看，也是由刚才的两个处理（候选区域的提取和 CNN 特征的计算）构成的。</p><p>在 R-CNN 的前半部分的处理——候选区域的提取（发现形似目标物体的处理）中，可以使用计算机视觉领域积累的各种各样的方法。R-CNN 的论文中使用了一种被称为 Selective Search 的方法，最近还提出了一种基于 CNN 来进行候选区域提取的 Faster R-CNN 方法 [36]。Faster R-CNN 用一个 CNN 来完成所有处理，使得高速处理成为可能。</p><h3 id="图像分割"><a href="#图像分割" class="headerlink" title="图像分割"></a>图像分割</h3><p>图像分割是指在像素水平上对图像进行分类。如图 8-19 所示，使用以像素为单位对各个对象分别着色的监督数据进行学习。然后，在推理时，对输入图像的所有像素进行分类。</p><p><img src="https://ws2.sinaimg.cn/large/006tKfTcly1g09sz7u0wvj30rs0ai7bu.jpg" alt="162"></p><p><strong>图 8-19　图像分割的例子（引用自文献 [34]）：左边是输入图像，右边是监督用的带标签图像</strong></p><p>之前实现的神经网络是对图像整体进行了分类，要将它落实到像素水平的话，该怎么做呢？</p><p>要基于神经网络进行图像分割，最简单的方法是以所有像素为对象，对每个像素执行推理处理。比如，准备一个对某个矩形区域中心的像素进行分类的网络，以所有像素为对象执行推理处理。正如大家能想到的，这样的方法需要按照像素数量进行相应次 <code>forward</code> 处理，因而需要耗费大量的时间（正确地说，卷积运算中会发生重复计算很多区域的无意义的计算）。为了解决这个无意义的计算问题，有人提出了一个名为 FCN（Fully Convolutional Network）[37] 的方法。该方法通过一次 <code>forward</code> 处理，对所有像素进行分类（图 8-20）。</p><p>FCN 的字面意思是“全部由卷积层构成的网络”。相对于一般的 CNN 包含全连接层，FCN 将全连接层替换成发挥相同作用的卷积层。在物体识别中使用的网络的全连接层中，中间数据的空间容量被作为排成一列的节点进行处理，而只由卷积层构成的网络中，空间容量可以保持原样直到最后的输出。</p><p>如图 8-20 所示，FCN 的特征在于最后导入了扩大空间大小的处理。基于这个处理，变小了的中间数据可以一下子扩大到和输入图像一样的大小。FCN 最后进行的扩大处理是基于双线性插值法的扩大（双线性插值扩大）。FCN 中，这个双线性插值扩大是通过去卷积（逆卷积运算）来实现的（细节请参考 FCN 的论文 [37]）。</p><p><img src="https://ws2.sinaimg.cn/large/006tKfTcly1g09t0t41j0j30rs0ejn2f.jpg" alt="163"></p><p><strong>图 8-20　FCN 的概略图</strong></p><pre class=" language-text"><code class="language-text">全连接层中，输出和全部的输入相连。使用卷积层也可以实现与此结构完全相同的连接。比如，针对输入大小是 32×10×10（通道数 32、高 10、长 10）的数据的全连接层可以替换成滤波器大小为 32×10×10 的卷积层。如果全连接层的输出节点数是 100，那么在卷积层准备 100 个 32×10×10 的滤波器就可以实现完全相同的处理。像这样，全连接层可以替换成进行相同处理的卷积层。</code></pre><h3 id="图像标题的生成"><a href="#图像标题的生成" class="headerlink" title="图像标题的生成"></a>图像标题的生成</h3><p>有一项融合了计算机视觉和自然语言的有趣的研究，该研究如图 8-21 所示，给出一个图像后，会自动生成介绍这个图像的文字（图像的标题）。</p><p>给出一个图像后，会像图 8-21 一样自动生成表示该图像内容的文本。比如，左上角的第一幅图像生成了文本“A person riding a motorcycle on a dirt road.”（在没有铺装的道路上骑摩托车的人），而且这个文本只从该图像自动生成。文本的内容和图像确实是一致的。并且，令人惊讶的是，除了“骑摩托车”之外，连“没有铺装的道路”都被正确理解了。</p><p><img src="https://ws4.sinaimg.cn/large/006tKfTcly1g09t24ib3cj30rs0hd7rq.jpg" alt="164"></p><p><strong>图 8-21　基于深度学习的图像标题生成的例子（引用自文献 [38]）</strong></p><p>一个基于深度学习生成图像标题的代表性方法是被称为 NIC（Neural Image Caption）的模型。如图 8-22 所示，NIC 由深层的 CNN 和处理自然语言的 RNN（Recurrent Neural Network）构成。RNN 是呈递归式连接的网络，经常被用于自然语言、时间序列数据等连续性的数据上。</p><p><img src="https://ws3.sinaimg.cn/large/006tKfTcly1g09t2qppkcj30rs0audpu.jpg" alt="165"></p><p><strong>图 8-22　Neural Image Caption（NIC）的整体结构</strong></p><p>NIC 基于 CNN 从图像中提取特征，并将这个特征传给 RNN。RNN 以 CNN 提取出的特征为初始值，递归地生成文本。这里，我们不深入讨论技术上的细节，不过基本上 NIC 是组合了两个神经网络（CNN 和 RNN）的简单结构。基于 NIC，可以生成惊人的高精度的图像标题。我们将组合图像和自然语言等多种信息进行的处理称为<strong>多模态处理</strong>。多模态处理是近年来备受关注的一个领域。</p><pre class=" language-text"><code class="language-text">RNN 的 R 表示 Recurrent（递归的）。这个递归指的是神经网络的递归的网络结构。根据这个递归结构，神经网络会受到之前生成的信息的影响（换句话说，会记忆过去的信息），这是 RNN 的特征。比如，生成“我”这个词之后，下一个要生成的词受到“我”这个词的影响，生成了“要”；然后，再受到前面生成的“我要”的影响，生成了“睡觉”这个词。对于自然语言、时间序列数据等连续性的数据，RNN 以记忆过去的信息的方式运行。</code></pre><h2 id="深度学习的未来"><a href="#深度学习的未来" class="headerlink" title="深度学习的未来"></a>深度学习的未来</h2><p>深度学习已经不再局限于以往的领域，开始逐渐应用于各个领域。本节将介绍几个揭示了深度学习的可能性和未来的研究。</p><h3 id="图像风格变换"><a href="#图像风格变换" class="headerlink" title="图像风格变换"></a>图像风格变换</h3><p>有一项研究是使用深度学习来“绘制”带有艺术气息的画。如图 8-23 所示，输入两个图像后，会生成一个新的图像。两个输入图像中，一个称为“内容图像”，另一个称为“风格图像”。</p><p>如图 8-23 所示，如果指定将梵高的绘画风格应用于内容图像，深度学习就会按照指示绘制出新的画作。此项研究出自论文“A Neural Algorithm of Artistic Style”[39]，一经发表就受到全世界的广泛关注。</p><p><img src="https://ws4.sinaimg.cn/large/006tKfTcly1g09t48nvogj30rs0mt7wh.jpg" alt="166"></p><p><strong>图 8-23　基于论文“A Neural Algorithm of Artistic Style”的图像风格变换的例子：左上角是风格图像，右上角是内容图像，下面的图像是新生成的图像（图像引用自文献[40]）</strong></p><p>这里我们不会介绍这项研究的详细内容，只是叙述一下这个技术的大致框架，即刚才的方法是在学习过程中使网络的中间数据近似内容图像的中间数据。这样一来，就可以使输入图像近似内容图像的形状。此外，为了从风格图像中吸收风格，导入了风格矩阵的概念。通过在学习过程中减小风格矩阵的偏差，就可以使输入图像接近梵高的风格。</p><h3 id="图像的生成"><a href="#图像的生成" class="headerlink" title="图像的生成"></a>图像的生成</h3><p>刚才的图像风格变换的例子在生成新的图像时输入了两个图像。不同于这种研究，现在有一种研究是生成新的图像时不需要任何图像（虽然需要事先使用大量的图像进行学习，但在“画”新图像时不需要任何图像）。比如，基于深度学习，可以实现从零生成“卧室”的图像。图 8-24 中展示的图像是基于 <strong>DCGAN</strong>（Deep Convolutional Generative Adversarial Network）[41] 方法生成的卧室图像的例子。</p><p><img src="https://ws1.sinaimg.cn/large/006tKfTcly1g09t5jzlygj30rs0dxat1.jpg" alt="167"></p><p><strong>图 8-24　基于 DCGAN 生成的新的卧室图像（引用自文献 [41]）</strong></p><p>图 8-24 的图像可能看上去像是真的照片，但其实这些图像都是基于 DCGAN 新生成的图像。也就是说，DCGAN 生成的图像是谁都没有见过的图像（学习数据中没有的图像），是从零生成的新图像。</p><p>能画出以假乱真的图像的 DCGAN 会将图像的生成过程模型化。使用大量图像（比如，印有卧室的大量图像）训练这个模型，学习结束后，使用这个模型，就可以生成新的图像。</p><p>DCGAN 中使用了深度学习，其技术要点是使用了 Generator（生成者）和 Discriminator（识别者）这两个神经网络。Generator 生成近似真品的图像，Discriminator 判别它是不是真图像（是 Generator 生成的图像还是实际拍摄的图像）。像这样，通过让两者以竞争的方式学习，Generator 会学习到更加精妙的图像作假技术，Discriminator 则会成长为能以更高精度辨别真假的鉴定师。两者互相切磋、共同成长，这是 <strong>GAN</strong>（Generative Adversarial Network）这个技术的有趣之处。在这样的切磋中成长起来的 Generator 最终会掌握画出足以以假乱真的图像的能力（或者说有这样的可能）。</p><pre class=" language-text"><code class="language-text">之前我们见到的机器学习问题都是被称为监督学习（supervised learning）的问题。这类问题就像手写数字识别一样，使用的是图像数据和教师标签成对给出的数据集。不过这里讨论的问题，并没有给出监督数据，只给了大量的图像（图像的集合），这样的问题称为无监督学习（unsupervised learning）。无监督学习虽然是很早之前就开始研究的领域（Deep Belief Network、Deep Boltzmann Machine 等很有名），但最近似乎并不是很活跃。今后，随着使用深度学习的 DCGAN 等方法受到关注，无监督学习有望得到进一步发展。</code></pre><h3 id="自动驾驶"><a href="#自动驾驶" class="headerlink" title="自动驾驶"></a>自动驾驶</h3><p>计算机代替人类驾驶汽车的自动驾驶技术有望得到实现。除了汽车制造商之外，IT 企业、大学、研究机构等也都在为实现自动驾驶而进行着激烈的竞争。自动驾驶需要结合各种技术的力量来实现，比如决定行驶路线的路线计划（path plan）技术、照相机或激光等传感技术等，在这些技术中，正确识别周围环境的技术据说尤其重要。这是因为要正确识别时刻变化的环境、自由来往的车辆和行人是非常困难的。</p><p>如果可以在各种环境中稳健地正确识别行驶区域的话，实现自动驾驶可能也就没那么遥远了。最近，在识别周围环境的技术中，深度学习的力量备受期待。比如，基于 CNN 的神经网络 SegNet[42]，可以像图 8-25 那样高精度地识别行驶环境。</p><p>图 8-25 中对输入图像进行了分割（像素水平的判别）。观察结果可知，在某种程度上正确地识别了道路、建筑物、人行道、树木、车辆等。今后若能基于深度学习使这种技术进一步实现高精度化、高速化的话，自动驾驶的实用化可能也就没那么遥远了。</p><p><img src="https://ws1.sinaimg.cn/large/006tKfTcly1g09t6vq8a4j30rs0d7n6y.jpg" alt="168"></p><p><strong>图 8-25　基于深度学习的图像分割的例子：道路、车辆、建筑物、人行道等被高精度地识别了出来（引用自文献 [43]）</strong></p><h3 id="Deep-Q-Network-强化学习"><a href="#Deep-Q-Network-强化学习" class="headerlink" title="Deep Q-Network (强化学习)"></a>Deep Q-Network (强化学习)</h3><p>就像人类通过摸索试验来学习一样（比如骑自行车），让计算机也在摸索试验的过程中自主学习，这称为<strong>强化学习</strong>（reinforcement learning）。强化学习和有“教师”在身边教的“监督学习”有所不同。</p><p>强化学习的基本框架是，代理（Agent）根据环境选择行动，然后通过这个行动改变环境。根据环境的变化，代理获得某种报酬。强化学习的目的是决定代理的行动方针，以获得更好的报酬（图 8-26）。</p><p>图 8-26 中展示了强化学习的基本框架。这里需要注意的是，报酬并不是确定的，只是“预期报酬”。比如，在《超级马里奥兄弟》这款电子游戏中，让马里奥向右移动能获得多少报酬不一定是明确的。这时需要从游戏得分（获得的硬币、消灭的敌人等）或者游戏结束等明确的指标来反向计算，决定“预期报酬”。如果是监督学习的话，每个行动都可以从“教师”那里获得正确的评价。</p><p><img src="https://ws2.sinaimg.cn/large/006tKfTcly1g09t7mnjiej30rp0nqaa2.jpg" alt="169"></p><p><strong>图 8-26　强化学习的基本框架：代理自主地进行学习，以获得更好的报酬</strong></p><p>在使用了深度学习的强化学习方法中，有一个叫作 Deep Q-Network（通称 <strong>DQN</strong>）[44] 的方法。该方法基于被称为 Q 学习的强化学习算法。这里省略 Q 学习的细节，不过在 Q 学习中，为了确定最合适的行动，需要确定一个被称为最优行动价值函数的函数。为了近似这个函数，DQN 使用了深度学习（CNN）。</p><p>在 DQN 的研究中，有让电子游戏自动学习，并实现了超过人类水平的操作的例子。如图 8-27 所示，DQN 中使用的 CNN 把游戏图像的帧（连续 4 帧）作为输入，最终输出游戏手柄的各个动作（控制杆的移动量、按钮操作的有无等）的“价值”。</p><p>之前在学习电子游戏时，一般是把游戏的状态（人物的地点等）事先提取出来，作为数据给模型。但是，在 DQN 中，如图 8-27 所示，输入数据只有电子游戏的图像。这是 DQN 值得大书特书的地方，可以说大幅提高了 DQN 的实用性。为什么呢？因为这样就无需根据每个游戏改变设置，只要给 DQN 游戏图像就可以了。实际上，DQN 可以用相同的结构学习《吃豆人》、<em>Atari</em> 等很多游戏，甚至在很多游戏中取得了超过人类的成绩。</p><p><img src="https://ws4.sinaimg.cn/large/006tKfTcly1g09t8zucn4j30rs0g1q84.jpg" alt="170"></p><p><strong>图 8-27　基于 Deep Q-Network 学习电子游戏的操作。输入是电子游戏的图像，经过摸索试验，学习出让专业玩家都自愧不如的游戏手柄（操作杆）的操作手法（引用自文献 [44]）</strong></p><pre class=" language-text"><code class="language-text">人工智能 AlphaGo[45] 击败围棋冠军的新闻受到了广泛关注。这个 AlphaGo 技术的内部也用了深度学习和强化学习。AlphaGo 学习了 3000 万个专业棋手的棋谱，并且不停地重复自己和自己的对战，积累了大量的学习经验。AlphaGo 和 DQN 都是 Google 的 Deep Mind 公司进行的研究，该公司今后的研究值得密切关注。</code></pre><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>本章我们实现了一个（稍微）深层的 CNN，并在手写数字识别上获得了超过 99% 的高识别精度。此外，还讲解了加深网络的动机，指出了深度学习在朝更深的方向前进。之后，又介绍了深度学习的趋势和应用案例，以及对高速化的研究和代表深度学习未来的研究案例。</p><p>深度学习领域还有很多尚未揭晓的东西，新的研究正一个接一个地出现。今后，全世界的研究者和技术专家也将继续积极从事这方面的研究，一定能实现目前无法想象的技术。</p><p>主要内容包括：</p><ul><li>对于大多数的问题，都可以期待通过加深网络来提高性能。</li><li>在最近的图像识别大赛ILSVRC中，基于深度学习的方法独占鳌头，使用的网络也在深化。</li><li>VGG、GoogLeNet、ResNet等是几个著名的网络。</li><li>基于GPU、分布式学习、位数精度的缩减，可以实现深度学习的高速化。</li><li>深度学习（神经网络）不仅可以用于物体识别，还可以用于物体检测、图像分割。</li><li>深度学习的应用包括图像标题的生成、图像的生成、强化学习等。最近，深度学习在自动驾驶上的应用也备受期待。</li></ul><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol><li><a href="http://www.ituring.com.cn/book/1921" target="_blank" rel="noopener">深度学习入门：基于Python的理论与实现</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> Deep Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Notes </tag>
            
            <tag> Deep Learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Notes-深度学习入门之卷积神经网络</title>
      <link href="/2018/11/17/32-dl-convolutional-neural-network-notes/"/>
      <url>/2018/11/17/32-dl-convolutional-neural-network-notes/</url>
      
        <content type="html"><![CDATA[<p>本章的主题是卷积神经网络（Convolutional Neural Network，CNN）。CNN 被用于图像识别、语音识别等各种场合，在图像识别的比赛中，基于深度学习的方法几乎都以 CNN 为基础。</p><h2 id="整体结构"><a href="#整体结构" class="headerlink" title="整体结构"></a>整体结构</h2><p>首先，来看一下 CNN 的网络结构，了解 CNN 的大致框架。CNN 和之前介绍的神经网络一样，可以像乐高积木一样通过组装层来构建。不过，CNN 中新出现了卷积层（Convolution 层）和池化层（Pooling 层）。</p><p>之前介绍的神经网络中，相邻层的所有神经元之间都有连接，这称为<strong>全连接</strong>（fully-connected）。另外，我们用 Affine 层实现了全连接层。如果使用这个 Affine 层，一个 5 层的全连接的神经网络就可以通过图 7-1 所示的网络结构来实现。</p><p><img src="https://ws2.sinaimg.cn/large/006tKfTcly1g09lexh5u3j30rs05tjrr.jpg" alt="116"></p><p><strong>图 7-1　基于全连接层（Affine 层）的网络的例子</strong></p><p>那么，CNN 会是什么样的结构呢？图 7-2 是 CNN 的一个例子。</p><p><img src="https://note.youdao.com/yws/api/personal/file/WEB2622b8cea05bb93dfd4ec5d901370972?method=download&amp;shareKey=c6573e2a678e9193cca977660653a1c4" alt=""><br>图 7-2　基于 CNN 的网络的例子：新增了 Convolution 层和 Pooling 层（用灰色的方块表示）</p><p>如图 7-2 所示，CNN 中新增了 Convolution 层和 Pooling 层。CNN 的层的连接顺序是“Convolution - ReLU -（Pooling）”（Pooling 层有时会被省略）。这可以理解为之前的“Affine - ReLU”连接被替换成了“Convolution - ReLU -（Pooling）”连接。</p><h2 id="卷积层"><a href="#卷积层" class="headerlink" title="卷积层"></a>卷积层</h2><p>CNN 中出现了一些特有的术语，比如填充、步幅等。此外，各层中传递的数据是有形状的数据（比如，3 维数据），这与之前的全连接网络不同，因此刚开始学习 CNN 时可能会感到难以理解。</p><h3 id="全连接层存在的问题"><a href="#全连接层存在的问题" class="headerlink" title="全连接层存在的问题"></a>全连接层存在的问题</h3><p>全连接层存在什么问题呢？那就是数据的形状被“忽视”了。比如，输入数据是图像时，图像通常是高、长、通道方向上的 3 维形状。但是，向全连接层输入时，需要将 3 维数据拉平为 1 维数据。实际上，前面提到的使用了 MNIST 数据集的例子中，输入图像就是 1 通道、高 28 像素、长 28 像素的（1, 28, 28）形状，但却被排成 1 列，以 784 个数据的形式输入到最开始的 Affine 层。</p><p>图像是 3 维形状，这个形状中应该含有重要的空间信息。比如，空间上邻近的像素为相似的值、RBG 的各个通道之间分别有密切的关联性、相距较远的像素之间没有什么关联等，3 维形状中可能隐藏有值得提取的本质模式。但是，因为全连接层会忽视形状，将全部的输入数据作为相同的神经元（同一维度的神经元）处理，所以无法利用与形状相关的信息。</p><p>而卷积层可以保持形状不变。当输入数据是图像时，卷积层会以 3 维数据的形式接收输入数据，并同样以 3 维数据的形式输出至下一层。因此，在 CNN 中，可以（有可能）正确理解图像等具有形状的数据。</p><p>另外，CNN 中，有时将卷积层的输入输出数据称为<strong>特征图</strong>（feature map）。其中，卷积层的输入数据称为输入特征图（input feature map），输出数据称为输出特征图（output feature map）。本书中将“输入输出数据”和“特征图”作为含义相同的词使用。</p><h3 id="卷积运算"><a href="#卷积运算" class="headerlink" title="卷积运算"></a>卷积运算</h3><p>卷积层进行的处理就是卷积运算。卷积运算相当于图像处理中的“滤波器运算”。</p><p>在介绍卷积运算时，我们来看一个具体的例子（图 7-3）。</p><p><img src="https://note.youdao.com/yws/api/personal/file/WEBf15909006372d6b9c069008a559521e1?method=download&amp;shareKey=6cbf97e9a027243252e5c4336b3a4756" alt=""></p><p><strong>图 7-3　卷积运算的例子</strong></p><p>如图 7-3 所示，卷积运算对输入数据应用滤波器。在这个例子中，输入数据是有高长方向的形状的数据，滤波器也一样，有高长方向上的维度。假设用（height, width）表示数据和滤波器的形状，则在本例中，输入大小是 (4, 4)，滤波器大小是 (3, 3)，输出大小是 (2, 2)。另外，有的文献中也会用“核”这个词来表示这里所说的“滤波器”。</p><p>图 7-4 中展示了卷积运算的计算顺序。<br><img src="https://note.youdao.com/yws/api/personal/file/WEB673015b6631f438b634a82a61526d367?method=download&amp;shareKey=385144cb5d4417bfd4c461bb27c5f4e3" alt=""></p><p><strong>图 7-4　卷积运算的计算顺序</strong></p><p>在全连接的神经网络中，除了权重参数，还存在偏置。CNN 中，滤波器的参数就对应之前的权重。并且，CNN 中也存在偏置。图 7-3 的卷积运算的例子一直展示到了应用滤波器的阶段。包含偏置的卷积运算的处理流如图 7-5 所示。</p><p><img src="https://note.youdao.com/yws/api/personal/file/WEB63763209e64407973220254e163babba?method=download&amp;shareKey=ad7892f18aaf72335924f75b267fcbcd" alt=""><br>图 7-5　卷积运算的偏置：向应用了滤波器的元素加上某个固定值（偏置）</p><h3 id="填充"><a href="#填充" class="headerlink" title="填充"></a>填充</h3><p>在进行卷积层的处理之前，有时要向输入数据的周围填入固定的数据（比如 0 等），这称为填充（padding），是卷积运算中经常会用到的处理。比如，在图 7-6 的例子中，对大小为 (4, 4) 的输入数据应用了幅度为 1 的填充。“幅度为 1 的填充”是指用幅度为 1 像素的 0 填充周围。</p><p><img src="https://note.youdao.com/yws/api/personal/file/WEB8208a08d5e3017ee1910c75ad2809589?method=download&amp;shareKey=5b97c16afed8e564f003a058ad0f3ece" alt=""></p><p><strong>图 7-6　卷积运算的填充处理：向输入数据的周围填入 0（图中用虚线表示填充，并省略了填充的内容“0”）</strong></p><pre class=" language-text"><code class="language-text">使用填充主要是为了调整输出的大小。比如，对大小为 (4, 4) 的输入数据应用 (3, 3) 的滤波器时，输出大小变为 (2, 2)，相当于输出大小比输入大小缩小了 2 个元素。这在反复进行多次卷积运算的深度网络中会成为问题。为什么呢？因为如果每次进行卷积运算都会缩小空间，那么在某个时刻输出大小就有可能变为 1，导致无法再应用卷积运算。为了避免出现这样的情况，就要使用填充。在刚才的例子中，将填充的幅度设为 1，那么相对于输入大小 (4, 4)，输出大小也保持为原来的 (4, 4)。因此，卷积运算就可以在保持空间大小不变的情况下将数据传给下一层。</code></pre><h3 id="步幅"><a href="#步幅" class="headerlink" title="步幅"></a>步幅</h3><p>应用滤波器的位置间隔称为<strong>步幅</strong>（stride）。<br>之前的例子中步幅都是 1，如果将步幅设为 2，则如图 7-7 所示，应用滤波器的窗口的间隔变为 2 个元素。</p><p><img src="https://note.youdao.com/yws/api/personal/file/WEB6a29890f92a8bfbd5040aff13f6f02e7?method=download&amp;shareKey=f3bd1ac096b8876064ad5c37827d51dd" alt=""><br>图 7-7　步幅为 2 的卷积运算的例子</p><p>综上，增大步幅后，输出大小会变小。而增大填充后，输出大小会变大。如果将这样的关系写成算式，会如何呢？接下来，我们看一下对于填充和步幅，如何计算输出大小。</p><p>这里，假设输入大小为 (<em>H</em>, <em>W</em>)，滤波器大小为 (<em>FH</em>, <em>FW</em>)，输出大小为 (<em>OH</em>, <em>OW</em>)，填充为 <em>P</em>，步幅为 <em>S</em>。此时，输出大小可通过式 (7.1) 进行计算。</p><p><img src="https://ws1.sinaimg.cn/large/006tKfTcly1g09n4a3ybwj30ya09vt9b.jpg" alt="7_71"></p><p>注意：当输出大小无法除尽时（结果是小数时），需要采取报错等对策。顺便说一下，根据深度学习的框架的不同，当值无法除尽时，有时会向最接近的整数四舍五入，不进行报错而继续运行。</p><h3 id="3-维数据的卷积运算"><a href="#3-维数据的卷积运算" class="headerlink" title="3 维数据的卷积运算"></a>3 维数据的卷积运算</h3><p>之前的卷积运算的例子都是以有高、长方向的 2 维形状为对象的。但是，图像是 3 维数据，除了高、长方向之外，还需要处理通道方向。这里，我们按照与之前相同的顺序，看一下对加上了通道方向的 3 维数据进行卷积运算的例子。</p><p>图 7-8 是卷积运算的例子，图 7-9 是计算顺序。这里以 3 通道的数据为例，展示了卷积运算的结果。和 2 维数据时（图 7-3 的例子）相比，可以发现纵深方向（通道方向）上特征图增加了。通道方向上有多个特征图时，会按通道进行输入数据和滤波器的卷积运算，并将结果相加，从而得到输出。</p><p><img src="https://note.youdao.com/yws/api/personal/file/WEBe08701d61c00d3cc00852fcc07197f5a?method=download&amp;shareKey=5ef1307fae6e9ba152190d7b95934395" alt=""><br>图 7-8　对 3 维数据进行卷积运算的例子</p><p><img src="https://ws1.sinaimg.cn/large/006tKfTcly1g09n77zaz0j30rs14q7az.jpg" alt="124"></p><p><strong>图 7-9　对 3 维数据进行卷积运算的计算顺序</strong></p><p>需要注意的是，在 3 维数据的卷积运算中，输入数据和滤波器的通道数要设为相同的值。在这个例子中，输入数据和滤波器的通道数一致，均为 3。滤波器大小可以设定为任意值（不过，每个通道的滤波器大小要全部相同）。这个例子中滤波器大小为 (3, 3)，但也可以设定为 (2, 2)、(1, 1)、(5, 5) 等任意值。再强调一下，通道数只能设定为和输入数据的通道数相同的值（本例中为 3）。</p><h3 id="结合方块思考"><a href="#结合方块思考" class="headerlink" title="结合方块思考"></a>结合方块思考</h3><p>将数据和滤波器结合长方体的方块来考虑，3 维数据的卷积运算会很容易理解。方块是如图 7-10 所示的 3 维长方体。把 3 维数据表示为多维数组时，书写顺序为（channel, height, width）。</p><p>将数据和滤波器结合长方体的方块来考虑，3 维数据的卷积运算会很容易理解。方块是如图 7-10 所示的 3 维长方体。把 3 维数据表示为多维数组时，书写顺序为（channel, height, width）。比如，通道数为 C、高度为 H、长度为 W 的数据的形状可以写成（C, H, W）。滤波器也一样，要按（channel, height, width）的顺序书写。比如，通道数为 C、滤波器高度为 FH（Filter Height）、长度为 FW（Filter Width）时，可以写成（C, FH, FW）。</p><p><img src="https://note.youdao.com/yws/api/personal/file/WEB4b7b79f45d662b13643228fc8cb30618?method=download&amp;shareKey=9d7718b0dc1027ef15c69b3bee9c43f5" alt=""><br>图 7-10　结合方块思考卷积运算。请注意方块的形状</p><p>在这个例子中，数据输出是 1 张特征图。所谓 1 张特征图，换句话说，就是通道数为 1 的特征图。那么，如果要在通道方向上也拥有多个卷积运算的输出，该怎么做呢？为此，就需要用到多个滤波器（权重）。用图表示的话，如图 7-11 所示。</p><p><img src="https://note.youdao.com/yws/api/personal/file/WEBe2d5bdfcd62036320207d7b44f7c249e?method=download&amp;shareKey=f540865080b86b3195b13680ce7fb76e" alt=""><br>图 7-11　基于多个滤波器的卷积运算的例子</p><p>图 7-11 中，通过应用 FN 个滤波器，输出特征图也生成了 FN 个。如果将这 FN 个特征图汇集在一起，就得到了形状为 (FN, OH, OW) 的方块。将这个方块传给下一层，就是 CNN 的处理流。</p><p>如图 7-11 所示，关于卷积运算的滤波器，也必须考虑滤波器的数量。因此，作为 4 维数据，滤波器的权重数据要按 (output_channel, input_channel, height, width) 的顺序书写。比如，通道数为 3、大小为 5 × 5 的滤波器有 20 个时，可以写成 (20, 3, 5, 5)。</p><p>卷积运算中（和全连接层一样）存在偏置。在图 7-11 的例子中，如果进一步追加偏置的加法运算处理，则结果如下面的图 7-12 所示。</p><p>图 7-12 中，每个通道只有一个偏置。这里，偏置的形状是 (FN, 1, 1)，滤波器的输出结果的形状是 (FN, OH, OW)。这两个方块相加时，要对滤波器的输出结果 (FN, OH, OW) 按通道加上相同的偏置值。另外，不同形状的方块相加时，可以基于 NumPy 的广播功能轻松实现（1.5.5 节）。</p><p><img src="https://note.youdao.com/yws/api/personal/file/WEB0b7ab754dd4e754cf4855a51e23e30b3?method=download&amp;shareKey=2062d2a6ea389cadb07c75d25a631265" alt=""><br>图 7-12　卷积运算的处理流（追加了偏置项）</p><h3 id="批处理"><a href="#批处理" class="headerlink" title="批处理"></a>批处理</h3><p>神经网络的处理中进行了将输入数据打包的批处理。之前的全连接神经网络的实现也对应了批处理，通过批处理，能够实现处理的高效化和学习时对 mini-batch 的对应。</p><p>我们希望卷积运算也同样对应批处理。为此，需要将在各层间传递的数据保存为 4 维数据。具体地讲，就是按 (batch_num, channel, height, width) 的顺序保存数据。比如，将图 7-12 中的处理改成对 N 个数据进行批处理时，数据的形状如图 7-13 所示。</p><p>图 7-13 的批处理版的数据流中，在各个数据的开头添加了批用的维度。像这样，数据作为 4 维的形状在各层间传递。这里需要注意的是，网络间传递的是 4 维数据，对这 N 个数据进行了卷积运算。也就是说，批处理将 N 次的处理汇总成了 1 次进行。</p><p><img src="https://note.youdao.com/yws/api/personal/file/WEBcea82a6626794bc00decdd2ad7c77a0f?method=download&amp;shareKey=9e8f1db1e298220e4ba0893d49ce8325" alt=""><br>图 7-13　卷积运算的处理流（批处理）</p><h2 id="池化层"><a href="#池化层" class="headerlink" title="池化层"></a>池化层</h2><p>池化是缩小高、长方向上的空间的运算。比如，如图 7-14 所示，进行将 2 × 2 的区域集约成 1 个元素的处理，缩小空间大小。</p><p><img src="https://note.youdao.com/yws/api/personal/file/WEBf99371e2c6169960d70aed17cb0381b0?method=download&amp;shareKey=e18ac143e9a415731d6c85e9113a827a" alt=""><br>图 7-14　Max 池化的处理顺序</p><p>图 7-14 的例子是按步幅 2 进行 2 × 2 的 Max 池化时的处理顺序。“Max 池化”是获取最大值的运算，“2 × 2”表示目标区域的大小。如图所示，从 2 × 2 的区域中取出最大的元素。此外，这个例子中将步幅设为了 2，所以 2 × 2 的窗口的移动间隔为 2 个元素。另外，一般来说，池化的窗口大小会和步幅设定成相同的值。比如，3 × 3 的窗口的步幅会设为 3，4 × 4 的窗口的步幅会设为 4 等。</p><pre class=" language-text"><code class="language-text">除了 Max 池化之外，还有 Average 池化等。相对于 Max 池化是从目标区域中取出最大值，Average 池化则是计算目标区域的平均值。在图像识别领域，主要使用 Max 池化。因此，本书中说到“池化层”时，指的是 Max 池化。</code></pre><h3 id="池化层的特征"><a href="#池化层的特征" class="headerlink" title="池化层的特征"></a>池化层的特征</h3><pre class=" language-text"><code class="language-text">1. 没有要学习的参数   池化层和卷积层不同，没有要学习的参数。池化只是从目标区域中取最大值（或者平均值），所以不存在要学习的参数。2. 通道数不发生变化   经过池化运算，输入数据和输出数据的通道数不会发生变化。3. 对微小的位置变化具有鲁棒性（健壮）</code></pre><h2 id="卷积层和池化层的实现"><a href="#卷积层和池化层的实现" class="headerlink" title="卷积层和池化层的实现"></a>卷积层和池化层的实现</h2><p>前面我们详细介绍了卷积层和池化层，本节我们就用 Python 来实现这两个层。和第 5 章一样，也给进行实现的类赋予 forward 和 backward 方法，并使其可以作为模块使用。</p><p>大家可能会感觉卷积层和池化层的实现很复杂，但实际上，通过使用某种技巧，就可以很轻松地实现。本节将介绍这种技巧，将问题简化，然后再进行卷积层的实现。</p><h3 id="4维数组"><a href="#4维数组" class="headerlink" title="4维数组"></a>4维数组</h3><p>如前所述，CNN 中各层间传递的数据是 4 维数据。所谓 4 维数据，比如数据的形状是 (10, 1, 28, 28)，则它对应 10 个高为 28、长为 28、通道为 1 的数据。</p><p>像这样，CNN 中处理的是 4 维数据，因此卷积运算的实现看上去会很复杂，但是通过使用下面要介绍的 im2col 这个技巧，问题就会变得很简单。</p><h3 id="基于-im2col-的展开"><a href="#基于-im2col-的展开" class="headerlink" title="基于 im2col 的展开"></a>基于 im2col 的展开</h3><p>如果老老实实地实现卷积运算，估计要重复好几层的 for 语句。这样的实现有点麻烦，而且，NumPy 中存在使用 for 语句后处理变慢的缺点（NumPy 中，访问元素时最好不要用 for 语句）。这里，我们不使用 for 语句，而是使用 im2col 这个便利的函数进行简单的实现。</p><p>im2col 是一个函数，将输入数据展开以适合滤波器（权重）。如图 7-17 所示，对 3 维的输入数据应用 im2col 后，数据转换为 2 维矩阵（正确地讲，是把包含批数量的 4 维数据转换成了 2 维数据）。</p><p><img src="https://note.youdao.com/yws/api/personal/file/WEB1107fd7e4b96749380079bdd75d59f8e?method=download&amp;shareKey=5f6a2613d7af85a7f8eeccf8e85cd742" alt=""><br>图 7-17　im2col 的示意图</p><p>im2col 会把输入数据展开以适合滤波器（权重）。具体地说，如图 7-18 所示，对于输入数据，将应用滤波器的区域（3 维方块）横向展开为 1 列。im2col 会在所有应用滤波器的地方进行这个展开处理。</p><p><img src="https://note.youdao.com/yws/api/personal/file/WEB34fffb7c7fe8179d1a7ff43ded2b2b66?method=download&amp;shareKey=7a890d742162ad75f322d90b8da61a93" alt=""><br>图 7-18　将滤波器的应用区域从头开始依次横向展开为 1 列</p><p>在图 7-18 中，为了便于观察，将步幅设置得很大，以使滤波器的应用区域不重叠。而在实际的卷积运算中，滤波器的应用区域几乎都是重叠的。在滤波器的应用区域重叠的情况下，使用 im2col 展开后，展开后的元素个数会多于原方块的元素个数。因此，使用 im2col 的实现存在比普通的实现消耗更多内存的缺点。但是，汇总成一个大的矩阵进行计算，对计算机的计算颇有益处。比如，在矩阵计算的库（线性代数库）等中，矩阵计算的实现已被高度最优化，可以高速地进行大矩阵的乘法运算。因此，通过归结到矩阵计算上，可以有效地利用线性代数库。</p><pre class=" language-text"><code class="language-text">im2col 这个名称是“image to column”的缩写，翻译过来就是“从图像到矩阵”的意思。Caffe、Chainer 等深度学习框架中有名为 im2col 的函数，并且在卷积层的实现中，都使用了 im2col。</code></pre><p>使用 im2col 展开输入数据后，之后就只需将卷积层的滤波器（权重）纵向展开为 1 列，并计算 2 个矩阵的乘积即可（参照图 7-19）。这和全连接层的 Affine层进行的处理基本相同。</p><p>如图 7-19 所示，基于 im2col 方式的输出结果是 2 维矩阵。因为 CNN 中数据会保存为 4 维数组，所以要将 2 维输出数据转换为合适的形状。以上就是卷积层的实现流程。</p><p><img src="https://note.youdao.com/yws/api/personal/file/WEB49705217a4f0901afab95b2b82c49ef4?method=download&amp;shareKey=b6693c54b87252e1ec6adbbbe1aacfcc" alt=""><br>图 7-19　卷积运算的滤波器处理的细节：将滤波器纵向展开为 1 列，并计算和 im2col 展开的数据的矩阵乘积，最后转换（reshape）为输出数据的大小</p><h3 id="卷积层的实现"><a href="#卷积层的实现" class="headerlink" title="卷积层的实现"></a>卷积层的实现</h3><p>本书提供了 im2col 函数，并将这个 im2col 函数作为黑盒（不关心内部实现）使用。im2col 的实现内容在 common/util.py 中，它的实现（实质上）是一个 10 行左右的简单函数。有兴趣的读者可以参考。</p><pre class=" language-python"><code class="language-python">im2col <span class="token punctuation">(</span>input_data<span class="token punctuation">,</span> filter_h<span class="token punctuation">,</span> filter_w<span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> pad<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span></code></pre><ul><li><code>input_data</code>——由（数据量，通道，高，长）的 4 维数组构成的输入数据</li><li><code>filter_h</code>——滤波器的高</li><li><code>filter_w</code>——滤波器的长</li><li><code>stride</code>——步幅</li><li><code>pad</code>——填充</li></ul><h3 id="池化层的实现"><a href="#池化层的实现" class="headerlink" title="池化层的实现"></a>池化层的实现</h3><p>池化层的实现和卷积层相同，都使用 im2col 展开输入数据。不过，池化的情况下，在通道方向上是独立的，这一点和卷积层不同。具体地讲，如图 7-21 所示，池化的应用区域按通道单独展开。</p><p><img src="https://note.youdao.com/yws/api/personal/file/WEB85b415850cd3d52b6d21f1f066f1ad39?method=download&amp;shareKey=a885f683a36887f3b41289ef01961bcd" alt=""><br>图 7-21　对输入数据展开池化的应用区域（2×2 的池化的例子）</p><p>像这样展开之后，只需对展开的矩阵求各行的最大值，并转换为合适的形状即可（图 7-22）。</p><p><img src="https://note.youdao.com/yws/api/personal/file/WEB0d1b6ef0835599a74431633611540a88?method=download&amp;shareKey=883fddbe8f9cb063bbc9285bbd707d36" alt=""><br>图 7-22　池化层的实现流程：池化的应用区域内的最大值元素用灰色表示</p><h2 id="CNN-的实现"><a href="#CNN-的实现" class="headerlink" title="CNN 的实现"></a>CNN 的实现</h2><p>我们已经实现了卷积层和池化层，现在来组合这些层，搭建进行手写数字识别的 CNN。这里要实现如图 7-23 所示的 CNN。</p><p><img src="https://note.youdao.com/yws/api/personal/file/WEB5c938fd7ffa3ba3460d07ddfabf197cf?method=download&amp;shareKey=cfdb73cddefe292224105158f90f4590" alt=""><br>图 7-23　简单 CNN 的网络构成</p><h2 id="CNN-的可视化"><a href="#CNN-的可视化" class="headerlink" title="CNN 的可视化"></a>CNN 的可视化</h2><p>CNN 中用到的卷积层在“观察”什么呢？本节将通过卷积层的可视化，探索 CNN 中到底进行了什么处理。</p><h3 id="第1层权重的可视化"><a href="#第1层权重的可视化" class="headerlink" title="第1层权重的可视化"></a>第1层权重的可视化</h3><p>学习前的滤波器是随机进行初始化的，所以在黑白的浓淡上没有规律可循，但学习后的滤波器变成了有规律的图像。</p><h3 id="基于分层结构的信息提取"><a href="#基于分层结构的信息提取" class="headerlink" title="基于分层结构的信息提取"></a>基于分层结构的信息提取</h3><p>上面的结果是针对第 1 层的卷积层得出的。第 1 层的卷积层中提取了边缘或斑块等“低级”信息，那么在堆叠了多层的 CNN 中，各层中又会提取什么样的信息呢？根据深度学习的可视化相关的研究 [17][18]，随着层次加深，提取的信息（正确地讲，是反映强烈的神经元）也越来越抽象。</p><p>如果堆叠了多层卷积层，则随着层次加深，提取的信息也愈加复杂、抽象，这是深度学习中很有意思的一个地方。最开始的层对简单的边缘有响应，接下来的层对纹理有响应，再后面的层对更加复杂的物体部件有响应。也就是说，随着层次加深，神经元从简单的形状向“高级”信息变化。换句话说，就像我们理解东西的“含义”一样，响应的对象在逐渐变化。</p><h2 id="具有代表性的-CNN"><a href="#具有代表性的-CNN" class="headerlink" title="具有代表性的 CNN"></a>具有代表性的 CNN</h2><p>关于 CNN，迄今为止已经提出了各种网络结构。这里，我们介绍其中特别重要的两个网络，一个是在 1998 年首次被提出的 CNN 元祖 LeNet[20]，另一个是在深度学习受到关注的 2012 年被提出的 AlexNet[21]。</p><h3 id="LeNet"><a href="#LeNet" class="headerlink" title="LeNet"></a>LeNet</h3><p>LeNet 在 1998 年被提出，是进行手写数字识别的网络。如图 7-27 所示，它有连续的卷积层和池化层（正确地讲，是只“抽选元素”的子采样层），最后经全连接层输出结果。</p><p><img src="https://note.youdao.com/yws/api/personal/file/WEBa704a335a795f7ca0a5c8dcebc808524?method=download&amp;shareKey=696e827bb76190e684fd0d7477883b2f" alt=""><br>图 7-27　LeNet 的网络结构</p><p>和“现在的 CNN”相比，LeNet 有几个不同点。第一个不同点在于激活函数。LeNet 中使用 sigmoid 函数，而现在的 CNN 中主要使用 ReLU 函数。此外，原始的 LeNet 中使用子采样（subsampling）缩小中间数据的大小，而现在的 CNN 中 Max 池化是主流。</p><p>综上，LeNet 与现在的 CNN 虽然有些许不同，但差别并不是那么大。想到 LeNet 是 20 多年前提出的最早的 CNN，还是很令人称奇的。</p><h3 id="AlexNet"><a href="#AlexNet" class="headerlink" title="AlexNet"></a>AlexNet</h3><p>在 LeNet 问世 20 多年后，AlexNet 被发布出来。AlexNet 是引发深度学习热潮的导火线，不过它的网络结构和 LeNet 基本上没有什么不同，如图 7-28 所示。</p><p><img src="https://note.youdao.com/yws/api/personal/file/WEB5848db07f3935c224933ae3e5e8c25ae?method=download&amp;shareKey=0a6d631670dc823d46347d448652b108" alt=""><br>图 7-28　AlexNet</p><p>AlexNet 叠有多个卷积层和池化层，最后经由全连接层输出结果。虽然结构上 AlexNet 和 LeNet 没有大的不同，但有以下几点差异。</p><ul><li>激活函数使用 ReLU。</li><li>使用进行局部正规化的 LRN（Local Response Normalization）层。</li><li>使用 Dropout（6.4.3 节）。</li></ul><p>如上所述，关于网络结构，LeNet 和 AlexNet 没有太大的不同。但是，围绕它们的环境和计算机技术有了很大的进步。具体地说，现在任何人都可以获得大量的数据。而且，擅长大规模并行计算的 GPU 得到普及，高速进行大量的运算已经成为可能。大数据和 GPU 已成为深度学习发展的巨大的原动力。</p><pre class=" language-text"><code class="language-text">大多数情况下，深度学习（加深了层次的网络）存在大量的参数。因此，学习需要大量的计算，并且需要使那些参数“满意”的大量数据。可以说是 GPU 和大数据给这些课题带来了希望。</code></pre><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>构成 CNN 的基本模块的卷积层和池化层虽然有些复杂，但是一旦理解了，之后就只是如何使用它们的问题了。本章为了使读者在实现层面上理解卷积层和池化层，花了不少时间进行介绍。在图像处理领域，几乎毫无例外地都会使用 CNN。请扎实地理解本章的内容，然后进入最后一章的学习。</p><p>主要内容：</p><ul><li>CNN在此前的全连接层的网络中新增了卷积层和池化层。</li><li>使用im2col函数可以简单、高效地实现卷积层和池化层。</li><li>通过CNN的可视化，可知随着层次变深，提取的信息愈加高级。</li><li>LeNet和AlexNet是CNN的代表性网络。</li><li>在深度学习的发展中，大数据和GPU做出了很大的贡献。</li></ul><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol><li><a href="http://www.ituring.com.cn/book/1921" target="_blank" rel="noopener">深度学习入门：基于Python的理论与实现</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> Deep Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Notes </tag>
            
            <tag> Deep Learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Notes-深度学习入门之与学习相关的技巧</title>
      <link href="/2018/11/16/30-dl-learning-skills-notes/"/>
      <url>/2018/11/16/30-dl-learning-skills-notes/</url>
      
        <content type="html"><![CDATA[<p>本章将介绍神经网络的学习中的一些重要观点，主题涉及<strong>寻找最优权重参数的最优化方法</strong>、<strong>权重参数的初始值</strong>、<strong>超参数的设定方法</strong>等。此外，为了应对过拟合，本章还将介绍权值衰减、Dropout 等正则化方法，并进行实现。最后将对近年来众多研究中使用的 Batch Normalization 方法进行简单的介绍。使用本章介绍的方法，可以高效地进行神经网络（深度学习）的学习，提高识别精度。让我们一起往下看吧！</p><h2 id="参数的更新"><a href="#参数的更新" class="headerlink" title="参数的更新"></a>参数的更新</h2><p>神经网络的学习的目的是找到使损失函数的值尽可能小的参数。这是寻找最优参数的问题，解决这个问题的过程称为<strong>最优化</strong>（optimization）。遗憾的是，神经网络的最优化问题非常难。这是因为参数空间非常复杂，无法轻易找到最优解（无法使用那种通过解数学式一下子就求得最小值的方法）。而且，在深度神经网络中，参数的数量非常庞大，导致最优化问题更加复杂。</p><p>在前几章中，为了找到最优参数，我们将参数的梯度（导数）作为了线索。使用参数的梯度，沿梯度方向更新参数，并重复这个步骤多次，从而逐渐靠近最优参数，这个过程称为随机梯度下降法（stochastic gradient descent），简称 SGD。SGD 是一个简单的方法，不过比起胡乱地搜索参数空间，也算是“聪明”的方法。但是，根据不同的问题，也存在比 SGD 更加聪明的方法。本节我们将指出 SGD 的缺点，并介绍 SGD 以外的其他最优化方法。</p><h3 id="探险家的故事"><a href="#探险家的故事" class="headerlink" title="探险家的故事"></a>探险家的故事</h3><p>进入正题前，我们先打一个比方，来说明关于最优化我们所处的状况。</p><pre class=" language-text"><code class="language-text">有一个性情古怪的探险家。他在广袤的干旱地带旅行，坚持寻找幽深的山谷。他的目标是要到达最深的谷底（他称之为“至深之地”）。这也是他旅行的目的。并且，他给自己制定了两个严格的“规定”：一个是不看地图；另一个是把眼睛蒙上。因此，他并不知道最深的谷底在这个广袤的大地的何处，而且什么也看不见。在这么严苛的条件下，这位探险家如何前往“至深之地”呢？他要如何迈步，才能迅速找到“至深之地”呢？</code></pre><p>寻找最优参数时，我们所处的状况和这位探险家一样，是一个漆黑的世界。我们必须在没有地图、不能睁眼的情况下，在广袤、复杂的地形中寻找“至深之地”。</p><p>在这么困难的状况下，地面的坡度显得尤为重要。探险家虽然看不到周围的情况，但是能够知道当前所在位置的坡度（通过脚底感受地面的倾斜状况）。于是，朝着当前所在位置的坡度最大的方向前进，就是 SGD 的策略。勇敢的探险家心里可能想着只要重复这一策略，总有一天可以到达“至深之地”。</p><h3 id="SGD"><a href="#SGD" class="headerlink" title="SGD"></a>SGD</h3><p>用数学式可以将 SGD 写成如下的式：</p><p><img src="https://note.youdao.com/yws/api/personal/file/WEB4a7ab338358cd58a1c3a8536119076b0?method=download&amp;shareKey=8edfdc9a125f9cd9d011c92b4857b9ac" alt=""></p><p>现在，我们将 SGD 实现为一个 Python 类（为方便后面使用，我们将其实现为一个名为 <code>SGD</code> 的类）。</p><pre class=" language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">SGD</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">0.01</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        self<span class="token punctuation">.</span>lr <span class="token operator">=</span> lr    <span class="token keyword">def</span> <span class="token function">update</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> params<span class="token punctuation">,</span> grads<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">for</span> key <span class="token keyword">in</span> params<span class="token punctuation">.</span>keys<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>            params<span class="token punctuation">[</span>key<span class="token punctuation">]</span> <span class="token operator">-=</span> self<span class="token punctuation">.</span>lr <span class="token operator">*</span> grads<span class="token punctuation">[</span>key<span class="token punctuation">]</span></code></pre><p>这里，进行初始化时的参数 <code>lr</code> 表示 learning rate（学习率）。这个学习率会保存为实例变量。此外，代码段中还定义了 <code>update(params, grads)</code> 方法，这个方法在 SGD 中会被反复调用。参数 <code>params</code> 和 <code>grads</code>（与之前的神经网络的实现一样）是字典型变量，按 <code>params[&#39;W1&#39;]</code>、<code>grads[&#39;W1&#39;]</code> 的形式，分别保存了权重参数和它们的梯度。</p><p>使用这个 <code>SGD</code> 类，可以按如下方式进行神经网络的参数的更新（下面的代码是不能实际运行的伪代码）。</p><pre class=" language-python"><code class="language-python">network <span class="token operator">=</span> TwoLayerNet<span class="token punctuation">(</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span>optimizer <span class="token operator">=</span> SGD<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span><span class="token number">10000</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>    x_batch<span class="token punctuation">,</span> t_batch <span class="token operator">=</span> get_mini_batch<span class="token punctuation">(</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># mini-batch</span>    grads <span class="token operator">=</span> network<span class="token punctuation">.</span>gradient<span class="token punctuation">(</span>x_batch<span class="token punctuation">,</span> t_batch<span class="token punctuation">)</span>    params <span class="token operator">=</span> network<span class="token punctuation">.</span>params    optimizer<span class="token punctuation">.</span>update<span class="token punctuation">(</span>params<span class="token punctuation">,</span> grads<span class="token punctuation">)</span></code></pre><p>这里首次出现的变量名 <code>optimizer</code> 表示“进行最优化的人”的意思，这里由 SGD 承担这个角色。参数的更新由 <code>optimizer</code> 负责完成。我们在这里需要做的只是将参数和梯度的信息传给 <code>optimizer</code>。</p><p>像这样，通过单独实现进行最优化的类，功能的模块化变得更简单。比如，后面我们马上会实现另一个最优化方法 Momentum，它同样会实现成拥有 <code>update(params, grads)</code> 这个共同方法的形式。这样一来，只需要将 <code>optimizer = SGD()</code> 这一语句换成 <code>optimizer = Momentum()</code>，就可以从 SGD 切换为 Momentum。</p><pre class=" language-text"><code class="language-text">很多深度学习框架都实现了各种最优化方法，并且提供了可以简单切换这些方法的构造。比如 Lasagne 深度学习框架，在 updates.py 这个文件中以函数的形式集中实现了最优化方法。用户可以从中选择自己想用的最优化方法。</code></pre><h3 id="SGD-的缺点"><a href="#SGD-的缺点" class="headerlink" title="SGD 的缺点"></a>SGD 的缺点</h3><p>虽然 SGD 简单，并且容易实现，但是在解决某些问题时可能没有效率。这里，在指出 SGD 的缺点之际，我们来思考一下求下面这个函数的最小值的问题。</p><p>$f(x,y)=\frac{1}{20}x^2+y^2\quad\quad\quad\quad\quad$ (6.2)</p><p>SGD 的缺点是，如果函数的形状非均向（anisotropic），比如呈延伸状，搜索的路径就会非常低效。</p><p>SGD 低效的根本原因是，<strong>梯度的方向并没有指向最小值的方向</strong>。</p><p>比如下图效果：<br><img src="https://note.youdao.com/yws/api/personal/file/WEBab4e561f3a958d432c01c883d7f62acb?method=download&amp;shareKey=ce0ff16b29d0006ce53a96719a8312de" alt=""><br><strong>图 6-3</strong> 基于 SGD 的最优化的更新路径：呈“之”字形朝最小值 (0, 0) 移动，效率低</p><h3 id="Momentum"><a href="#Momentum" class="headerlink" title="Momentum"></a>Momentum</h3><p>Momentum 是“动量”的意思，和物理有关。用数学式表示 Momentum 方法，如下所示：</p><p><img src="https://note.youdao.com/yws/api/personal/file/WEB4cc83eabb5a5764ab8cb7a574e61e04d?method=download&amp;shareKey=4a9a3b4e0b54e0a9bf995b4d1386c6ee" alt=""></p><p><img src="https://note.youdao.com/yws/api/personal/file/WEB3ce41e644db7900e3ef31c68a9207a12?method=download&amp;shareKey=6aed247dd3199388db6eb8d690ff5183" alt=""></p><p>和前面的 SGD 一样，W 表示要更新的权重参数</p><p>效果：</p><p><img src="https://note.youdao.com/yws/api/personal/file/WEB98f63d86b2a27a31e051a123311478a0?method=download&amp;shareKey=1b6263efef55978d51d05753c0600351" alt=""><br><strong>图 6-5</strong>基于 Momentum 的最优化的更新路径</p><h3 id="AdaGrad"><a href="#AdaGrad" class="headerlink" title="AdaGrad"></a>AdaGrad</h3><p>在神经网络的学习中，学习率（数学式中记为 η）的值很重要。学习率过小，会导致学习花费过多时间；反过来，学习率过大，则会导致学习发散而不能正确进行。</p><p>在关于学习率的有效技巧中，有一种被称为学习率衰减（learning rate decay）的方法，即随着学习的进行，使学习率逐渐减小。实际上，一开始“多”学，然后逐渐“少”学的方法，在神经网络的学习中经常被使用。</p><p>逐渐减小学习率的想法，相当于将“全体”参数的学习率值一起降低。而 AdaGrad 进一步发展了这个想法，针对“一个一个”的参数，赋予其“定制”的值。</p><p>AdaGrad 会为参数的每个元素适当地调整学习率，与此同时进行学习（AdaGrad 的 Ada 来自英文单词 Adaptive，即“适当的”的意思）。下面，让我们用数学式表示 AdaGrad 的更新方法。</p><p><img src="https://note.youdao.com/yws/api/personal/file/WEB4478b29a94033141183b442da904cfc6?method=download&amp;shareKey=c232c455d88eaeea4f263f6e3ca2e92f" alt=""></p><p><img src="https://note.youdao.com/yws/api/personal/file/WEB2d1db5569dd1f888005c57430900c19f?method=download&amp;shareKey=f8d5db483cc8708630ab930132f112e9" alt=""></p><p>和前面的 SGD 一样，W 表示要更新的权重参数。</p><pre class=" language-text"><code class="language-text">AdaGrad 会记录过去所有梯度的平方和。因此，学习越深入，更新的幅度就越小。实际上，如果无止境地学习，更新量就会变为 0，完全不再更新。为了改善这个问题，可以使用 RMSProp[7]方法。RMSProp 方法并不是将过去所有的梯度一视同仁地相加，而是逐渐地遗忘过去的梯度，在做加法运算时将新梯度的信息更多地反映出来。这种操作从专业上讲，称为“指数移动平均”，呈指数函数式地减小过去的梯度的尺度。</code></pre><p>现在，让我们试着使用 AdaGrad 解决式（6.2）的最优化问题，结果如图 6-6 所示。</p><p><img src="https://note.youdao.com/yws/api/personal/file/WEBf2ab2d990547d9e0b0f1abc6c4755356?method=download&amp;shareKey=76f21ec0251550286eacbb49beef392c" alt=""><br><strong>图6-6</strong> 基于 AdaGrad 的最优化的更新路径</p><p>由图 6-6 的结果可知，函数的取值高效地向着最小值移动。由于 <em>y</em> 轴方向上的梯度较大，因此刚开始变动较大，但是后面会根据这个较大的变动按比例进行调整，减小更新的步伐。因此，<em>y</em> 轴方向上的更新程度被减弱，“之”字形的变动程度有所衰减。</p><h3 id="Adam"><a href="#Adam" class="headerlink" title="Adam"></a>Adam</h3><p>Momentum 参照小球在碗中滚动的物理规则进行移动，AdaGrad 为参数的每个元素适当地调整更新步伐。如果将这两个方法融合在一起会怎么样呢？这就是 Adam[8] 方法的基本思路 {1[这里关于 Adam 方法的说明只是一个直观的说明，并不完全正确。详细内容请参考原作者的论文。]}。</p><p>Adam 是 2015 年提出的新方法。它的理论有些复杂，直观地讲，就是融合了 Momentum 和 AdaGrad 的方法。通过组合前面两个方法的优点，有望实现参数空间的高效搜索。此外，进行超参数的“偏置校正”也是 Adam 的特征。</p><p><img src="https://note.youdao.com/yws/api/personal/file/WEB06a7d141e6779a8fb96c63dd00c7804d?method=download&amp;shareKey=669cf0f7c18da37d04a1b061ad6e13f2" alt=""><br><strong>图 6-7</strong> 基于 Adam 的最优化的更新路径</p><p>基于 Adam 的更新过程就像小球在碗中滚动一样。虽然 Momentun 也有类似的移动，但是相比之下，Adam 的小球左右摇晃的程度有所减轻。这得益于学习的更新程度被适当地调整了。</p><pre class=" language-python"><code class="language-python">Adam 会设置 <span class="token number">3</span> 个超参数。一个是学习率（论文中以 α 出现），另外两个是一次 momentum系数 β<span class="token number">1</span> 和二次 momentum系数 β<span class="token number">2</span>。根据论文，标准的设定值是 β<span class="token number">1</span> 为 <span class="token number">0.9</span>，β<span class="token number">2</span> 为 <span class="token number">0.999</span>。设置了这些值后，大多数情况下都能顺利运行。</code></pre><h3 id="使用哪种更新方法"><a href="#使用哪种更新方法" class="headerlink" title="使用哪种更新方法"></a>使用哪种更新方法</h3><p>如图 6-8 所示，根据使用的方法不同，参数更新的路径也不同。只看这个图的话，AdaGrad 似乎是最好的，不过也要注意，结果会根据要解决的问题而变。并且，很显然，超参数（学习率等）的设定值不同，结果也会发生变化。</p><p><img src="https://ws4.sinaimg.cn/large/006tKfTcly1g09h8pjhl6j30rs0ltwjf.jpg" alt="099"></p><p><strong>图 6-8　最优化方法的比较：SGD、Momentum、AdaGrad、Adam</strong></p><p>上面我们介绍了 SGD、Momentum、AdaGrad、Adam 这 4 种方法，那么用哪种方法好呢？非常遗憾，（目前）并不存在能在所有问题中都表现良好的方法。这 4 种方法各有各的特点，都有各自擅长解决的问题和不擅长解决的问题。</p><p>很多研究中至今仍在使用 SGD。Momentum 和 AdaGrad 也是值得一试的方法。最近，很多研究人员和技术人员都喜欢用 Adam。本书将主要使用 SGD 或者 Adam，读者可以根据自己的喜好多多尝试。</p><h3 id="基于-MNIST-数据集的更新方法"><a href="#基于-MNIST-数据集的更新方法" class="headerlink" title="基于 MNIST 数据集的更新方法"></a>基于 MNIST 数据集的更新方法</h3><p>我们以手写数字识别为例，比较前面介绍的 SGD、Momentum、AdaGrad、Adam 这 4 种方法，并确认不同的方法在学习进展上有多大程度的差异。先来看一下结果，如图 6-9 所示：</p><p><img src="https://ws1.sinaimg.cn/large/006tKfTcly1g09hcfyiltj30hs0dcjt8.jpg" alt="image-20190217160248492"></p><p><strong>图 6-9　基于 MNIST 数据集的 4 种更新方法的比较：横轴表示学习的迭代次数（iteration），纵轴表示损失函数的值（loss）</strong></p><p>这个实验以一个 5 层神经网络为对象，其中每层有 100 个神经元。激活函数使用的是 ReLU。</p><p>从图 6-9 的结果中可知，与 SGD 相比，其他 3 种方法学习得更快，而且速度基本相同，仔细看的话，AdaGrad 的学习进行得稍微快一点。这个实验需要注意的地方是，实验结果会随学习率等超参数、神经网络的结构（几层深等）的不同而发生变化。不过，<strong>一般而言，与 SGD 相比，其他 3 种方法可以学习得更快，有时最终的识别精度也更高。</strong></p><h2 id="权重的初始值"><a href="#权重的初始值" class="headerlink" title="权重的初始值"></a>权重的初始值</h2><p>在神经网络的学习中，权重的初始值特别重要。实际上，设定什么样的权重初始值，经常关系到神经网络的学习能否成功。</p><h3 id="可以将权重初始值设为0吗"><a href="#可以将权重初始值设为0吗" class="headerlink" title="可以将权重初始值设为0吗"></a>可以将权重初始值设为0吗</h3><p>后面我们会介绍抑制过拟合、提高泛化能力的技巧——权值衰减（weight decay）。简单地说，权值衰减就是一种以减小权重参数的值为目的进行学习的方法。通过减小权重参数的值来抑制过拟合的发生。</p><p>如果想减小权重的值，一开始就将初始值设为较小的值才是正途。实际上，在这之前的权重初始值都是像 <code>0.01 * np.random.randn(10, 100)</code> 这样，使用由高斯分布生成的值乘以 0.01 后得到的值（标准差为 0.01 的高斯分布）。</p><p>如果我们把权重初始值全部设为 0 以减小权重的值，会怎么样呢？从结论来说，将权重初始值设为 0 不是一个好主意。事实上，将权重初始值设为 0 的话，将无法正确进行学习。</p><p>为什么不能将权重初始值设为 0 呢？严格地说，为什么不能将权重初始值设成一样的值呢？这是因为在误差反向传播法中，所有的权重值都会进行相同的更新。比如，在 2 层神经网络中，假设第 1 层和第 2 层的权重为 0。这样一来，正向传播时，因为输入层的权重为 0，所以第 2 层的神经元全部会被传递相同的值。第 2 层的神经元中全部输入相同的值，这意味着反向传播时第 2 层的权重全部都会进行相同的更新（回忆一下“乘法节点的反向传播”的内容）。因此，权重被更新为相同的值，并拥有了对称的值（重复的值）。这使得神经网络拥有许多不同的权重的意义丧失了。为了防止“权重均一化”（严格地讲，是为了瓦解权重的对称结构），必须随机生成初始值。</p><h3 id="隐藏层的激活值得分布"><a href="#隐藏层的激活值得分布" class="headerlink" title="隐藏层的激活值得分布"></a>隐藏层的激活值得分布</h3><p>观察隐藏层的激活值 {2[这里我们将激活函数的输出数据称为“激活值”，但是有的文献中会将在层之间流动的数据也称为“激活值”。]}（激活函数的输出数据）的分布，可以获得很多启发。这里，我们来做一个简单的实验，观察权重初始值是如何影响隐藏层的激活值的分布的。这里要做的实验是，向一个 5 层神经网络（激活函数使用 sigmoid 函数）传入随机生成的输入数据，用直方图绘制各层激活值的数据分布。</p><p><img src="https://ws3.sinaimg.cn/large/006tKfTcly1g09i40dojjj30rs07ugn1.jpg" alt="101"></p><p><strong>图 6-10　使用标准差为 1 的高斯分布作为权重初始值时的各层激活值的分布</strong></p><p>从图 6-10 可知，各层的激活值呈偏向 0 和 1 的分布。这里使用的 sigmoid 函数是 S 型函数，随着输出不断地靠近 0（或者靠近 1），它的导数的值逐渐接近 0。因此，偏向 0 和 1 的数据分布会造成反向传播中梯度的值不断变小，最后消失。这个问题称为<strong>梯度消失</strong>（gradient vanishing）。层次加深的深度学习中，梯度消失的问题可能会更加严重。</p><p>使用标准差为 0.01 的高斯分布时，各层的激活值的分布如图 6-11 所示。</p><p><img src="https://ws1.sinaimg.cn/large/006tKfTcly1g09i52h1suj30rs07u3zo.jpg" alt="102"></p><p><strong>图 6-11　使用标准差为 0.01 的高斯分布作为权重初始值时的各层激活值的分布</strong></p><p>这次呈集中在 0.5 附近的分布。因为不像刚才的例子那样偏向 0 和 1，所以不会发生梯度消失的问题。但是，激活值的分布有所偏向，说明在表现力上会有很大问题。为什么这么说呢？因为如果有多个神经元都输出几乎相同的值，那它们就没有存在的意义了。比如，如果 100 个神经元都输出几乎相同的值，那么也可以由 1 个神经元来表达基本相同的事情。因此，激活值在分布上有所偏向会出现“表现力受限”的问题。</p><pre class=" language-text"><code class="language-text">各层的激活值的分布都要求有适当的广度。为什么呢？因为通过在各层间传递多样性的数据，神经网络可以进行高效的学习。反过来，如果传递的是有所偏向的数据，就会出现梯度消失或者“表现力受限”的问题，导致学习可能无法顺利进行。</code></pre><p>接着，我们尝试使用 Xavier Glorot 等人的论文 [9] 中推荐的权重初始值（俗称“Xavier 初始值”）。现在，在一般的深度学习框架中，Xavier 初始值已被作为标准使用。比如，Caffe 框架中，通过在设定权重初始值时赋予 xavier 参数，就可以使用 Xavier 初始值。</p><p>Xavier 的论文中，为了使各层的激活值呈现出具有相同广度的分布，推导了合适的权重尺度。推导出的结论是，如果前一层的节点数为 <em>n</em>，则初始值使用标准差为 $\frac{1}{\sqrt{n}}$ 的分布。</p><p><img src="https://ws4.sinaimg.cn/large/006tKfTcly1g09i9xxoywj30rs0g6n08.jpg" alt="103"></p><p> <strong>6-12　Xavier 初始值：与前一层有 n 个节点连接时，初始值使用标准差为$\frac{1}{\sqrt{n}}$的分布</strong></p><p>使用 Xavier 初始值后，前一层的节点数越多，要设定为目标节点的初始值的权重尺度就越小。</p><p>使用 Xavier 初始值后的结果如图 6-13 所示。从这个结果可知，越是后面的层，图像变得越歪斜，但是呈现了比之前更有广度的分布。因为各层间传递的数据有适当的广度，所以 sigmoid 函数的表现力不受限制，有望进行高效的学习。</p><p><img src="https://ws3.sinaimg.cn/large/006tKfTcly1g09ibocrj6j30rs07ugmx.jpg" alt="104"></p><p><strong>图 6-13　使用 Xavier 初始值作为权重初始值时的各层激活值的分布</strong></p><pre class=" language-text"><code class="language-text">图 6-13 的分布中，后面的层的分布呈稍微歪斜的形状。如果用 tanh 函数（双曲线函数）代替 sigmoid 函数，这个稍微歪斜的问题就能得到改善。实际上，使用 tanh 函数后，会呈漂亮的吊钟型分布。tanh 函数和 sigmoid 函数同是 S 型曲线函数，但 tanh 函数是关于原点 (0, 0) 对称的 S 型曲线，而 sigmoid 函数是关于 (x, y)=(0, 0.5) 对称的 S 型曲线。众所周知，用作激活函数的函数最好具有关于原点对称的性质。</code></pre><h3 id="ReLU-的权重初始值"><a href="#ReLU-的权重初始值" class="headerlink" title="ReLU 的权重初始值"></a>ReLU 的权重初始值</h3><p>Xavier 初始值是以激活函数是线性函数为前提而推导出来的。因为 <code>sigmoid</code> 函数和 <code>tanh</code> 函数左右对称，且中央附近可以视作线性函数，所以适合使用 Xavier 初始值。但当激活函数使用 ReLU 时，一般推荐使用 ReLU 专用的初始值，也就是 Kaiming He 等人推荐的初始值，也称为“He 初始值”[10]。当前一层的节点数为 <em>n</em> 时，He 初始值使用标准差为 $ \sqrt{\frac{2}{n}}$ 的高斯分布。直观上）可以解释为，因为 ReLU 的负值区域的值为 0，为了使它更有广度，所以需要 2 倍的系数。</p><p>现在来看一下激活函数使用 ReLU 时激活值的分布。我们给出了 3 个实验的结果（图 6-14），依次是权重初始值为标准差是 0.01 的高斯分布（下文简写为“std = 0.01”）时、初始值为 Xavier 初始值时、初始值为 ReLU 专用的“He 初始值”时的结果。</p><p><img src="https://ws4.sinaimg.cn/large/006tKfTcly1g09iendfpfj30rs0w043v.jpg" alt="105"></p><p><strong>图 6-14　激活函数使用 ReLU 时，不同权重初始值的激活值分布的变化</strong></p><p>总结一下，当激活函数使用 ReLU 时，权重初始值使用 He 初始值，当激活函数为 sigmoid 或 tanh 等 S 型曲线函数时，初始值使用 Xavier 初始值。这是目前的最佳实践。</p><h3 id="基于-MINIST-数据集的权重初始值的比较"><a href="#基于-MINIST-数据集的权重初始值的比较" class="headerlink" title="基于 MINIST 数据集的权重初始值的比较"></a>基于 MINIST 数据集的权重初始值的比较</h3><p>下面通过实际的数据，观察不同的权重初始值的赋值方法会在多大程度上影响神经网络的学习。这里，我们基于 std = 0.01、Xavier 初始值、He 初始值进行实验。</p><p><img src="https://ws1.sinaimg.cn/large/006tKfTcly1g09iv73j6cj30hs0dcwfg.jpg" alt="image-20190217165524883"></p><p><strong>图 6-15　基于 MNIST 数据集的权重初始值的比较：横轴是学习的迭代次数（iterations），纵轴是损失函数的值（loss）</strong></p><p>这个实验中，神经网络有 5 层，每层有 100 个神经元，激活函数使用的是 ReLU。从图 6-15 的结果可知，std = 0.01 时完全无法进行学习。这和刚才观察到的激活值的分布一样，是因为正向传播中传递的值很小（集中在 0 附近的数据）。因此，逆向传播时求到的梯度也很小，权重几乎不进行更新。相反，当权重初始值为 Xavier 初始值和 He 初始值时，学习进行得很顺利。并且，我们发现 He 初始值时的学习进度更快一些。</p><p>综上，在神经网络的学习中，权重初始值非常重要。很多时候权重初始值的设定关系到神经网络的学习能否成功。权重初始值的重要性容易被忽视，而任何事情的开始（初始值）总是关键的，因此在结束本节之际，再次强调一下权重初始值的重要性。</p><h2 id="Batch-Normalization"><a href="#Batch-Normalization" class="headerlink" title="Batch Normalization"></a>Batch Normalization</h2><p>在上一节，我们观察了各层的激活值分布，并从中了解到如果设定了合适的权重初始值，则各层的激活值分布会有适当的广度，从而可以顺利地进行学习。那么，为了使各层拥有适当的广度，“强制性”地调整激活值的分布会怎样呢？实际上，Batch Normalization[11] 方法就是基于这个想法而产生的。</p><h3 id="Batch-Normalization-的算法"><a href="#Batch-Normalization-的算法" class="headerlink" title="Batch Normalization 的算法"></a>Batch Normalization 的算法</h3><p>Batch Normalization（下文简称 Batch Norm）是 2015 年提出的方法。Batch Norm 虽然是一个问世不久的新方法，但已经被很多研究人员和技术人员广泛使用。实际上，看一下机器学习竞赛的结果，就会发现很多通过使用这个方法而获得优异结果的例子。</p><p>为什么 Batch Norm 这么惹人注目呢？因为 Batch Norm 有以下优点。</p><ul><li>可以使学习快速进行（可以增大学习率）。</li><li>不那么依赖初始值（对于初始值不用那么神经质）。</li><li>抑制过拟合（降低 Dropout 等的必要性）。</li></ul><p>考虑到深度学习要花费很多时间，第一个优点令人非常开心。另外，后两点也可以帮我们消除深度学习的学习中的很多烦恼。</p><p>Batch Norm 的思路是调整各层的激活值分布使其拥有适当的广度。为此，要向神经网络中插入对数据分布进行正规化的层，即 Batch Normalization 层（下文简称 Batch Norm 层），如图 6-16 所示：</p><p><img src="https://note.youdao.com/yws/api/personal/file/WEB2206546724f27f4e3cab36e1f7844191?method=download&amp;shareKey=dbcc67390e813782bacd3f838bdc90f2" alt=""><br>图 6-16　使用了 Batch Normalization 的神经网络的例子（Batch Norm 层的背景为灰色）</p><p>Batch Norm，顾名思义，以进行学习时的 mini-batch 为单位，按 mini-batch 进行正规化。具体而言，就是进行使数据分布的均值为 0、方差为 1 的正规化。用数学式表示的话，如下所示。</p><p><img src="https://ws3.sinaimg.cn/large/006tKfTcly1g09j4w146ej30z00gxmy2.jpg" alt="dl_67"></p><p>通过将这个处理插入到激活函数的前面（或者后面）{5[文献 [11]、文献 [12] 等中有讨论（做过实验）应该把 Batch Normalization 插入到激活函数的前面还是后面。]}，可以减小数据分布的偏向。</p><p>接着，Batch Norm 层会对正规化后的数据进行缩放和平移的变换，用数学式可以如下表示。</p><p>$y_i\leftarrow\gamma\hat{x}_i+\beta\quad\quad\quad\quad\quad$</p><p>这里，<em>γ</em> 和 <em>β</em> 是参数。一开始 <em>γ</em> = 1，<em>β</em> = 0，然后再通过学习调整到合适的值。</p><p>上面就是 Batch Norm 的算法。这个算法是神经网络上的正向传播。如果使用第 5 章介绍的计算图，Batch Norm 可以表示为图 6-17。</p><p><img src="/Users/yuzhongchun/Downloads/108.png" alt="108"></p><p><strong>图 6-17　Batch Normalization 的计算图（引用自文献 [13]）</strong></p><p>Batch Norm 的反向传播的推导有些复杂，这里我们不进行介绍。不过如果使用图 6-17 的计算图来思考的话，Batch Norm 的反向传播或许也能比较轻松地推导出来。Frederik Kratzert 的博客“Understanding the backward pass through Batch Normalization Layer”[13] 里有详细说明。</p><h3 id="Batch-Normalization-的评估"><a href="#Batch-Normalization-的评估" class="headerlink" title="Batch Normalization 的评估"></a>Batch Normalization 的评估</h3><p>现在我们使用 Batch Norm 层进行实验。首先，使用 MNIST 数据集，观察使用Batch Norm 层和不使用 Batch Norm 层时学习的过程会如何变化，如果6-18所示：</p><p><img src="/Users/yuzhongchun/Downloads/109.png" alt="109"></p><p><strong>图 6-18　基于 Batch Norm 的效果：使用 Batch Norm 后，学习进行得更快了</strong></p><p>从图 6-18 的结果可知，使用 Batch Norm 后，学习进行得更快了。接着，给予不同的初始值尺度，观察学习的过程如何变化。图 6-19 是权重初始值的标准差为各种不同的值时的学习过程图。</p><p><strong>图 6-19　图中的实线是使用了 Batch Norm 时的结果，虚线是没有使用 Batch Norm 时的结果：图的标题处标明了权重初始值的标准差</strong></p><p>几乎所有的情况下都是使用 Batch Norm 时学习进行得更快。同时也可以发现，实际上，在不使用 Batch Norm 的情况下，如果不赋予一个尺度好的初始值，学习将完全无法进行。</p><p>综上，通过使用 Batch Norm，可以推动学习的进行。并且，对权重初始值变得健壮（“对初始值健壮”表示不那么依赖初始值）。Batch Norm 具备了如此优良的性质，一定能应用在更多场合中。</p><h2 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a>正则化</h2><p>机器学习的问题中，<strong>过拟合</strong>是一个很常见的问题。过拟合指的是只能拟合训练数据，但不能很好地拟合不包含在训练数据中的其他数据的状态。机器学习的目标是提高泛化能力，即便是没有包含在训练数据里的未观测数据，也希望模型可以进行正确的识别。我们可以制作复杂的、表现力强的模型，但是相应地，抑制过拟合的技巧也很重要。</p><h3 id="过拟合"><a href="#过拟合" class="headerlink" title="过拟合"></a>过拟合</h3><p>发生过拟合的原因，主要有以下两个。</p><ul><li>模型拥有大量参数、表现力强。</li><li>训练数据少。</li></ul><h3 id="权值衰减"><a href="#权值衰减" class="headerlink" title="权值衰减"></a>权值衰减</h3><p>权值衰减是一直以来经常被使用的一种抑制过拟合的方法。该方法通过在学习的过程中对大的权重进行惩罚，来抑制过拟合。很多过拟合原本就是因为权重参数取值过大才发生的。</p><h3 id="Dropout"><a href="#Dropout" class="headerlink" title="Dropout"></a>Dropout</h3><p>作为抑制过拟合的方法，前面我们介绍了为损失函数加上权重的 L2 范数的权值衰减方法。该方法可以简单地实现，在某种程度上能够抑制过拟合。但是，如果网络的模型变得很复杂，只用权值衰减就难以应对了。在这种情况下，我们经常会使用 Dropout [14] 方法。</p><p>Dropout 是一种在学习的过程中随机删除神经元的方法。训练时，随机选出隐藏层的神经元，然后将其删除。被删除的神经元不再进行信号的传递，如图 6-22 所示。训练时，每传递一次数据，就会随机选择要删除的神经元。然后，测试时，虽然会传递所有的神经元信号，但是对于各个神经元的输出，要乘上训练时的删除比例后再输出。</p><p><img src="https://note.youdao.com/yws/api/personal/file/WEB8f75985c91622e3bd3daad97d5a1ffb7?method=download&amp;shareKey=5200cd20aadf706bdfb8e34b26efaa8e" alt=""></p><p>图 6-22　Dropout 的概念图（引用自文献 [14]）：左边是一般的神经网络，右边是应用了 Dropout 的网络。Dropout 通过随机选择并删除神经元，停止向前传递信号</p><p>通过使用 Dropout，训练数据和测试数据的识别精度的差距变小了。并且，训练数据也没有到达 100% 的识别精度。像这样，通过使用 Dropout，即便是表现力强的网络，也可以抑制过拟合。</p><pre class=" language-text"><code class="language-text">机器学习中经常使用集成学习。所谓集成学习，就是让多个模型单独进行学习，推理时再取多个模型的输出的平均值。用神经网络的语境来说，比如，准备 5 个结构相同（或者类似）的网络，分别进行学习，测试时，以这 5 个网络的输出的平均值作为答案。实验告诉我们，通过进行集成学习，神经网络的识别精度可以提高好几个百分点。这个集成学习与 Dropout 有密切的关系。这是因为可以将 Dropout 理解为，通过在学习过程中随机删除神经元，从而每一次都让不同的模型进行学习。并且，推理时，通过对神经元的输出乘以删除比例（比如，0.5 等），可以取得模型的平均值。也就是说，可以理解成，Dropout将集成学习的效果（模拟地）通过一个网络实现了。</code></pre><h2 id="超参数的验证"><a href="#超参数的验证" class="headerlink" title="超参数的验证"></a>超参数的验证</h2><p>神经网络中，除了权重和偏置等参数，超参数（hyper-parameter）也经常出现。这里所说的超参数是指，比如各层的神经元数量、batch 大小、参数更新时的学习率或权值衰减等。如果这些超参数没有设置合适的值，模型的性能就会很差。虽然超参数的取值非常重要，但是在决定超参数的过程中一般会伴随很多的试错。</p><h3 id="验证数据"><a href="#验证数据" class="headerlink" title="验证数据"></a>验证数据</h3><p><strong>训练数据</strong>用于参数（权重和偏置）的学习，<strong>验证数据</strong>用于超参数的性能评估。为了确认泛化能力，要在最后使用（比较理想的是只用一次）<strong>测试数据</strong>。</p><p>根据不同的数据集，有的会事先分成训练数据、验证数据、测试数据三部分，有的只分成训练数据和测试数据两部分，有的则不进行分割。在这种情况下，用户需要自行进行分割。</p><h3 id="超参数的最优化"><a href="#超参数的最优化" class="headerlink" title="超参数的最优化"></a>超参数的最优化</h3><p>进行超参数的最优化时，逐渐缩小超参数的“好值”的存在范围非常重要。所谓逐渐缩小范围，是指一开始先大致设定一个范围，从这个范围中随机选出一个超参数（采样），用这个采样到的值进行识别精度的评估；然后，多次重复该操作，观察识别精度的结果，根据这个结果缩小超参数的“好值”的范围。通过重复这一操作，就可以逐渐确定超参数的合适范围。</p><p>在进行神经网络的超参数的最优化时，与网格搜索等有规律的搜索相比，随机采样的搜索方式效果更好。这是因为在多个超参数中，各个超参数对最终的识别精度的影响程度不同。</p><p>在超参数的最优化中，要注意的是深度学习需要很长时间（比如，几天或几周）。因此，在超参数的搜索中，需要尽早放弃那些不符合逻辑的超参数。于是，在超参数的最优化中，减少学习的 epoch，缩短一次评估所需的时间是一个不错的办法。</p><p>超参数的最优化步骤：</p><pre><code>步骤 0    设定超参数的范围步骤 1    从设定的超参数范围中随机采样步骤 2    使用步骤1中采样到的超参数的值进行学习，通过验证数据评估识别精度（但是要将 epoch 设置得很小）步骤 3    重复步骤 1 和步骤 2 (100次等)，根据它们的识别精度的结果，缩小超参数的范围。</code></pre><p>反复进行上述操作，不断缩小超参数的范围，在缩小到一定程度时，从该范围中选出一个超参数的值。这就是进行超参数的最优化的一种方法。</p><h3 id="超参数最优化的实现"><a href="#超参数最优化的实现" class="headerlink" title="超参数最优化的实现"></a>超参数最优化的实现</h3><p>观察可以使学习顺利进行的超参数的范围，从而缩小值的范围。然后，在这个缩小的范围中重复相同的操作。这样就能缩小到合适的超参数的存在范围，然后在某个阶段，选择一个最终的超参数的值。</p><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>本章介绍神经网络学习中的几个重要技巧：参数的更新方法、权重初始值得赋值方法、Batch Normalization、Dropout等，这些都是现在神经网络中不可或缺的技术。</p><ul><li>参数的更新方法，除了 SGD 之外，还有 Momentum、AdaGrad、Adam等方法。</li><li>权重初始值的赋值方法对进行正确的学习非常重要。</li><li>作为权重初始值，Xavier 初始值、He 初始值等比较有效。</li><li>通过使用 Batch Normalization，可以加速学习，并且对初始值变得健壮。</li><li>抑制过拟合的正则化技术有权值衰减、Dropout等。</li><li>逐渐缩小“好值”存在的范围是搜索超参数的一个有效的方法。</li></ul><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol><li><a href="http://www.ituring.com.cn/book/1921" target="_blank" rel="noopener">深度学习入门：基于Python的理论与实现</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> Deep Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Notes </tag>
            
            <tag> Deep Learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Notes-深度学习入门之反向传播算法</title>
      <link href="/2018/11/11/29-dl-backward-propagation-algorithm-notes/"/>
      <url>/2018/11/11/29-dl-backward-propagation-algorithm-notes/</url>
      
        <content type="html"><![CDATA[<p>上一章中，我们介绍了神经网络的学习，并通过数值微分计算了神经网络的权重参数的梯度（严格来说，是损失函数关于权重参数的梯度）。数值微分虽然简单，也容易实现，但<strong>缺点</strong>是计算上比较费时间。本章我们将学习一个能够高效计算权重参数的梯度的方法——误差反向传播法。</p><p>要正确理解误差反向传播法，我个人认为有两种方法：一种是<strong>基于数学式</strong>；另一种是<strong>基于计算图</strong>（computational graph）。前者是比较常见的方法，机器学习相关的图书中多数都是以数学式为中心展开论述的。因为这种方法严密且简洁，所以确实非常合理，但如果一上来就围绕数学式进行探讨，会忽略一些根本的东西，止步于式子的罗列。因此，本章希望大家通过计算图，直观地理解误差反向传播法。然后，再结合实际的代码加深理解，相信大家一定会有种“原来如此！”的感觉。</p><h2 id="计算图"><a href="#计算图" class="headerlink" title="计算图"></a>计算图</h2><p>计算图将计算过程用图形表示出来。这里说的图形是数据结构图，通过多个节点和边表示（连接节点的直线称为“边”）。为了让大家熟悉计算图，本节先用计算图解一些简单的问题。从这些简单的问题开始，逐步深入，最终抵达误差反向传播法。</p><h3 id="用计算图求解"><a href="#用计算图求解" class="headerlink" title="用计算图求解"></a>用计算图求解</h3><p><strong>问题 1</strong>：太郎在超市买了 2 个 100 日元一个的苹果，消费税是 10%，请计算支付金额。</p><p><img src="https://ws3.sinaimg.cn/large/006tKfTcly1g076nlaz88j30rs08idh5.jpg" alt="059"></p><p><strong>图 5-2　基于计算图求解的问题 1 的答案：“苹果的个数”和“消费税”作为变量标在○外面</strong></p><p>用计算图解题的情况下，需要按如下流程进行：</p><ul><li>构建计算图</li><li>在计算图上，从左向右进行计算</li></ul><p>这里的第 2 歩“从左向右进行计算”是一种正方向上的传播，简称为正向传播（forward propagation）。正向传播是从计算图出发点到结束点的传播。既然有正向传播这个名称，当然也可以考虑反向（从图上看的话，就是从右向左）的传播。实际上，这种传播称为反向传播（backward propagation）。反向传播将在接下来的导数计算中发挥重要作用。</p><h3 id="局部计算"><a href="#局部计算" class="headerlink" title="局部计算"></a>局部计算</h3><p>计算图可以集中精力于局部计算。无论全局的计算有多么复杂，各个步骤所要做的就是对象节点的局部计算。虽然局部计算非常简单，但是通过传递它的计算结果，可以获得全局的复杂计算的结果。</p><p>计算图将复杂的计算分割成简单的局部计算，和流水线作业一样，将局部计算的结果传递给下一个节点。在将复杂的计算分解成简单的计算这一点上与汽车的组装有相似之处。</p><h3 id="为何用计算图解题"><a href="#为何用计算图解题" class="headerlink" title="为何用计算图解题"></a>为何用计算图解题</h3><p>计算图到底有什么优点：</p><ul><li>局部计算：无论全局是多么复杂的计算，都可以通过局部计算使各个节点致力于简单的计算，从而简化问题</li><li>利用计算图可以将中间的计算结果全部保存起来</li><li>使用计算图最大的原因是，可以通过反向传播高效计算导数</li></ul><p>综上，计算图的优点是，可以通过正向传播和反向传播高效地计算各个变量的导数值。</p><p>问题 1 中，我们计算了购买 2 个苹果时加上消费税最终需要支付的金额。这里，假设我们想知道苹果价格的上涨会在多大程度上影响最终的支付金额，即求“支付金额关于苹果的价格的导数”。</p><p><img src="https://ws1.sinaimg.cn/large/006tKfTcly1g076o6b09qj30rs08pgn4.jpg" alt="062"></p><p><strong>图 5-5　基于反向传播的导数的传递</strong></p><p>如图 5-5 所示，反向传播使用与正方向相反的箭头（粗线）表示。反向传播传递“局部导数”，将导数的值写在箭头的下方。在这个例子中，反向传播从右向左传递导数的值（1 → 1.1 → 2.2）。从这个结果中可知，“支付金额关于苹果的价格的导数”的值是 2.2。这意味着，如果苹果的价格上涨 1 日元，最终的支付金额会增加 2.2 日元（严格地讲，如果苹果的价格增加某个微小值，则最终的支付金额将增加那个微小值的 2.2 倍）。</p><p>这里只求了关于苹果的价格的导数，不过“支付金额关于消费税的导数”“支付金额关于苹果的个数的导数”等也都可以用同样的方式算出来。并且，计算中途求得的导数的结果（中间传递的导数）可以被共享，从而可以高效地计算多个导数。综上，计算图的优点是，可以通过正向传播和反向传播高效地计算各个变量的导数值。</p><h2 id="链式法则"><a href="#链式法则" class="headerlink" title="链式法则"></a>链式法则</h2><p>前面介绍的计算图的正向传播将计算结果正向（从左到右）传递，其计算过程是我们日常接触的计算过程，所以感觉上可能比较自然。而反向传播将局部导数向正方向的反方向（从右到左）传递，一开始可能会让人感到困惑。传递这个局部导数的原理，是基于<strong>链式法则</strong>（chain rule）的。本节将介绍链式法则，并阐明它是如何对应计算图上的反向传播的。</p><h3 id="计算图的反向传播"><a href="#计算图的反向传播" class="headerlink" title="计算图的反向传播"></a>计算图的反向传播</h3><p>计算图的反向传播：沿着与正方向相反的方向，乘上局部导数。</p><p>这就是反向传播的计算顺序。通过这样的计算，可以高效地求出导数的值，这是反向传播的要点。那么这是如何实现的呢？我们可以从链式法则的原理进行解释。下面我们就来介绍链式法则。</p><h3 id="什么是链式法则"><a href="#什么是链式法则" class="headerlink" title="什么是链式法则"></a>什么是链式法则</h3><p>介绍链式法则时，我们需要先从<strong>复合函数</strong>说起。复合函数是由多个函数构成的函数。</p><p>链式法则是关于<strong>复合函数</strong>的导数的性质：</p><pre class=" language-text"><code class="language-text">如果某个函数由复合函数表示，则该复合函数的导数可以用构成复合函数的各个函数的导数的乘积表示。</code></pre><h3 id="链式法则和计算图"><a href="#链式法则和计算图" class="headerlink" title="链式法则和计算图"></a>链式法则和计算图</h3><p>计算图：沿着与正方向相反的方向，乘上局部导数后传递</p><h2 id="反向传播"><a href="#反向传播" class="headerlink" title="反向传播"></a>反向传播</h2><p>上一节介绍了计算图的反向传播是基于链式法则成立的。本节将以“+”和“×”等运算为例，介绍反向传播的结构。</p><h3 id="加法节点的反向传播"><a href="#加法节点的反向传播" class="headerlink" title="加法节点的反向传播"></a>加法节点的反向传播</h3><p>加法节点的反向传播将上游的值原封不动地输出到下游。</p><h3 id="乘法节点的反向传播"><a href="#乘法节点的反向传播" class="headerlink" title="乘法节点的反向传播"></a>乘法节点的反向传播</h3><p><img src="https://ws1.sinaimg.cn/large/006tKfTcly1g0776h7kmej30rs0ba0tt.jpg" alt="070"></p><p><strong>图 5-12　乘法的反向传播：左图是正向传播，右图是反向传播</strong></p><p>乘法的反向传播会将上游的值乘以正向传播时的输入信号的“翻转值”后传递给下游。翻转值表示一种翻转关系，如图 5-12 所示，正向传播时信号是 x 的话，反向传播时则是 y；正向传播时信号是 y 的话，反向传播时则是 x。</p><p>法的反向传播只是将上游的值传给下游，并不需要正向传播的输入信号。但是，乘法的反向传播需要正向传播时的输入信号值。因此，实现乘法节点的反向传播时，要保存正向传播的输入信号。</p><h3 id="苹果的例子"><a href="#苹果的例子" class="headerlink" title="苹果的例子"></a>苹果的例子</h3><p>再来思考一下本章最开始举的购买苹果的例子（2 个苹果和消费税）。这里要解的问题是苹果的价格、苹果的个数、消费税这 3 个变量各自如何影响最终支付的金额。这个问题相当于求“支付金额关于苹果的价格的导数”“支付金额关于苹果的个数的导数”“支付金额关于消费税的导数”。用计算图的反向传播来解的话，求解过程如图 5-14 所示。</p><p><img src="https://ws4.sinaimg.cn/large/006tKfTcly1g0777ihyp8j30rs0a4wgh.jpg" alt="072"></p><p><strong>图 5-14　购买苹果的反向传播的例子</strong></p><h2 id="简单层的实现"><a href="#简单层的实现" class="headerlink" title="简单层的实现"></a>简单层的实现</h2><p>本节将用 Python 实现前面的购买苹果的例子。这里，我们把要实现的计算图的乘法节点称为“乘法层”（<code>MulLayer</code>），加法节点称为“加法层”（<code>AddLayer</code>）。</p><pre class=" language-text"><code class="language-text">下一节，我们将把构建神经网络的“层”实现为一个类。这里所说的“层”是神经网络中功能的单位。比如，负责 sigmoid 函数的 Sigmoid、负责矩阵乘积的 Affine 等，都以层为单位进行实现。因此，这里也以层为单位来实现乘法节点和加法节点。</code></pre><h3 id="乘法层的实现"><a href="#乘法层的实现" class="headerlink" title="乘法层的实现"></a>乘法层的实现</h3><p>层的实现中有两个共通的方法（接口）<code>forward()</code> 和 <code>backward()</code>。<code>forward()</code> 对应正向传播，<code>backward()</code> 对应反向传播。</p><p>现在来实现乘法层。乘法层作为 <code>MulLayer</code> 类，其实现过程如下所示</p><pre class=" language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">MulLayer</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        self<span class="token punctuation">.</span>x <span class="token operator">=</span> None        self<span class="token punctuation">.</span>y <span class="token operator">=</span> None    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">,</span> y<span class="token punctuation">)</span><span class="token punctuation">:</span>        self<span class="token punctuation">.</span>x <span class="token operator">=</span> x        self<span class="token punctuation">.</span>y <span class="token operator">=</span> y        out <span class="token operator">=</span> x <span class="token operator">*</span> y        <span class="token keyword">return</span> out    <span class="token keyword">def</span> <span class="token function">backward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> dout<span class="token punctuation">)</span><span class="token punctuation">:</span>        dx <span class="token operator">=</span> dout <span class="token operator">*</span> self<span class="token punctuation">.</span>y <span class="token comment" spellcheck="true"># 翻转x和y</span>        dy <span class="token operator">=</span> dout <span class="token operator">*</span> self<span class="token punctuation">.</span>x        <span class="token keyword">return</span> dx<span class="token punctuation">,</span> dy</code></pre><h3 id="加法层的实现"><a href="#加法层的实现" class="headerlink" title="加法层的实现"></a>加法层的实现</h3><pre class=" language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">AddLayer</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">pass</span>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">,</span> y<span class="token punctuation">)</span><span class="token punctuation">:</span>        out <span class="token operator">=</span> x <span class="token operator">+</span> y        <span class="token keyword">return</span> out    <span class="token keyword">def</span> <span class="token function">backward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> dout<span class="token punctuation">)</span><span class="token punctuation">:</span>        dx <span class="token operator">=</span> dout <span class="token operator">*</span> <span class="token number">1</span>        dy <span class="token operator">=</span> dout <span class="token operator">*</span> <span class="token number">1</span>        <span class="token keyword">return</span> dx<span class="token punctuation">,</span> dy</code></pre><h2 id="激活函数层的实现"><a href="#激活函数层的实现" class="headerlink" title="激活函数层的实现"></a>激活函数层的实现</h2><p>现在，我们将计算图的思路应用到神经网络中。这里，我们把构成神经网络的层实现为一个类。先来实现激活函数的 <code>ReLU</code> 层和 <code>Sigmoid</code> 层。</p><h3 id="ReLU-层"><a href="#ReLU-层" class="headerlink" title="ReLU 层"></a>ReLU 层</h3><p>激活函数 ReLU（Rectified Linear Unit）由下式（5.7）表示。</p><p><img src="https://ws2.sinaimg.cn/large/006tKfTcly1g077miwnvrj30x50680sw.jpg" alt="188"></p><p>通过式（5.7），可以求出 <em>y</em> 关于 <em>x</em> 的导数，如式（5.8）所示。</p><p><img src="https://ws2.sinaimg.cn/large/006tKfTcly1g077mllfkqj30xp06aq36.jpg" alt="189"></p><p>如果正向传播时的输入 x 大于 0，则反向传播会将上游的值原封不动地传给下游。反过来，如果正向传播时的 x 小于等于 0，则反向传播中传给下游的信号将停在此处。</p><p><img src="https://ws4.sinaimg.cn/large/006tKfTcly1g077quqdbaj30rs06o3zb.jpg" alt="076"></p><p><strong>图 5-18　ReLU 层的计算图</strong></p><pre class=" language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">Relu</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        self<span class="token punctuation">.</span>mask <span class="token operator">=</span> None    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>        self<span class="token punctuation">.</span>mask <span class="token operator">=</span> <span class="token punctuation">(</span>x <span class="token operator">&lt;=</span> <span class="token number">0</span><span class="token punctuation">)</span>        out <span class="token operator">=</span> x<span class="token punctuation">.</span>copy<span class="token punctuation">(</span><span class="token punctuation">)</span>        out<span class="token punctuation">[</span>self<span class="token punctuation">.</span>mask<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">0</span>        <span class="token keyword">return</span> out    <span class="token keyword">def</span> <span class="token function">backward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> dout<span class="token punctuation">)</span><span class="token punctuation">:</span>        dout<span class="token punctuation">[</span>self<span class="token punctuation">.</span>mask<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">0</span>        dx <span class="token operator">=</span> dout        <span class="token keyword">return</span> dx</code></pre><pre class=" language-text"><code class="language-text">ReLU 层的作用就像电路中的开关一样。正向传播时，有电流通过的话，就将开关设为 ON；没有电流通过的话，就将开关设为 OFF。反向传播时，开关为 ON 的话，电流会直接通过；开关为 OFF 的话，则不会有电流通过。</code></pre><h3 id="Sigmoid-层"><a href="#Sigmoid-层" class="headerlink" title="Sigmoid 层"></a>Sigmoid 层</h3><p>sigmoid 函数由式（5.9）表示。</p><p><img src="https://ws4.sinaimg.cn/large/006tKfTcly1g077r6uudtg30770150mq.gif" alt="gif"></p><p>用计算图表示式（5.9）的话，则如图 5-19 所示。</p><p><img src="https://ws2.sinaimg.cn/large/006tKfTcly1g077rplchoj30rs06hmxx.jpg" alt="077"></p><p>Sigmoid层的计算图如图5-20所示：</p><p><img src="/Users/yuzhongchun/Downloads/081.png" alt="081"></p><p><strong>图 5-20　Sigmoid 层的计算图</strong></p><p>最终，Sigmoid的计算图（简洁版）如5-21所示：</p><p><img src="https://note.youdao.com/yws/api/personal/file/WEBb80b06e14402edc2713c0a8786695e53?method=download&amp;shareKey=4bb2fbbed6e9ffafe47accefb1c087be" alt=""></p><p><strong>图 5-21　Sigmoid 层的计算图（简洁版）</strong></p><p>图 5-20 的计算图和简洁版的图 5-21 的计算图的计算结果是相同的，但是，简洁版的计算图可以省略反向传播中的计算过程，因此计算效率更高。此外，通过对节点进行集约化，可以不用在意 Sigmoid 层中琐碎的细节，而只需要专注它的输入和输出，这一点也很重要。</p><p>另外，可以进一步整理为：</p><p><img src="https://note.youdao.com/yws/api/personal/file/WEB060508652aac5da164d3394bc9777b6b?method=download&amp;shareKey=21050fc18d504494022c4ee20a31df92" alt=""></p><p>因此，图 5-21 所表示的 Sigmoid 层的反向传播，只根据正向传播的输出就能计算出来。</p><p><img src="https://note.youdao.com/yws/api/personal/file/WEB876fcd0a1642c652c8fd6052f57937f9?method=download&amp;shareKey=91e837041cddb8e988337a8c9ec7e0c7" alt=""></p><p><strong>图 5-22　Sigmoid 层的计算图：可以根据正向传播的输出 y 计算反向传播</strong></p><h2 id="Affine-Softmax-层的实现"><a href="#Affine-Softmax-层的实现" class="headerlink" title="Affine/Softmax 层的实现"></a>Affine/Softmax 层的实现</h2><h3 id="Affine-层"><a href="#Affine-层" class="headerlink" title="Affine 层"></a>Affine 层</h3><pre class=" language-text"><code class="language-text">神经网络的正向传播中进行的矩阵的乘积运算在几何学领域被称为“仿射变换”{1[几何中，仿射变换包括一次线性变换和一次平移，分别对应神经网络的加权和运算与加偏置运算。——译者注]}。因此，这里将进行仿射变换的处理实现为“Affine 层”。</code></pre><p>现在将这里进行的求矩阵的乘积与偏置的和的运算用计算图表示出来。将乘积运算用“dot”节点表示的话，则 <code>np.dot(X, W) + B</code> 的运算可用图 5-24 所示的计算图表示出来。另外，在各个变量的上方标记了它们的形状（比如，计算图上显示了 <strong>X</strong> 的形状为 (2,)，<strong>X</strong> · <strong>W</strong> 的形状为 (3,) 等）。</p><p><img src="https://ws4.sinaimg.cn/large/006tKfTcly1g08mxaad97j30rs0htdho.jpg" alt="085"></p><p><strong>图 5-24　Affine 层的计算图（注意变量是矩阵，各个变量的上方标记了该变量的形状）</strong></p><p>图 5-24 是比较简单的计算图，不过要注意 <strong>X</strong>、<strong>W</strong>、<strong>B</strong> 是矩阵（多维数组）。之前我们见到的计算图中各个节点间流动的是标量，而这个例子中各个节点间传播的是矩阵。</p><p>现在我们来考虑图 5-24 的计算图的反向传播。以矩阵为对象的反向传播，按矩阵的各个元素进行计算时，步骤和以标量为对象的计算图相同。实际写一下的话，可以得到下式（这里省略了式（5.13）的推导过程）。</p><p><img src="https://ws3.sinaimg.cn/large/006tKfTcly1g08n1dxv5mj30m806n74a.jpg" alt="5_13"></p><p>现在，我们根据式（5.13），尝试写出计算图的反向传播，如图 5-25 所示。</p><p><img src="https://ws1.sinaimg.cn/large/006tKfTcly1g08n9c2zaxj30rs0er0vl.jpg" alt="086"></p><p><strong>图 5-25　Affine 层的反向传播：注意变量是多维数组。反向传播时各个变量的下方标记了该变量的形状</strong></p><p><img src="https://ws4.sinaimg.cn/large/006tKfTcly1g08nbzvqu3j30rs0a9gnb.jpg" alt="087"></p><p><strong>图 5-26　矩阵的乘积（“dot”节点）的反向传播可以通过组建使矩阵对应维度的元素个数一致的乘积运算而推导出来</strong></p><h3 id="批版本的的-Affine-层"><a href="#批版本的的-Affine-层" class="headerlink" title="批版本的的 Affine 层"></a>批版本的的 Affine 层</h3><p>前面介绍的 Affine层的输入 <strong>X</strong> 是以单个数据为对象的。现在我们考虑 <em>N</em> 个数据一起进行正向传播的情况，也就是批版本的 Affine层。</p><p>先给出批版本的 Affine层的计算图，如图 5-27 所示。</p><p><img src="https://ws3.sinaimg.cn/large/006tKfTcly1g08ndvs01sj30rs0fy427.jpg" alt="088"></p><p><strong>图 5-27　批版本的 Affine 层的计算图</strong></p><h3 id="Softmax-with-Loss-层"><a href="#Softmax-with-Loss-层" class="headerlink" title="Softmax-with-Loss 层"></a>Softmax-with-Loss 层</h3><p>最后介绍一下输出层的 softmax 函数。前面我们提到过，softmax 函数会将输入值正规化之后再输出。比如手写数字识别时，Softmax 层的输出如图：<br><img src="https://note.youdao.com/yws/api/personal/file/WEB286e1df6cedf604becb8a12da92ca472?method=download&amp;shareKey=2a9b77c4b9d45dfe61c7096a1437f802" alt=""></p><p><strong>图 5-28</strong> 输入图像通过 Affine 层和 ReLU 层进行转换，10 个输入通过 Softmax 层进行正规化。在这个例子中，“0”的得分是 5.3，这个值经过 Softmax 层转换为 0.008（0.8%）；“2”的得分是 10.1，被转换为 0.991（99.1%）</p><p>在图 5-28 中，Softmax 层将输入值正规化（将输出值的和调整为 1）之后再输出。另外，因为手写数字识别要进行 10 类分类，所以向Softmax 层的输入也有 10 个。</p><pre class=" language-text"><code class="language-text">神经网络中进行的处理有推理（inference）和学习两个阶段。神经网络的推理通常不使用 Softmax 层。比如，用图 5-28 的网络进行推理时，会将最后一个 Affine 层的输出作为识别结果。神经网络中未被正规化的输出结果（图 5-28 中 Softmax 层前面的 Affine 层的输出）有时被称为“得分”。也就是说，当神经网络的推理只需要给出一个答案的情况下，因为此时只对得分最大值感兴趣，所以不需要 Softmax 层。不过，神经网络的学习阶段则需要 Softmax 层。</code></pre><p>下面来实现 Softmax 层。考虑到这里也包含作为损失函数的交叉熵误差（cross entropy error），所以称为“Softmax-with-Loss 层”。Softmax-with-Loss 层（Softmax 函数和交叉熵误差）的计算图如图 5-29 所示。</p><p><img src="https://ws1.sinaimg.cn/large/006tKfTcly1g08nuk2t8dj30rs096whk.jpg" alt="090"></p><p><strong>图 5-29　Softmax-with-Loss 层的计算图</strong></p><p>图 5-29 的计算图可以简化成图 5-30。</p><p>图 5-30 的计算图中，softmax 函数记为 Softmax 层，交叉熵误差记为 Cross Entropy Error 层。这里假设要进行 3 类分类，从前面的层接收 3 个输入（得分）。如图 5-30 所示，Softmax 层将输入(<em>a</em>1, <em>a</em>2, <em>a</em>3) 正规化，输出(<em>y</em>1, <em>y</em>2, <em>y</em>3)。Cross Entropy Error 层接收 Softmax 的输出(<em>y</em>1, <em>y</em>2, <em>y</em>3) 和教师标签(<em>t</em>1, <em>t</em>2, <em>t</em>3)，从这些数据中输出损失 <em>L</em>。</p><p><img src="https://ws1.sinaimg.cn/large/006tKfTcly1g08nvl0yrfj30rs0hv0tv.jpg" alt="091"></p><p><strong>图 5-30　“简易版”的 Softmax-with-Loss 层的计算图</strong></p><p>图 5-30 中要注意的是反向传播的结果。Softmax 层的反向传播得到了(<em>y</em>1-<em>t</em>1, <em>y</em>2-<em>t</em>2, <em>y</em>3-<em>t</em>3)这样“漂亮”的结果。由于(<em>y</em>1, <em>y</em>2, <em>y</em>3)是 Softmax 层的输出，(<em>t</em>1, <em>t</em>2, <em>t</em>3)是监督数据，所以(<em>y</em>1-<em>t</em>1, <em>y</em>2-<em>t</em>2, <em>y</em>3-<em>t</em>3)是 Softmax 层的输出和教师标签的差分。神经网络的反向传播会把这个差分表示的误差传递给前面的层，这是神经网络学习中的重要性质。</p><p>神经网络学习的目的就是通过调整权重参数，使神经网络的输出（Softmax 的输出）接近教师标签。因此，必须将神经网络的输出与教师标签的误差高效地传递给前面的层。刚刚的(y1-t1, y2-t2, y3-t3)正是 Softmax 层的输出与教师标签的差，直截了当地表示了当前神经网络的输出与教师标签的误差。</p><pre class=" language-text"><code class="language-text">使用交叉熵误差作为 softmax 函数的损失函数后，反向传播得到(y1-t1, y2-t2, y3-t3)这样“漂亮”的结果。实际上，这样“漂亮”的结果并不是偶然的，而是为了得到这样的结果，特意设计了交叉熵误差函数。回归问题中输出层使用“恒等函数”，损失函数使用“平方和误差”，也是出于同样的理由（3.5 节）。也就是说，使用“平方和误差”作为“恒等函数”的损失函数，反向传播才能得到(y1-t1, y2-t2, y3-t3)这样“漂亮”的结果。</code></pre><h2 id="误差反向传播法的实现"><a href="#误差反向传播法的实现" class="headerlink" title="误差反向传播法的实现"></a>误差反向传播法的实现</h2><p>通过像组装乐高积木一样组装上一节中实现的层，可以构建神经网络。本节我们将通过组装已经实现的层来构建神经网络。</p><h3 id="神经网络学习的全貌图"><a href="#神经网络学习的全貌图" class="headerlink" title="神经网络学习的全貌图"></a>神经网络学习的全貌图</h3><p>在进行具体的实现之前，我们再来确认一下神经网络学习的全貌图。神经网络学习的步骤如下所示。</p><pre><code>前提    神经网络中有合适的权重和偏置，调整权重和偏置以便拟合训练数据的过程称为学习。神经网络的学习分为下面 4 个步骤。步骤1： mini-batch    从训练数据中随机选择一部分数据。步骤2：计算梯度    计算损失函数关于各个权重参数的梯度。步骤3：更新参数    将权重参数沿梯度方向进行微小的更新。步骤4：重复    重复步骤 1、步骤 2、步骤 3。</code></pre><h3 id="对应误差反向传播法的神经网络的实现"><a href="#对应误差反向传播法的神经网络的实现" class="headerlink" title="对应误差反向传播法的神经网络的实现"></a>对应误差反向传播法的神经网络的实现</h3><p>像这样通过将神经网络的组成元素以层的方式实现，可以轻松地构建神经网络。这个用层进行模块化的实现具有很大优点。因为想另外构建一个神经网络（比如 5 层、10 层、20 层……的大的神经网络）时，只需像组装乐高积木那样添加必要的层就可以了。之后，通过各个层内部实现的正向传播和反向传播，就可以正确计算进行识别处理或学习所需的梯度。</p><h3 id="误差反向传播法的梯度确认"><a href="#误差反向传播法的梯度确认" class="headerlink" title="误差反向传播法的梯度确认"></a>误差反向传播法的梯度确认</h3><p>到目前为止，我们介绍了两种求梯度的方法。一种是基于数值微分的方法，另一种是解析性地求解数学式的方法。后一种方法通过使用误差反向传播法，即使存在大量的参数，也可以高效地计算梯度。因此，后文将不再使用耗费时间的数值微分，而是使用误差反向传播法求梯度。</p><p>数值微分的计算很耗费时间，而且如果有误差反向传播法的（正确的）实现的话，就没有必要使用数值微分的实现了。那么数值微分有什么用呢？实际上，在确认误差反向传播法的实现是否正确时，是需要用到数值微分的。</p><p>数值微分的优点是实现简单，因此，一般情况下不太容易出错。而误差反向传播法的实现很复杂，容易出错。所以，经常会比较数值微分的结果和误差反向传播法的结果，以确认误差反向传播法的实现是否正确。确认数值微分求出的梯度结果和误差反向传播法求出的结果是否一致（严格地讲，是非常相近）的操作称为梯度确认（gradient check）。</p><pre class=" language-text"><code class="language-text">数值微分和误差反向传播法的计算结果之间的误差为 0 是很少见的。这是因为计算机的计算精度有限（比如，32 位浮点数）。受到数值精度的限制，刚才的误差一般不会为 0，但是如果实现正确的话，可以期待这个误差是一个接近 0 的很小的值。如果这个值很大，就说明误差反向传播法的实现存在错误。</code></pre><h3 id="使用误差反向传播法的学习"><a href="#使用误差反向传播法的学习" class="headerlink" title="使用误差反向传播法的学习"></a>使用误差反向传播法的学习</h3><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>本章我们介绍了将计算过程可视化的计算图，并使用计算图，介绍了神经网络中的误差反向传播法，并以层为单位实现了神经网络中的处理。我们学过的层有 ReLU 层、Softmax-with-Loss 层、Affine 层、Softmax 层等，这些层中实现了 forward 和 backward 方法，通过将数据正向和反向地传播，可以高效地计算权重参数的梯度。通过使用层进行模块化，神经网络中可以自由地组装层，轻松构建出自己喜欢的网络。</p><ul><li>通过使用计算图，可以直观地把握计算过程。</li><li>计算图的节点是由局部计算构成的。局部计算构成全局计算。</li><li>计算图的正向传播进行一般的计算。通过计算图的反向传播，可以计算各个节点的导数。</li><li>通过将神经网络的组成元素实现为层，可以高效地计算梯度（反向传播法）。</li><li>通过比较数值微分和误差反向传播法的结果，可以确认误差反向传播法的实现是否正确（梯度确认）。</li></ul><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol><li><a href="http://www.ituring.com.cn/book/1921" target="_blank" rel="noopener">深度学习入门：基于Python的理论与实现</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> Deep Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Notes </tag>
            
            <tag> Deep Learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Machine Learning Guide</title>
      <link href="/2018/11/10/31-machine-learning-guide/"/>
      <url>/2018/11/10/31-machine-learning-guide/</url>
      
        <content type="html"><![CDATA[<p>Machine learning is a subset of artificial intelligence in the field of computer science that often uses statistical techniques to give computers the ability to “learn” (i.e., progressively improve performance on a specific task) with data, without being explicitly programmed.</p><h2 id="ML-Types"><a href="#ML-Types" class="headerlink" title="ML Types"></a>ML Types</h2><p><img src="https://ws1.sinaimg.cn/large/006tKfTcly1g0hf8a1ulfj30qj0m3jst.jpg" alt=""></p><h2 id="ML-Workflow"><a href="#ML-Workflow" class="headerlink" title="ML Workflow"></a>ML Workflow</h2><p><img src="https://ws4.sinaimg.cn/large/006tKfTcly1g0hegs6rnxj31c40u07on.jpg" alt=""></p><h2 id="ML-Steps"><a href="#ML-Steps" class="headerlink" title="ML Steps"></a>ML Steps</h2><p><img src="https://ws3.sinaimg.cn/large/006tKfTcly1g0hdl3ur2qj30u01jsgrn.jpg" alt=""></p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol><li><a href="https://www.coursera.org/learn/machine-learning" target="_blank" rel="noopener">Machine Learning Coursera</a></li><li><a href="https://book.douban.com/subject/10590856/" target="_blank" rel="noopener">统计学习方法 李航</a></li><li><a href="https://book.douban.com/subject/26708119/" target="_blank" rel="noopener">机器学习 周志华</a></li><li><a href="http://shop.oreilly.com/product/0636920052289.do" target="_blank" rel="noopener">Hands-On Machine Learning with Scikit-Learn and TensorFlow</a></li><li><a href="https://book.douban.com/subject/11542972/" target="_blank" rel="noopener">数据挖掘概念与技术  Jiawei Han</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Machine Learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Notes-深度学习入门之神经网络的学习</title>
      <link href="/2018/11/09/28-dl-neural-networks-learning-notes/"/>
      <url>/2018/11/09/28-dl-neural-networks-learning-notes/</url>
      
        <content type="html"><![CDATA[<p>神经网络的学习是指从训练数据中自动获取最优权重参数的过程。为了使神经网络能进行学习，将导入损失函数这一指标。而学习的目的就是以该损失函数为基准，找出能使它的值达到最小的权重参数。</p><h2 id="从数据中学习"><a href="#从数据中学习" class="headerlink" title="从数据中学习"></a>从数据中学习</h2><p>神经网络的特征就是可以从数据中学习。所谓“从数据中学习”，是指可以由数据自动决定权重参数的值。这是非常了不起的事情！因为如果所有的参数都需要人工决定的话，工作量就太大了。在第 2 章介绍的感知机的例子中，我们对照着真值表，人工设定了参数的值，但是那时的参数只有 3 个。而在实际的神经网络中，参数的数量成千上万，在层数更深的深度学习中，参数的数量甚至可以上亿，想要人工决定这些参数的值是不可能的。本章将介绍神经网络的学习，即利用数据决定参数值的方法，并用 Python 实现对 MNIST 手写数字数据集的学习。</p><h3 id="数据驱动"><a href="#数据驱动" class="headerlink" title="数据驱动"></a>数据驱动</h3><p>数据是机器学习的命根子。从数据中寻找答案、从数据中发现模式、根据数据讲故事……这些机器学习所做的事情，如果没有数据的话，就无从谈起。因此，数据是机器学习的核心。这种数据驱动的方法，也可以说脱离了过往以人为中心的方法。</p><p>通常要解决某个问题，特别是需要发现某种模式时，人们一般会综合考虑各种因素后再给出回答。“这个问题好像有这样的规律性？”“不对，可能原因在别的地方。”——类似这样，人们以自己的经验和直觉为线索，通过反复试验推进工作。而机器学习的方法则极力避免人为介入，尝试从收集到的数据中发现答案（模式）。神经网络或深度学习则比以往的机器学习方法更能避免人为介入。</p><p>如图 4-2 所示，神经网络直接学习图像本身。在第 2 个方法，即利用特征量和机器学习的方法中，特征量仍是由人工设计的，而在神经网络中，连图像中包含的重要特征量也都是由机器来学习的。</p><p><img src="https://ws2.sinaimg.cn/large/006tKfTcly1g0626whiaoj30rs0do760.jpg" alt="047"></p><p>图4-2 从人工设计规则转变为由机器从数据中学习：没有人为介入的方块用灰色表示</p><pre class=" language-text"><code class="language-text">深度学习有时也称为端到端机器学习（end-to-end machine learning）。这里所说的端到端是指从一端到另一端的意思，也就是从原始数据（输入）中获得目标结果（输出）的意思。</code></pre><p>神经网络的优点是对所有的问题都可以用同样的流程来解决。比如，不管要求解的问题是识别 5，还是识别狗，抑或是识别人脸，神经网络都是通过不断地学习所提供的数据，尝试发现待求解的问题的模式。也就是说，与待处理的问题无关，神经网络可以将数据直接作为原始数据，进行“端对端”的学习。</p><h3 id="训练数据和测试数据"><a href="#训练数据和测试数据" class="headerlink" title="训练数据和测试数据"></a>训练数据和测试数据</h3><p>机器学习中，一般将数据分为训练数据和测试数据两部分来进行学习和实验等。首先，使用训练数据进行学习，寻找最优的参数；然后，使用测试数据评价训练得到的模型的实际能力。为什么需要将数据分为训练数据和测试数据呢？因为我们追求的是模型的泛化能力。为了正确评价模型的泛化能力，就必须划分训练数据和测试数据。另外，训练数据也可以称为监督数据。</p><p>泛化能力是指处理未被观察过的数据（不包含在训练数据中的数据）的能力。获得泛化能力是机器学习的最终目标。比如，在识别手写数字的问题中，泛化能力可能会被用在自动读取明信片的邮政编码的系统上。此时，手写数字识别就必须具备较高的识别“某个人”写的字的能力。注意这里不是“特定的某个人写的特定的文字”，而是“任意一个人写的任意文字”。如果系统只能正确识别已有的训练数据，那有可能是只学习到了训练数据中的个人的习惯写法。</p><p>因此，仅仅用一个数据集去学习和评价参数，是无法进行正确评价的。这样会导致可以顺利地处理某个数据集，但无法处理其他数据集的情况。顺便说一下，只对某个数据集过度拟合的状态称为<strong>过拟合</strong>（over fitting）。避免过拟合也是机器学习的一个重要课题。</p><h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><p>神经网络的学习中所用的指标称为损失函数（loss function）。这个损失函数可以使用任意函数，但一般用均方误差和交叉熵误差等。</p><pre class=" language-text"><code class="language-text">损失函数是表示神经网络性能的“恶劣程度”的指标，即当前的神经网络对监督数据在多大程度上不拟合，在多大程度上不一致。以“性能的恶劣程度”为指标可能会使人感到不太自然，但是如果给损失函数乘上一个负值，就可以解释为“在多大程度上不坏”，即“性能有多好”。并且，“使性能的恶劣程度达到最小”和“使性能的优良程度达到最大”是等价的，不管是用“恶劣程度”还是“优良程度”，做的事情本质上都是一样的。</code></pre><h3 id="均方误差"><a href="#均方误差" class="headerlink" title="均方误差"></a>均方误差</h3><p>可以用作损失函数的函数有很多，其中最有名的是均方误差（mean squared error）。</p><p><img src="https://ws1.sinaimg.cn/large/006tKfTcly1g061wqcfc2g307s01a0ru.gif" alt=""></p><p>如式（4.1）所示，均方误差会计算神经网络的输出和正确解监督数据的各个元素之差的平方，再求总和。现在，我们用 Python 来实现这个均方误差，实现方式如下所示。</p><pre class=" language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">mean_squared_error</span><span class="token punctuation">(</span>y<span class="token punctuation">,</span> t<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">return</span> <span class="token number">0.5</span> <span class="token operator">*</span> np<span class="token punctuation">.</span>sum<span class="token punctuation">(</span><span class="token punctuation">(</span>y<span class="token operator">-</span>t<span class="token punctuation">)</span><span class="token operator">**</span><span class="token number">2</span><span class="token punctuation">)</span></code></pre><h3 id="交叉熵误差"><a href="#交叉熵误差" class="headerlink" title="交叉熵误差"></a>交叉熵误差</h3><p>除了均方误差之外，交叉熵误差（cross entropy error）也经常被用作损失函数。</p><p><img src="https://ws4.sinaimg.cn/large/006tKfTcly1g061xqra9wg307j0130pq.gif" alt=""></p><p>下面，我们来用代码实现交叉熵误差。</p><pre class=" language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">cross_entropy_error</span><span class="token punctuation">(</span>y<span class="token punctuation">,</span> t<span class="token punctuation">)</span><span class="token punctuation">:</span>    delta <span class="token operator">=</span> <span class="token number">1e</span><span class="token operator">-</span><span class="token number">7</span>    <span class="token keyword">return</span> <span class="token operator">-</span>np<span class="token punctuation">.</span>sum<span class="token punctuation">(</span>t <span class="token operator">*</span> np<span class="token punctuation">.</span>log<span class="token punctuation">(</span>y <span class="token operator">+</span> delta<span class="token punctuation">)</span><span class="token punctuation">)</span></code></pre><h3 id="mini-batch-学习"><a href="#mini-batch-学习" class="headerlink" title="mini-batch 学习"></a>mini-batch 学习</h3><p>机器学习使用训练数据进行学习。使用训练数据进行学习，严格来说，就是针对训练数据计算损失函数的值，找出使该值尽可能小的参数。因此，计算损失函数时必须将所有的训练数据作为对象。也就是说，如果训练数据有 100 个的话，我们就要把这 100 个损失函数的总和作为学习的指标。</p><p>前面介绍的损失函数的例子中考虑的都是针对单个数据的损失函数。如果要求所有训练数据的损失函数的总和，以交叉熵误差为例，可以写成下面的式（4.3）。</p><p><img src="https://ws4.sinaimg.cn/large/006tKfTcly1g061xqcqg2g309c01a741.gif" alt=""></p><p>另外，MNIST 数据集的训练数据有 60000 个，如果以全部数据为对象求损失函数的和，则计算过程需要花费较长的时间。再者，如果遇到大数据，数据量会有几百万、几千万之多，这种情况下以全部数据为对象计算损失函数是不现实的。因此，我们从全部数据中选出一部分，作为全部数据的“近似”。神经网络的学习也是从训练数据中选出一批数据（称为 mini-batch, 小批量），然后对每个 mini-batch 进行学习。比如，从 60000 个训练数据中随机选择 100 笔，再用这 100 笔数据进行学习。这种学习方式称为 mini-batch 学习。</p><h3 id="mini-batch-版交叉熵误差的实现"><a href="#mini-batch-版交叉熵误差的实现" class="headerlink" title="mini-batch 版交叉熵误差的实现"></a>mini-batch 版交叉熵误差的实现</h3><p>如何实现对应 mini-batch 的交叉熵误差呢？只要改良一下之前实现的对应单个数据的交叉熵误差就可以了。这里，我们来实现一个可以同时处理单个数据和批量数据（数据作为 batch 集中输入）两种情况的函数。</p><pre class=" language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">cross_entropy_error</span><span class="token punctuation">(</span>y<span class="token punctuation">,</span> t<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">if</span> y<span class="token punctuation">.</span>ndim <span class="token operator">==</span> <span class="token number">1</span><span class="token punctuation">:</span>        t <span class="token operator">=</span> t<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> t<span class="token punctuation">.</span>size<span class="token punctuation">)</span>        y <span class="token operator">=</span> y<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> y<span class="token punctuation">.</span>size<span class="token punctuation">)</span>    batch_size <span class="token operator">=</span> y<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>    <span class="token keyword">return</span> <span class="token operator">-</span>np<span class="token punctuation">.</span>sum<span class="token punctuation">(</span>t <span class="token operator">*</span> np<span class="token punctuation">.</span>log<span class="token punctuation">(</span>y <span class="token operator">+</span> <span class="token number">1e</span><span class="token operator">-</span><span class="token number">7</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">/</span> batch_size</code></pre><p>此外，当监督数据是标签形式（非 one-hot 表示，而是像“2”“7”这样的标签）时，交叉熵误差可通过如下代码实现。</p><pre class=" language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">cross_entropy_error</span><span class="token punctuation">(</span>y<span class="token punctuation">,</span> t<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">if</span> y<span class="token punctuation">.</span>ndim <span class="token operator">==</span> <span class="token number">1</span><span class="token punctuation">:</span>        t <span class="token operator">=</span> t<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> t<span class="token punctuation">.</span>size<span class="token punctuation">)</span>        y <span class="token operator">=</span> y<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> y<span class="token punctuation">.</span>size<span class="token punctuation">)</span>    batch_size <span class="token operator">=</span> y<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>    <span class="token keyword">return</span> <span class="token operator">-</span>np<span class="token punctuation">.</span>sum<span class="token punctuation">(</span>np<span class="token punctuation">.</span>log<span class="token punctuation">(</span>y<span class="token punctuation">[</span>np<span class="token punctuation">.</span>arange<span class="token punctuation">(</span>batch_size<span class="token punctuation">)</span><span class="token punctuation">,</span> t<span class="token punctuation">]</span> <span class="token operator">+</span> <span class="token number">1e</span><span class="token operator">-</span><span class="token number">7</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">/</span> batch_size</code></pre><h3 id="为何要设定损失函数"><a href="#为何要设定损失函数" class="headerlink" title="为何要设定损失函数"></a>为何要设定损失函数</h3><p>在神经网络的学习中，寻找最优参数（权重和偏置）时，要寻找使损失函数的值尽可能小的参数。为了找到使损失函数的值尽可能小的地方，需要计算参数的导数（确切地讲是梯度），然后以这个导数为指引，逐步更新参数的值。</p><p>假设有一个神经网络，现在我们来关注这个神经网络中的某一个权重参数。此时，对该权重参数的损失函数求导，表示的是“如果稍微改变这个权重参数的值，损失函数的值会如何变化”。如果导数的值为负，通过使该权重参数向正方向改变，可以减小损失函数的值；反过来，如果导数的值为正，则通过使该权重参数向负方向改变，可以减小损失函数的值。不过，当导数的值为 0 时，无论权重参数向哪个方向变化，损失函数的值都不会改变，此时该权重参数的更新会停在此处。</p><p>之所以不能用识别精度作为指标，是因为这样一来绝大多数地方的导数都会变为 0，导致参数无法更新。</p><pre class=" language-text"><code class="language-text">在进行神经网络的学习时，不能将识别精度作为指标。因为如果以识别精度为指标，则参数的导数在绝大多数地方都会变为 0。</code></pre><p>为什么用识别精度作为指标时，参数的导数在绝大多数地方都会变成 0 呢？为了回答这个问题，我们来思考另一个具体例子。假设某个神经网络正确识别出了 100 笔训练数据中的 32 笔，此时识别精度为 32 %。如果以识别精度为指标，即使稍微改变权重参数的值，识别精度也仍将保持在 32 %，不会出现变化。也就是说，仅仅微调参数，是无法改善识别精度的。即便识别精度有所改善，它的值也不会像 32.0123 … % 这样连续变化，而是变为 33 %、34 % 这样的不连续的、离散的值。而如果把损失函数作为指标，则当前损失函数的值可以表示为 0.92543 … 这样的值。并且，如果稍微改变一下参数的值，对应的损失函数也会像 0.93432 … 这样发生连续性的变化。</p><p>识别精度对微小的参数变化基本上没有什么反应，即便有反应，它的值也是不连续地、突然地变化。</p><h2 id="数值微分"><a href="#数值微分" class="headerlink" title="数值微分"></a>数值微分</h2><p>梯度法使用梯度的信息决定前进的方向。</p><h3 id="导数"><a href="#导数" class="headerlink" title="导数"></a>导数</h3><p>数值微分含有误差。为了减小这个误差，我们可以计算函数 <em>f</em> 在 (<em>x</em> + <em>h</em>) 和 (<em>x</em> - <em>h</em>) 之间的差分。因为这种计算方法以 <em>x</em> 为中心，计算它左右两边的差分，所以也称为<strong>中心差分</strong>（而 (<em>x</em> + <em>h</em>) 和 <em>x</em> 之间的差分称为<strong>前向差分</strong>）。</p><p>数值微分</p><pre class=" language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">numerical_diff</span><span class="token punctuation">(</span>f<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>    h <span class="token operator">=</span> <span class="token number">1e</span><span class="token operator">-</span><span class="token number">4</span> <span class="token comment" spellcheck="true"># 0.0001</span>    <span class="token keyword">return</span> <span class="token punctuation">(</span>f<span class="token punctuation">(</span>x<span class="token operator">+</span>h<span class="token punctuation">)</span> <span class="token operator">-</span> f<span class="token punctuation">(</span>x<span class="token operator">-</span>h<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">/</span> <span class="token punctuation">(</span><span class="token number">2</span><span class="token operator">*</span>h<span class="token punctuation">)</span></code></pre><pre class=" language-text"><code class="language-text">利用微小的差分求导数的过程称为数值微分（numerical differentiation）。而基于数学式的推导求导数的过程，则用“解析性”（analytic）一词，称为“解析性求解”或者“解析性求导”。解析性求导得到的导数是不含误差的“真的导数”。</code></pre><h3 id="数值微分的例子"><a href="#数值微分的例子" class="headerlink" title="数值微分的例子"></a>数值微分的例子</h3><p>现在我们试着用上述的数值微分对简单函数进行求导。先来看一个由下式表示的 2 次函数。</p><p><img src="https://ws2.sinaimg.cn/large/006tKfTcgy1g06c0gennjg307b00k0l5.gif" alt="gif"></p><p>用 Python 来实现式（4.5），如下所示。</p><pre class=" language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">function_1</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">return</span> <span class="token number">0.01</span><span class="token operator">*</span>x<span class="token operator">**</span><span class="token number">2</span> <span class="token operator">+</span> <span class="token number">0.1</span><span class="token operator">*</span>x</code></pre><p><img src="https://ws1.sinaimg.cn/large/006tKfTcgy1g06cb6qg4kj30rs0ltab7.jpg" alt="051"></p><p>图 4-6　$f(x)=0.01x^2+0.1x$的图像</p><p>我们来计算一下这个函数在 <em>x</em> = 5 和 <em>x</em> = 10 处的导数。</p><pre class=" language-python"><code class="language-python"><span class="token operator">>></span><span class="token operator">></span> numerical_diff<span class="token punctuation">(</span>function_1<span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">)</span><span class="token number">0.1999999999990898</span><span class="token operator">>></span><span class="token operator">></span> numerical_diff<span class="token punctuation">(</span>function_1<span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span><span class="token number">0.2999999999986347</span></code></pre><h3 id="偏导数"><a href="#偏导数" class="headerlink" title="偏导数"></a>偏导数</h3><p>偏导数和单变量的导数一样，都是求某个地方的斜率。不过，偏导数需要将多个变量中的某一个变量定为目标变量，并将其他变量固定为某个值。</p><h2 id="梯度"><a href="#梯度" class="headerlink" title="梯度"></a>梯度</h2><p>由全部变量的偏导数汇总而成的向量称为<strong>梯度</strong>（gradient）。</p><p>如图 4-9 所示，</p><p><img src="https://ws3.sinaimg.cn/large/006tKfTcgy1g06ct8t868g304h00m0hv.gif" alt="fx"></p><p>的梯度呈现为有向向量（箭头）。</p><p><img src="https://ws2.sinaimg.cn/large/006tKfTcgy1g06cnhw7o5j30rs0lkn42.jpg" alt="054"></p><p>图 4-9　$f(x_0 + x_1) = {x_0}^2 + {x_1}^2$的梯度</p><p>虽然图 4-9 中的梯度指向了最低处，但并非任何时候都这样。实际上，梯度会指向各点处的函数值降低的方向。<br>更严格地讲，梯度指示的方向是各点处的函数值减小最多的方向。</p><h3 id="梯度法"><a href="#梯度法" class="headerlink" title="梯度法"></a>梯度法</h3><p>机器学习的主要任务是在学习时寻找最优参数。同样地，神经网络也必须在学习时找到最优参数（权重和偏置）。这里所说的最优参数是指损失函数取最小值时的参数。但是，一般而言，损失函数很复杂，参数空间庞大，我们不知道它在何处能取得最小值。而通过巧妙地使用梯度来寻找函数最小值（或者尽可能小的值）的方法就是梯度法。</p><p>这里需要注意的是，梯度表示的是各点处的函数值减小最多的方向。因此，无法保证梯度所指的方向就是函数的最小值或者真正应该前进的方向。实际上，在复杂的函数中，梯度指示的方向基本上都不是函数值最小处。</p><pre class=" language-text"><code class="language-text">函数的极小值、最小值以及被称为鞍点（saddle point）的地方，梯度为 0。极小值是局部最小值，也就是限定在某个范围内的最小值。鞍点是从某个方向上看是极大值，从另一个方向上看则是极小值的点。虽然梯度法是要寻找梯度为 0 的地方，但是那个地方不一定就是最小值（也有可能是极小值或者鞍点）。此外，当函数很复杂且呈扁平状时，学习可能会进入一个（几乎）平坦的地区，陷入被称为“学习高原”的无法前进的停滞期。</code></pre><p>虽然梯度的方向并不一定指向最小值，但沿着它的方向能够最大限度地减小函数的值。因此，在寻找函数的最小值（或者尽可能小的值）的位置的任务中，要以梯度的信息为线索，决定前进的方向。</p><p>此时梯度法就派上用场了。在梯度法中，函数的取值从当前位置沿着梯度方向前进一定距离，然后在新的地方重新求梯度，再沿着新梯度方向前进，如此反复，不断地沿梯度方向前进。像这样，通过不断地沿梯度方向前进，逐渐减小函数值的过程就是梯度法（gradient method）。梯度法是解决机器学习中最优化问题的常用方法，特别是在神经网络的学习中经常被使用。</p><pre class=" language-text"><code class="language-text">根据目的是寻找最小值还是最大值，梯度法的叫法有所不同。严格地讲，寻找最小值的梯度法称为梯度下降法（gradient descent method），寻找最大值的梯度法称为梯度上升法（gradient ascent method）。但是通过反转损失函数的符号，求最小值的问题和求最大值的问题会变成相同的问题，因此“下降”还是“上升”的差异本质上并不重要。一般来说，神经网络（深度学习）中，梯度法主要是指梯度下降法。</code></pre><p>现在，我们尝试用数学式来表示梯度法，如式（4.7）所示。</p><p><img src="https://ws3.sinaimg.cn/large/006tKfTcly1g06z3xvmhsj30tt08k3yv.jpg" alt="4_nn_47"></p><p>下面，我们用 Python 来实现梯度下降法。如下所示，这个实现很简单。</p><pre class=" language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">gradient_descent</span><span class="token punctuation">(</span>f<span class="token punctuation">,</span> init_x<span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">0.01</span><span class="token punctuation">,</span> step_num<span class="token operator">=</span><span class="token number">100</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    x <span class="token operator">=</span> init_x    <span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span>step_num<span class="token punctuation">)</span><span class="token punctuation">:</span>        grad <span class="token operator">=</span> numerical_gradient<span class="token punctuation">(</span>f<span class="token punctuation">,</span> x<span class="token punctuation">)</span>        x <span class="token operator">-=</span> lr <span class="token operator">*</span> grad    <span class="token keyword">return</span> x</code></pre><p>学习率过大的话，会发散成一个很大的值；反过来，学习率过小的话，基本上没怎么更新就结束了。也就是说，设定合适的学习率是一个很重要的问题。</p><pre class=" language-text"><code class="language-text">像学习率这样的参数称为超参数。这是一种和神经网络的参数（权重和偏置）性质不同的参数。相对于神经网络的权重参数是通过训练数据和学习算法自动获得的，学习率这样的超参数则是人工设定的。一般来说，超参数需要尝试多个值，以便找到一种可以使学习顺利进行的设定。</code></pre><h3 id="神经网络的梯度"><a href="#神经网络的梯度" class="headerlink" title="神经网络的梯度"></a>神经网络的梯度</h3><p>神经网络的学习也要求梯度。这里所说的梯度是指损失函数关于权重参数的梯度。</p><p>比如，有一个只有一个形状为 2 × 3 的权重 <strong>W</strong> 的神经网络，损失函数用 <em>L</em> 表示。此时，梯度可以用$\frac{\partial{L}}{\partial{W}}$表示。用数学式表示的话，如下所示。</p><p><img src="https://ws3.sinaimg.cn/large/006tKfTcly1g06zpiqkixj30mn07f0t8.jpg" alt="4._nn_48"></p><h2 id="学习算法的实现"><a href="#学习算法的实现" class="headerlink" title="学习算法的实现"></a>学习算法的实现</h2><p>关于神经网络学习的基础知识，到这里就全部介绍完了。“损失函数”“mini-batch”“梯度”“梯度下降法”等关键词已经陆续登场，这里我们来确认一下神经网络的学习步骤，顺便复习一下这些内容。神经网络的学习步骤如下所示。</p><pre class=" language-text"><code class="language-text">前提神经网络存在合适的权重和偏置，调整权重和偏置以便拟合训练数据的过程称为“学习”。神经网络的学习分成下面 4 个步骤。步骤 1（mini-batch）从训练数据中随机选出一部分数据，这部分数据称为 mini-batch。我们的目标是减小 mini-batch 的损失函数的值。步骤 2（计算梯度）为了减小 mini-batch 的损失函数的值，需要求出各个权重参数的梯度。梯度表示损失函数的值减小最多的方向。步骤 3（更新参数）将权重参数沿梯度方向进行微小更新。步骤 4（重复）重复步骤 1、步骤 2、步骤 3。</code></pre><p>神经网络的学习按照上面 4 个步骤进行。这个方法通过梯度下降法更新参数，不过因为这里使用的数据是随机选择的mini batch 数据，所以又称为随机梯度下降法（stochastic gradient descent）。“随机”指的是“随机选择的”的意思，因此，随机梯度下降法是“对随机选择的数据进行的梯度下降法”。深度学习的很多框架中，随机梯度下降法一般由一个名为 SGD 的函数来实现。SGD 来源于随机梯度下降法的英文名称的首字母。</p><h3 id="2-层神经网络的类"><a href="#2-层神经网络的类" class="headerlink" title="2 层神经网络的类"></a>2 层神经网络的类</h3><p>使用MNIST数据集，设计一个2层神经网络，输入图像的大小是 784（28 × 28），输出为 10 个类别，所以指定参数 <code>input_size=784</code>、<code>output_size=10</code>，将隐藏层的个数 <code>hidden_size</code> 设置为一个合适的值即可。</p><h3 id="mini-batch-的实现"><a href="#mini-batch-的实现" class="headerlink" title="mini-batch 的实现"></a>mini-batch 的实现</h3><p>神经网络的学习的实现使用的是前面介绍过的 mini-batch 学习。所谓 mini-batch 学习，就是从训练数据中随机选择一部分数据（称为 mini-batch），再以这些 mini-batch 为对象，使用梯度法更新参数的过程。</p><h3 id="基于测试数据的评价"><a href="#基于测试数据的评价" class="headerlink" title="基于测试数据的评价"></a>基于测试数据的评价</h3><p>训练数据的损失函数值减小，虽说是神经网络的学习正常进行的一个信号，但光看这个结果还不能说明该神经网络在其他数据集上也一定能有同等程度的表现。</p><p>神经网络的学习中，必须确认是否能够正确识别训练数据以外的其他数据，即确认是否会发生过拟合。过拟合是指，虽然训练数据中的数字图像能被正确辨别，但是不在训练数据中的数字图像却无法被识别的现象。</p><p>神经网络学习的最初目标是掌握泛化能力，因此，要评价神经网络的泛化能力，就必须使用不包含在训练数据中的数据。下面的代码在进行学习的过程中，会定期地对训练数据和测试数据记录识别精度。这里，每经过一个 epoch，我们都会记录下训练数据和测试数据的识别精度。</p><pre class=" language-text"><code class="language-text">epoch 是一个单位。一个 epoch 表示学习中所有训练数据均被使用过一次时的更新次数。比如，对于 10000 笔训练数据，用大小为 100 笔数据的 mini-batch 进行学习时，重复随机梯度下降法 100 次，所有的训练数据就都被“看过”了 (实际上，一般做法是事先将所有训练数据随机打乱，然后按指定的批次大小，按序生成 mini-batch。这样每个 mini-batch 均有一个索引号，比如此例可以是 0, 1, 2, ... , 99，然后用索引号可以遍历所有的 mini-batch。遍历一次所有数据，就称为一个epoch。请注意，本节中的mini-batch 每次都是随机选择的，所以不一定每个数据都会被看到。——译者注)。此时，100 次就是一个 epoch。</code></pre><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>以损失函数为基准，找出使它的值达到最小的权重参数，就是神经网络学习的目标。</p><ul><li>机器学习中使用的数据集分为训练数据和测试数据</li><li>神经网络用训练数据进行学习，并用测试数据评价学习到的模型的泛化能力</li><li>神经网络的学习以损失函数为指标，更新权重参数，以使损失函数的值减小</li><li>利用某个给定的微小值得差分求导数的过程，成为数值微分</li><li>利用数值微分，可以计算权重参数的梯度</li><li>数据微分虽然费时间，但是实现起来简单</li></ul><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol><li><a href="http://www.ituring.com.cn/book/1921" target="_blank" rel="noopener">深度学习入门：基于Python的理论与实现</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> Deep Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Notes </tag>
            
            <tag> Deep Learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Notes-深度学习入门之神经网络</title>
      <link href="/2018/11/07/27-dl-neural-networks-notes/"/>
      <url>/2018/11/07/27-dl-neural-networks-notes/</url>
      
        <content type="html"><![CDATA[<p>上一章我们学习了感知机。关于感知机，既有好消息，也有坏消息。好消息是，即便对于复杂的函数，感知机也隐含着能够表示它的可能性。上一章已经介绍过，即便是计算机进行的复杂处理，感知机（理论上）也可以将其表示出来。坏消息是，设定权重的工作，即确定合适的、能符合预期的输入与输出的权重，现在还是由人工进行的。上一章中，我们结合与门、或门的真值表人工决定了合适的权重。</p><p>神经网络的出现就是为了解决刚才的坏消息。具体地讲，神经网络的一个重要性质是它可以自动地从数据中学习到合适的权重参数。本章中，我们会先介绍神经网络的概要，然后重点关注神经网络进行识别时的处理。</p><h2 id="从感知机到神经网络"><a href="#从感知机到神经网络" class="headerlink" title="从感知机到神经网络"></a>从感知机到神经网络</h2><h3 id="神经网络的例子"><a href="#神经网络的例子" class="headerlink" title="神经网络的例子"></a>神经网络的例子</h3><p><img src="https://note.youdao.com/yws/api/personal/file/WEBeb20bbfd6913a60d54d65b3b44a024e6?method=download&amp;shareKey=ff10baf984ecbf110a88d649fe51c58a" alt=""></p><p>图 3-1 中的网络一共由 3 层神经元构成，但实质上只有 2 层神经元有权重，因此将其称为“2 层网络”。请注意，有的书也会根据构成网络的层数，把图 3-1 的网络称为“3 层网络”。本书将根据实质上拥有权重的层数（输入层、隐藏层、输出层的总数减去 1 后的数量）来表示网络的名称。</p><h3 id="复习感知机"><a href="#复习感知机" class="headerlink" title="复习感知机"></a>复习感知机</h3><h3 id="激活函数登场"><a href="#激活函数登场" class="headerlink" title="激活函数登场"></a>激活函数登场</h3><p>激活函数 (activation function)</p><p>激活函数的作用在于决定如何来激活输入信号的总和。</p><p>激活函数是连接感知机和神经网络的桥梁。</p><p>一般而言，“朴素感知机”是指单层网络，指的是激活函数使用了阶跃函数 （阶跃函数是指一旦输入超过阈值，就切换输出的函数） 的模型。<br>“多层感知机”是指神经网络，即使用 sigmoid 函数（后述）等平滑的激活函数的多层网络。</p><h2 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h2><p>感知机中使用了阶跃函数作为激活函数。也就是说，在激活函数的众多候选函数中，感知机使用了阶跃函数。</p><h3 id="sigmoid-函数"><a href="#sigmoid-函数" class="headerlink" title="sigmoid 函数"></a>sigmoid 函数</h3><p>神经网络中用 sigmoid 函数作为激活函数，进行信号的转换，转换后的信号被传送给下一个神经元。实际上，上一章介绍的感知机和接下来要介绍的神经网络的主要区别就在于这个激活函数。其他方面，比如神经元的多层连接的构造、信号的传递方法等，基本上和感知机是一样的。</p><h3 id="阶跃函数的实现"><a href="#阶跃函数的实现" class="headerlink" title="阶跃函数的实现"></a>阶跃函数的实现</h3><pre class=" language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">step_function</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">:</span>    y <span class="token operator">=</span> x <span class="token operator">></span> <span class="token number">0</span>    <span class="token keyword">return</span> y<span class="token punctuation">.</span>astype<span class="token punctuation">(</span>np<span class="token punctuation">.</span>int<span class="token punctuation">)</span></code></pre><h3 id="阶跃函数的图形"><a href="#阶跃函数的图形" class="headerlink" title="阶跃函数的图形"></a>阶跃函数的图形</h3><h3 id="sigmoid-函数的实现"><a href="#sigmoid-函数的实现" class="headerlink" title="sigmoid 函数的实现"></a>sigmoid 函数的实现</h3><pre class=" language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">sigmoid</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">return</span> <span class="token number">1</span> <span class="token operator">/</span> <span class="token punctuation">(</span><span class="token number">1</span> <span class="token operator">+</span> np<span class="token punctuation">.</span>exp<span class="token punctuation">(</span><span class="token operator">-</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span></code></pre><h3 id="sigmoid-函数和阶跃函数的比较"><a href="#sigmoid-函数和阶跃函数的比较" class="headerlink" title="sigmoid 函数和阶跃函数的比较"></a>sigmoid 函数和阶跃函数的比较</h3><p><strong>不同点</strong></p><ul><li><p>首先注意到的是“平滑性”的不同。sigmoid 函数是一条平滑的曲线，输出随着输入发生连续性的变化。而阶跃函数以 0 为界，输出发生急剧性的变化。sigmoid 函数的平滑性对神经网络的学习具有重要意义。</p></li><li><p>另一个不同点是，相对于阶跃函数只能返回 0 或 1，sigmoid 函数可以返回 0.731 …、0.880 … 等实数（这一点和刚才的平滑性有关）。也就是说，感知机中神经元之间流动的是 0 或 1 的二元信号，而神经网络中流动的是连续的实数值信号。</p></li></ul><p><strong>相同点</strong></p><ul><li>阶跃函数和 sigmoid 函数虽然在平滑性上有差异，但是如果从宏观视角看图 3-8，可以发现它们具有相似的形状。实际上，两者的结构均是“输入小时，输出接近 0（为 0）；随着输入增大，输出向 1 靠近（变成 1）”。也就是说，当输入信号为重要信息时，阶跃函数和 sigmoid 函数都会输出较大的值；当输入信号为不重要的信息时，两者都输出较小的值。</li><li>还有一个共同点是，不管输入信号有多小，或者有多大，输出信号的值都在 0 到 1 之间。</li></ul><h3 id="非线性函数"><a href="#非线性函数" class="headerlink" title="非线性函数"></a>非线性函数</h3><p>阶跃函数和 sigmoid 函数还有其他共同点，就是两者均为非线性函数。sigmoid 函数是一条曲线，阶跃函数是一条像阶梯一样的折线，两者都属于非线性的函数。</p><p>神经网络的激活函数必须使用非线性函数。换句话说，激活函数不能使用线性函数。为什么不能使用线性函数呢？因为使用线性函数的话，加深神经网络的层数就没有意义了。</p><p>线性函数的问题在于，不管如何加深层数，总是存在与之等效的“无隐藏层的神经网络”。</p><p>使用线性函数时，无法发挥多层网络带来的优势。因此，为了发挥叠加层所带来的优势，激活函数必须使用非线性函数。</p><h3 id="ReLU-函数"><a href="#ReLU-函数" class="headerlink" title="ReLU 函数"></a>ReLU 函数</h3><p>在神经网络发展的历史上，sigmoid 函数很早就开始被使用了，而最近则主要使用 ReLU（Rectified Linear Unit）函数。</p><p>ReLU（Rectified Linear Unit）函数</p><p>ReLU 函数在输入大于 0 时，直接输出该值；在输入小于等于 0 时，输出 0。</p><pre class=" language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">relu</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">return</span> np<span class="token punctuation">.</span>maximum<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> x<span class="token punctuation">)</span></code></pre><h2 id="多维数组的运算"><a href="#多维数组的运算" class="headerlink" title="多维数组的运算"></a>多维数组的运算</h2><h3 id="多维数组"><a href="#多维数组" class="headerlink" title="多维数组"></a>多维数组</h3><p>多维数组就是“数字的集合”，数字排成一列的集合、排成长方形的集合、排成三维状或者（更加一般化的）N 维状的集合都称为多维数组。</p><h3 id="矩阵乘法"><a href="#矩阵乘法" class="headerlink" title="矩阵乘法"></a>矩阵乘法</h3><h3 id="神经网络的内积"><a href="#神经网络的内积" class="headerlink" title="神经网络的内积"></a>神经网络的内积</h3><p>通过矩阵的乘积一次性完成计算的技巧，在实现的层面上可以说是非常重要的。</p><h2 id="3层神经网络的实现"><a href="#3层神经网络的实现" class="headerlink" title="3层神经网络的实现"></a>3层神经网络的实现</h2><p><img src="https://note.youdao.com/yws/api/personal/file/WEBa80f3fa277753b55d223ab0b855fab1e?method=download&amp;shareKey=fb85afafaea3a9569dfc0e90a2bccf14" alt=""></p><p>图 3-15　3 层神经网络：输入层（第 0 层）有 2 个神经元，第 1 个隐藏层（第 1 层）有 3 个神经元，第 2 个隐藏层（第 2 层）有 2 个神经元，输出层（第 3 层）有 2 个神经元</p><h3 id="符号确认"><a href="#符号确认" class="headerlink" title="符号确认"></a>符号确认</h3><p><img src="https://note.youdao.com/yws/api/personal/file/WEBdbb9371f96dead4b5f9ea2f1b1724040?method=download&amp;shareKey=5d56679e2450e9a8eeee28e03e3971c7" alt=""></p><p>图 3-16　权重的符号</p><h3 id="各层间信号传递的实现"><a href="#各层间信号传递的实现" class="headerlink" title="各层间信号传递的实现"></a>各层间信号传递的实现</h3><p><img src="https://note.youdao.com/yws/api/personal/file/WEB1d37ee9cc77bd70c88d975df157c60ea?method=download&amp;shareKey=e14e448e636523ce7f8167435b453d70" alt=""><br>图 3-18　从输入层到第 1 层的信号传递</p><p>输出层所用的激活函数，要根据求解问题的性质决定。一般地，回归问题可以使用恒等函数，二元分类问题可以使用 sigmoid 函数，多元分类问题可以使用 softmax 函数。</p><h3 id="代码实现小结"><a href="#代码实现小结" class="headerlink" title="代码实现小结"></a>代码实现小结</h3><p>至此，神经网络的前向处理的实现就完成了。通过巧妙地使用 NumPy 多维数组，高效地实现了神经网络。</p><h2 id="输出层的设计"><a href="#输出层的设计" class="headerlink" title="输出层的设计"></a>输出层的设计</h2><p>神经网络可以用在分类问题和回归问题上，不过需要根据情况改变输出层的激活函数。一般而言，回归问题就用恒等函数，分类问题用 softmax 函数。</p><h3 id="恒等函数和-softmax-函数"><a href="#恒等函数和-softmax-函数" class="headerlink" title="恒等函数和 softmax 函数"></a>恒等函数和 softmax 函数</h3><p>恒等函数会将输入按原样输出，对于输入的信息，不加以任何改动地直接输出。因此，在输出层使用恒等函数时，输入信号会原封不动地被输出。</p><p><img src="https://note.youdao.com/yws/api/personal/file/WEBadda8e2a7eb060c1096942730afdce8b?method=download&amp;shareKey=7e2df3b03ea84cb558a45dc5a67127fb" alt=""></p><p>图 3-21　恒等函数</p><p>类问题中使用的 softmax 函数可以用下面的式（3.10）表示。</p><p><img src="https://note.youdao.com/yws/api/personal/file/WEBd6614b621fc683d557389315bbb61248?method=download&amp;shareKey=86a3581df3a637cbfa0ada1d5b7983d4" alt=""></p><p>用图表示 softmax 函数的话，如图 3-22 所示。图 3-22 中，softmax 函数的输出通过箭头与所有的输入信号相连。这是因为，从式（3.10）可以看出，输出层的各个神经元都受到所有输入信号的影响。</p><p><img src="https://note.youdao.com/yws/api/personal/file/WEBd1d90e48bcc30f6cf9598b540abec08b?method=download&amp;shareKey=1acedc89ec02a4d33d72953d923363e6" alt=""></p><p>图 3-22　softmax 函数</p><pre class=" language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">softmax</span><span class="token punctuation">(</span>a<span class="token punctuation">)</span><span class="token punctuation">:</span>    exp_a <span class="token operator">=</span> np<span class="token punctuation">.</span>exp<span class="token punctuation">(</span>a<span class="token punctuation">)</span>    sum_exp_a <span class="token operator">=</span> np<span class="token punctuation">.</span>sum<span class="token punctuation">(</span>exp_a<span class="token punctuation">)</span>    y <span class="token operator">=</span> exp_a <span class="token operator">/</span> sum_exp_a    <span class="token keyword">return</span> y</code></pre><h3 id="实现-softmax-函数时的注意事项"><a href="#实现-softmax-函数时的注意事项" class="headerlink" title="实现 softmax 函数时的注意事项"></a>实现 softmax 函数时的注意事项</h3><p>上面的 softmax 函数的实现虽然正确描述了式（3.10），但在计算机的运算上有一定的缺陷。这个缺陷就是溢出问题。<br>softmax 函数的实现中要进行指数函数的运算，但是此时指数函数的值很容易变得非常大。</p><p>softmax 函数的实现可以像式（3.11）这样进行改进。</p><p><img src="https://note.youdao.com/yws/api/personal/file/WEB9993e79f53ee02ac1cc81a50535ba5bd?method=download&amp;shareKey=377f4e77c4d229691b8624ad477cc826" alt=""></p><p>式（3.11）说明，在进行 softmax 的指数函数的运算时，加上（或者减去）某个常数并不会改变运算的结果。这里的 C’ 可以使用任何值，但是为了防止溢出，一般会使用输入信号中的最大值。我们来看一个具体的例子。</p><pre class=" language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">softmax</span><span class="token punctuation">(</span>a<span class="token punctuation">)</span><span class="token punctuation">:</span>    c <span class="token operator">=</span> np<span class="token punctuation">.</span>max<span class="token punctuation">(</span>a<span class="token punctuation">)</span>    exp_a <span class="token operator">=</span> np<span class="token punctuation">.</span>exp<span class="token punctuation">(</span>a <span class="token operator">-</span> c<span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># 溢出对策</span>    sum_exp_a <span class="token operator">=</span> np<span class="token punctuation">.</span>sum<span class="token punctuation">(</span>exp_a<span class="token punctuation">)</span>    y <span class="token operator">=</span> exp_a <span class="token operator">/</span> sum_exp_a    <span class="token keyword">return</span> y</code></pre><h3 id="softmax-函数的特征"><a href="#softmax-函数的特征" class="headerlink" title="softmax 函数的特征"></a>softmax 函数的特征</h3><p>softmax 函数的输出是 0.0 到 1.0 之间的实数。并且，softmax 函数的输出值的总和是 1。输出总和为 1 是 softmax 函数的一个重要性质。正因为有了这个性质，我们才可以把 softmax 函数的输出解释为“概率”。</p><p>一般而言，神经网络只把输出值最大的神经元所对应的类别作为识别结果。并且，即便使用 softmax 函数，输出值最大的神经元的位置也不会变。因此，神经网络在进行分类时，输出层的 softmax 函数可以省略。在实际的问题中，由于指数函数的运算需要一定的计算机运算量，因此输出层的 softmax 函数一般会被省略。</p><p>求解机器学习问题的步骤可以分为“学习”{5[“学习”也称为“训练”，为了强调算法从数据中学习模型，本书使用“学习”一词。——译者注]} 和“推理”两个阶段。首先，在学习阶段进行模型的学习 {6[这里的“学习”是指使用训练数据、自动调整参数的过程，具体请参考第 4 章。——译者注]}，然后，在推理阶段，用学到的模型对未知的数据进行推理（分类）。如前所述，推理阶段一般会省略输出层的 softmax 函数。在输出层使用 softmax 函数是因为它和神经网络的学习有关系（详细内容请参考下一章）。</p><h3 id="输出神经元的数量"><a href="#输出神经元的数量" class="headerlink" title="输出神经元的数量"></a>输出神经元的数量</h3><p>输出层的神经元数量需要根据待解决的问题来决定。对于分类问题，输出层的神经元数量一般设定为类别的数量。</p><h2 id="手写数字的识别"><a href="#手写数字的识别" class="headerlink" title="手写数字的识别"></a>手写数字的识别</h2><p>介绍完神经网络的结构之后，现在我们来试着解决实际问题。这里我们来进行手写数字图像的分类。假设学习已经全部结束，我们使用学习到的参数，先实现神经网络的“推理处理”。这个推理处理也称为神经网络的前向传播（forward propagation）。</p><p>和求解机器学习问题的步骤（分成学习和推理两个阶段进行）一样，使用神经网络解决问题时，也需要首先使用训练数据（学习数据）进行权重参数的学习；进行推理时，使用刚才学习到的参数，对输入数据进行分类。</p><h3 id="MNIST-数据集"><a href="#MNIST-数据集" class="headerlink" title="MNIST 数据集"></a>MNIST 数据集</h3><p>MNIST 是机器学习领域最有名的数据集之一，被应用于从简单的实验到发表的论文研究等各种场合。实际上，在阅读图像识别或机器学习的论文时，MNIST 数据集经常作为实验用的数据出现。</p><p>MNIST 数据集是由 0 到 9 的数字图像构成的。训练图像有 6 万张，测试图像有 1 万张，这些图像可以用于学习和推理。MNIST 数据集的一般使用方法是，先用训练图像进行学习，再用学习到的模型度量能在多大程度上对测试图像进行正确的分类。</p><p>MNIST 的图像数据是 28 像素 × 28 像素的灰度图像（1 通道），各个像素的取值在 0 到 255 之间。每个图像数据都相应地标有“7”, “2”, “1”等标签。</p><p>Python 有 pickle 这个便利的功能。这个功能可以将程序运行中的对象保存为文件。如果加载保存过的 pickle 文件，可以立刻复原之前程序运行中的对象。用于读入 MNIST 数据集的 load_mnist() 函数内部也使用了 pickle 功能（在第 2 次及以后读入时）。利用 pickle 功能，可以高效地完成 MNIST 数据的准备工作。</p><h3 id="神经网络的推理处理"><a href="#神经网络的推理处理" class="headerlink" title="神经网络的推理处理"></a>神经网络的推理处理</h3><p>下面，我们对这个 MNIST 数据集实现神经网络的推理处理。神经网络的输入层有 784 个神经元，输出层有 10 个神经元。输入层的 784 这个数字来源于图像大小的 28 × 28 = 784，输出层的 10 这个数字来源于 10 类别分类（数字 0 到 9，共 10 类别）。此外，这个神经网络有 2 个隐藏层，第 1 个隐藏层有 50 个神经元，第 2 个隐藏层有 100 个神经元。这个 50 和 100 可以设置为任何值。</p><p>预处理在神经网络（深度学习）中非常实用，其有效性已在提高识别性能和学习的效率等众多实验中得到证明。在刚才的例子中，作为一种预处理，我们将各个像素值除以 255，进行了简单的正规化。实际上，很多预处理都会考虑到数据的整体分布。比如，利用数据整体的均值或标准差，移动数据，使数据整体以 0 为中心分布，或者进行正规化，把数据的延展控制在一定范围内。除此之外，还有将数据整体的分布形状均匀化的方法，即数据白化（whitening）等。</p><h3 id="批处理"><a href="#批处理" class="headerlink" title="批处理"></a>批处理</h3><p>批处理对计算机的运算大有利处，可以大幅缩短每张图像的处理时间。那么为什么批处理可以缩短处理时间呢？这是因为大多数处理数值计算的库都进行了能够高效处理大型数组运算的最优化。并且，在神经网络的运算中，当数据传送成为瓶颈时，批处理可以减轻数据总线的负荷（严格地讲，相对于数据读入，可以将更多的时间用在计算上）。也就是说，批处理一次性计算大型数组要比分开逐步计算各个小型数组速度更快。</p><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>本章介绍了神经网络的前向传播。本章介绍的神经网络和上一章的感知机在信号的按层传递这一点上是相同的，但是，向下一个神经元发送信号时，改变信号的激活函数有很大差异。神经网络中使用的是平滑变化的 sigmoid 函数，而感知机中使用的是信号急剧变化的阶跃函数。这个差异对于神经网络的学习非常重要，我们将在下一章介绍。</p><ul><li>神经网络中的激活函数使用平滑变化的 sigmoid 函数或 ReLU 函数</li><li>通过巧妙地使用 NumPy 多维数组，可以高效地实现神经网络</li><li>机器学习的问题大体上可以分为回归问题和分类问题</li><li>关于输出层的激活函数，回归问题中一般用恒等函数，分类问题中一般用 softmax 函数</li><li>分类问题中，输出层的神经元数量设置为要分类的类别数</li><li>输入数据的集合称为批，通过以批为单位进行推理处理，能够实现高效运算</li></ul><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol><li><a href="https://playground.tensorflow.org/#activation=tanh&amp;batchSize=10&amp;dataset=circle&amp;regDataset=reg-plane&amp;learningRate=0.03&amp;regularizationRate=0&amp;noise=0&amp;networkShape=4,2&amp;seed=0.45086&amp;showTestData=false&amp;discretize=false&amp;percTrainData=50&amp;x=true&amp;y=true&amp;xTimesY=false&amp;xSquared=false&amp;ySquared=false&amp;cosX=false&amp;sinX=false&amp;cosY=false&amp;sinY=false&amp;collectStats=false&amp;problem=classification&amp;initZero=false&amp;hideText=false" target="_blank" rel="noopener">https://playground.tensorflow.org</a></li><li><a href="https://github.com/oreilly-japan/deep-learning-from-scratch/tree/master/ch03" target="_blank" rel="noopener">第3章代码 GitHub</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> Deep Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Notes </tag>
            
            <tag> Deep Learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Notes-深度学习入门之感知机</title>
      <link href="/2018/11/05/26-dl-perceptron-notes/"/>
      <url>/2018/11/05/26-dl-perceptron-notes/</url>
      
        <content type="html"><![CDATA[<p>本章将介绍感知机(严格地讲，本章中所说的感知机应该称为“人工神经元”或“朴素感知机”，但是因为很多基本的处理都是共通的，所以这里就简单地称为“感知机”。)<br>（perceptron）这一算法。感知机是由美国学者 Frank Rosenblatt 在 1957 年提出来的。为何我们现在还要学习这一很久以前就有的算法呢？因为感知机也是作为神经网络（深度学习）的起源的算法。因此，学习感知机的构造也就是学习通向神经网络和深度学习的一种重要思想。</p><h2 id="感知机什么"><a href="#感知机什么" class="headerlink" title="感知机什么"></a>感知机什么</h2><p>感知机接收多个输入信号，输出一个信号。</p><p>感知机的多个输入信号都有各自固有的权重，这些权重发挥着控制各个信号的重要性的作用。也就是说，权重越大，对应该权重的信号的重要性就越高。</p><h2 id="简单逻辑电路"><a href="#简单逻辑电路" class="headerlink" title="简单逻辑电路"></a>简单逻辑电路</h2><h3 id="与门"><a href="#与门" class="headerlink" title="与门"></a>与门</h3><p>两个条件同为一个为1，为true。</p><h3 id="与非门"><a href="#与非门" class="headerlink" title="与非门"></a>与非门</h3><p>与门相反，任其中一个条件不为true，为true，即两个都不为true时，为true。</p><h3 id="或门"><a href="#或门" class="headerlink" title="或门"></a>或门</h3><p>其中一个条件为true，为true。</p><h2 id="感知机的实现"><a href="#感知机的实现" class="headerlink" title="感知机的实现"></a>感知机的实现</h2><h3 id="简单的实现"><a href="#简单的实现" class="headerlink" title="简单的实现"></a>简单的实现</h3><h3 id="导入权重和偏置"><a href="#导入权重和偏置" class="headerlink" title="导入权重和偏置"></a>导入权重和偏置</h3><h3 id="使用权重和偏置的实现"><a href="#使用权重和偏置的实现" class="headerlink" title="使用权重和偏置的实现"></a>使用权重和偏置的实现</h3><h2 id="感知机的局限性"><a href="#感知机的局限性" class="headerlink" title="感知机的局限性"></a>感知机的局限性</h2><h3 id="异或门"><a href="#异或门" class="headerlink" title="异或门"></a>异或门</h3><h3 id="线性和非线性"><a href="#线性和非线性" class="headerlink" title="线性和非线性"></a>线性和非线性</h3><p>感知机的局限性就在于它只能表示由一条直线分割的空间。<br>严格地讲，单层感知机无法表示异或门或者单层感知机无法分离非线性空间。</p><h2 id="多层感知机"><a href="#多层感知机" class="headerlink" title="多层感知机"></a>多层感知机</h2><h3 id="已有门电路的结合"><a href="#已有门电路的结合" class="headerlink" title="已有门电路的结合"></a>已有门电路的结合</h3><h3 id="异或门的实现"><a href="#异或门的实现" class="headerlink" title="异或门的实现"></a>异或门的实现</h3><h2 id="从与非门到计算机"><a href="#从与非门到计算机" class="headerlink" title="从与非门到计算机"></a>从与非门到计算机</h2><p>多层感知机可以实现比之前见到的电路更复杂的电路。比如，进行加法运算的加法器也可以用感知机实现。此外，将二进制转换为十进制的编码器、满足某些条件就输出 1 的电路（用于等价检验的电路）等也可以用感知机表示。实际上，使用感知机甚至可以表示计算机！</p><p>计算机是处理信息的机器。向计算机中输入一些信息后，它会按照某种既定的方法进行处理，然后输出结果。所谓“按照某种既定的方法进行处理”是指，计算机和感知机一样，也有输入和输出，会按照某个既定的规则进行计算。</p><p>人们一般会认为计算机内部进行的处理非常复杂，而令人惊讶的是，实际上只需要通过与非门的组合，就能再现计算机进行的处理。这一令人吃惊的事实说明了什么呢？说明使用感知机也可以表示计算机。前面也介绍了，与非门可以使用感知机实现。也就是说，如果通过组合与非门可以实现计算机的话，那么通过组合感知机也可以表示计算机（感知机的组合可以通过叠加了多层的单层感知机来表示）。</p><p>综上，多层感知机能够进行复杂的表示，甚至可以构建计算机。那么，什么构造的感知机才能表示计算机呢？层级多深才可以构建计算机呢？</p><p>理论上可以说 2 层感知机就能构建计算机。这是因为，已有研究证明，2 层感知机（严格地说是激活函数使用了非线性的 sigmoid 函数的感知机，具体请参照下一章）可以表示任意函数。但是，使用 2 层感知机的构造，通过设定合适的权重来构建计算机是一件非常累人的事情。实际上，在用与非门等低层的元件构建计算机的情况下，分阶段地制作所需的零件（模块）会比较自然，即先实现与门和或门，然后实现半加器和全加器，接着实现算数逻辑单元（ALU），然后实现 CPU。因此，通过感知机表示计算机时，使用叠加了多层的构造来实现是比较自然的流程。</p><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><ul><li>感知机是具有输入和输出的算法。给定一个输入后，将输出一个既定的值。</li><li>感知机将权重和偏置设定为参数。</li><li>使用感知机可以表示与门和或门等逻辑电路。</li><li>异或门无法通过单层感知机来表示。</li><li>使用2层感知机可以表示异或门。</li><li>单层感知机只能表示线性空间，而多层感知机可以表示非线性空间。</li><li>多层感知机（在理论上）可以表示计算机。</li></ul><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol><li><a href="http://www.ituring.com.cn/article/507311" target="_blank" rel="noopener">《深度学习入门》第 2 章 感知机 笔记</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> Deep Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Notes </tag>
            
            <tag> Deep Learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Notes-深度学习入门之Python基础</title>
      <link href="/2018/11/04/25-dl-python-basics-notes/"/>
      <url>/2018/11/04/25-dl-python-basics-notes/</url>
      
        <content type="html"><![CDATA[<p>简单介绍了 python 的基本语法，初步介绍了 NumPy（线性代数矩阵和数组），Matplotlib（绘制图形）库的基本使用。</p><h2 id="基本用法"><a href="#基本用法" class="headerlink" title="基本用法"></a>基本用法</h2><h3 id="with"><a href="#with" class="headerlink" title="with"></a>with</h3><ol><li><p>with语句时用于对try except finally 的优化，让代码更加美观</p></li><li><p>除了打开文件，with语句还可以用于哪些地方呢</p><p> with只适用于上下文管理器的调用，除了文件外，with还支持 threading、decimal等模块，当然我们也可以自己定义可以给with调用的上下文管理器</p></li></ol><h3 id="用于序列化的两个模块"><a href="#用于序列化的两个模块" class="headerlink" title="用于序列化的两个模块"></a>用于序列化的两个模块</h3><h4 id="json"><a href="#json" class="headerlink" title="json"></a>json</h4><p>用于字符串和Python数据类型间进行转换</p><p>json提供四个功能：dumps,dump,loads,load</p><h4 id="pickle"><a href="#pickle" class="headerlink" title="pickle"></a>pickle</h4><p>用于python特有的类型和python的数据类型间进行转换</p><p>pickle提供四个功能：dumps,dump,loads,load</p>]]></content>
      
      
      <categories>
          
          <category> Deep Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Notes </tag>
            
            <tag> Deep Learning </tag>
            
            <tag> Python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Python机器学习开发环境</title>
      <link href="/2018/11/04/24-python-machine-learning-env/"/>
      <url>/2018/11/04/24-python-machine-learning-env/</url>
      
        <content type="html"><![CDATA[<p>Python是著名的“龟叔”Guido van Rossum在1989年圣诞节期间，为了打发无聊的圣诞节而编写的一个编程语言。</p><p>目前，Python是机器学习实验、测试和开发的最常用语言。Python是解释型语言，语法简单，上手快，使用方便。</p><p>Python有很多的机器学习库，可以很方便的供用户调用，实现一些比较复杂的应用。</p><p>Python的哲学就是简单优雅，尽量写容易看明白的代码，尽量写少的代码。</p><h2 id="Python-数据分析库"><a href="#Python-数据分析库" class="headerlink" title="Python 数据分析库"></a>Python 数据分析库</h2><h3 id="Python-编程语言"><a href="#Python-编程语言" class="headerlink" title="Python 编程语言"></a>Python 编程语言</h3><p>Pythong Tutorial: <a href="https://docs.python.org/3/tutorial/" target="_blank" rel="noopener">https://docs.python.org/3/tutorial/</a></p><h3 id="NumPy"><a href="#NumPy" class="headerlink" title="NumPy"></a>NumPy</h3><p>提供常用的数值数组、矩阵等函数，为Python提供快速的多维数组处理能力。</p><p>官网：<a href="http://www.numpy.org" target="_blank" rel="noopener">http://www.numpy.org</a></p><p>文档QuickStart：<a href="https://docs.scipy.org/doc/numpy/user/quickstart.html" target="_blank" rel="noopener">https://docs.scipy.org/doc/numpy/user/quickstart.html</a></p><h3 id="SciPy"><a href="#SciPy" class="headerlink" title="SciPy"></a>SciPy</h3><p>是一种使用NumPy来做高等数学、信号处理、优化、统计的扩展包。<br>在NumPy基础上添加了众多科学计算工具包。</p><p>官网：<a href="https://www.scipy.org" target="_blank" rel="noopener">https://www.scipy.org</a></p><p><a href="http://docs.scipy.org/doc/" target="_blank" rel="noopener">Numpy and Scipy Documentation</a></p><p><a href="http://scipy.github.io/devdocs/" target="_blank" rel="noopener">Scipy Documentation</a></p><h3 id="Pandas"><a href="#Pandas" class="headerlink" title="Pandas"></a>Pandas</h3><p>Python Data AnalysiS Library</p><p>是一种构建于NumPy的高级数据结构和精巧工具，能快速简单的处理数据。</p><p>在Numpy基础上提供了更多的数据读写工具。</p><p>官网：<a href="http://pandas.pydata.org/" target="_blank" rel="noopener">http://pandas.pydata.org/</a></p><p>文档：<a href="http://pandas.pydata.org/pandas-docs/stable/" target="_blank" rel="noopener">http://pandas.pydata.org/pandas-docs/stable/</a></p><h3 id="Matplotlib"><a href="#Matplotlib" class="headerlink" title="Matplotlib"></a>Matplotlib</h3><p>Python绘图库</p><p>官网：<a href="http://matplotlib.org/" target="_blank" rel="noopener">http://matplotlib.org/</a></p><p>文档：<a href="http://matplotlib.org/contents.html" target="_blank" rel="noopener">matplotlib Documentation</a></p><h3 id="seaborn"><a href="#seaborn" class="headerlink" title="seaborn"></a>seaborn</h3><p>Seaborn is a Python data visualization library based on matplotlib. It provides a high-level interface for drawing attractive and informative statistical graphics.</p><p>官网：<a href="http://seaborn.pydata.org/" target="_blank" rel="noopener">http://seaborn.pydata.org/</a></p><h3 id="NLTK"><a href="#NLTK" class="headerlink" title="NLTK"></a>NLTK</h3><p>自然语言处理工具包 (Natural Language Toolkit)</p><p>官网：<a href="http://www.nltk.org/" target="_blank" rel="noopener">http://www.nltk.org/</a></p><h3 id="igraph"><a href="#igraph" class="headerlink" title="igraph"></a>igraph</h3><p>图计算和社交网络分析库</p><p><a href="http://igraph.org/python/" target="_blank" rel="noopener">http://igraph.org/python/</a></p><h3 id="Scikit-learn"><a href="#Scikit-learn" class="headerlink" title="Scikit-learn"></a>Scikit-learn</h3><p>是建立在Scipy之上的一个用于机器学习的Python模块。</p><p><a href="http://scikit-learn.org/stable/index.html" target="_blank" rel="noopener">http://scikit-learn.org/stable/index.html</a></p><h3 id="TensorFlow"><a href="#TensorFlow" class="headerlink" title="TensorFlow"></a>TensorFlow</h3><p><a href="https://www.tensorflow.org/" target="_blank" rel="noopener">官网：https://www.tensorflow.org/</a></p><h2 id="Python-开发环境"><a href="#Python-开发环境" class="headerlink" title="Python 开发环境"></a>Python 开发环境</h2><h3 id="pip"><a href="#pip" class="headerlink" title="pip"></a>pip</h3><p>pip 是一个Python包管理工具，主要是用于安装 PyPI 上的软件包，可以替代 easy_install 工具。</p><p>安装Python包的推荐工具: <a href="https://pypi.python.org/pypi/pip" target="_blank" rel="noopener">https://pypi.python.org/pypi/pip</a></p><p>更换国内源:<code>pip install -i https://pypi.tuna.tsinghua.edu.cn/simplenumpy</code></p><p>升级 TensorFlow</p><pre class=" language-shell"><code class="language-shell">sudo pip install --upgrade tensorflow</code></pre><h3 id="IPython"><a href="#IPython" class="headerlink" title="IPython"></a>IPython</h3><p>IPython是一个交互式的Python环境，是Python的原生交互式 shell 的增强版，可以完成许多不同寻常的任务，比如帮助实现并行化计算；主要使用它提供的交互性帮助，比如代码着色、改进了的命令行回调、制表符完成、宏功能以及改进了的交互式帮助。</p><p><a href="http://ipython.org" target="_blank" rel="noopener">官网：http://ipython.org</a></p><h3 id="Jupyter-Notebook"><a href="#Jupyter-Notebook" class="headerlink" title="Jupyter Notebook"></a>Jupyter Notebook</h3><p>Jupyter Notebook，以前又叫IPython notebook，是一个交互式的编程环境， 现在已支持运行40+种编程语言，可以用来编写漂亮的交互式文档。用Jupyter Notebook编写Python代码，能很好的交互式展现运行结果。</p><p><a href="https://jupyter.org" target="_blank" rel="noopener">官网：https://jupyter.org</a></p><h3 id="Virtualenv"><a href="#Virtualenv" class="headerlink" title="Virtualenv"></a>Virtualenv</h3><pre class=" language-shell"><code class="language-shell">pip install virtualenv</code></pre><p><a href="https://pypi.org/project/virtualenv/" target="_blank" rel="noopener">官网：virtualenv</a></p><p><a href="https://virtualenv.pypa.io/en/stable/" target="_blank" rel="noopener">virtualenv doc</a></p><h3 id="Anaconda"><a href="#Anaconda" class="headerlink" title="Anaconda"></a>Anaconda</h3><p>Anaconda Python 是Python科学技术包的合集，功能和Python(x,y) 类似。它是新起之秀，已更新多次了。包管理使用conda，GUI基于 PySide，所有的包基本上都是最新版，没有PyQt和wxpython等，容量适中，但该有的科学计算包都有：numpy，sicpy，matplotlib，spyder….。</p><p>Anaconda Python 是完全免费的企业级的Python发行大规模数据处理、预测分析和科学计算工具。</p><p>Linux系统里面，Anaconda 安装、更新和删除都很方便，且所有的东西都只安装在一个目录中 /home/user/anaconda/。Anaconda的开发和维护中有Python创始人和社区的核心成员。Anaconda目前提供Python 2.6.X,Python 2.7.X,Python 3.3.X和Python 3.4.X四个系列发行包，这也是其他发行版所望尘莫及的。因此在各种操作系统中，无论是Linux，还是Windows、Mac,都推荐Anaconda！</p><p>由于Anacoda是Python科学技术包的合集，所以不同的包所遵循的协议不一样，可以参看<a href="http://docs.continuum.io/anaconda/licenses.html" target="_blank" rel="noopener">http://docs.continuum.io/anaconda/licenses.html</a></p><p><a href="https://www.anaconda.com/" target="_blank" rel="noopener">官网：https://www.anaconda.com/</a></p><p>Anacoda 常用文档如下：</p><p><a href="https://docs.continuum.io" target="_blank" rel="noopener">Anaconda 官方文档</a></p><p><a href="https://conda.io/docs/using/using.html" target="_blank" rel="noopener">conda 官方文档</a></p><p><a href="https://anaconda.org/bermaker/dashboard" target="_blank" rel="noopener">My Anaconda Landscape</a></p><p><a href="http://python.jobbole.com/86236/" target="_blank" rel="noopener">Anaconda使用总结</a></p><p>Anaconda集成了IPthon、Jupyter Notebook，能自动解决Python的依赖问题。使用Anaconda安装、管理、使用Python及Python的各种包很方便，推荐使用Anaconda。</p><h4 id="Anaconda-基本用法"><a href="#Anaconda-基本用法" class="headerlink" title="Anaconda 基本用法"></a>Anaconda 基本用法</h4><ol><li><p>查看已安装的环境</p><pre class=" language-shell"><code class="language-shell">conda info -e</code></pre><p>输出如下：</p><pre><code># conda environments:#py36                  *  /Users/huangxiaomu/anaconda/envs/py36root                     /Users/huangxiaomu/anaconda</code></pre></li><li><p>切换开发环境</p><pre class=" language-shell"><code class="language-shell">source activate py36</code></pre></li><li><p>切换国内源<br>在当前用户根目录下执行</p><pre class=" language-shell"><code class="language-shell">conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/conda config --set show_channel_urls yes</code></pre><p>执行完上述命令后会在当前用户目录下生成.condarc文件</p></li></ol><p>确认.condarc文件内容：</p><pre><code>channels:  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/  - defaultsshow_channel_urls: true</code></pre><p>确认无误后，关闭控制台窗口，重新打开才会重新加载配置。</p><ol start="4"><li>安装 flake8<pre class=" language-shell"><code class="language-shell">conda install --name base flake8</code></pre></li></ol><h2 id="查看Python版本"><a href="#查看Python版本" class="headerlink" title="查看Python版本"></a>查看Python版本</h2><pre class=" language-python"><code class="language-python"><span class="token keyword">import</span> sys<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'Python: {}'</span><span class="token punctuation">.</span>format<span class="token punctuation">(</span>sys<span class="token punctuation">.</span>version<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">import</span> scipy<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'scipy: {}'</span><span class="token punctuation">.</span>format<span class="token punctuation">(</span>scipy<span class="token punctuation">.</span>__version__<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># numpy</span><span class="token keyword">import</span> numpy<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'numpy: {}'</span><span class="token punctuation">.</span>format<span class="token punctuation">(</span>numpy<span class="token punctuation">.</span>__version__<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># matplotlib</span><span class="token keyword">import</span> matplotlib<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'matplotlib: {}'</span><span class="token punctuation">.</span>format<span class="token punctuation">(</span>matplotlib<span class="token punctuation">.</span>__version__<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># pandas</span><span class="token keyword">import</span> pandas<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'pandas: {}'</span><span class="token punctuation">.</span>format<span class="token punctuation">(</span>pandas<span class="token punctuation">.</span>__version__<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># scikit-learn</span><span class="token keyword">import</span> sklearn<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'sklearn: {}'</span><span class="token punctuation">.</span>format<span class="token punctuation">(</span>sklearn<span class="token punctuation">.</span>__version__<span class="token punctuation">)</span><span class="token punctuation">)</span></code></pre><p>我的开发环境输出如下：</p><pre><code>Python: 2.7.13 |Anaconda custom (x86_64)| (default, Dec 20 2016, 23:05:08)[GCC 4.2.1 Compatible Apple LLVM 6.0 (clang-600.0.57)]scipy: 0.19.0numpy: 1.12.1matplotlib: 2.0.2pandas: 0.20.1sklearn: 0.19.0</code></pre><p>python 3.6 环境如下：</p><pre><code>Python: 3.6.6 |Anaconda, Inc.| (default, Jun 28 2018, 11:07:29)[GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)]scipy: 1.1.0numpy: 1.14.5matplotlib: 2.2.2pandas: 0.23.2sklearn: 0.19.1</code></pre><p>Anaconda3 base 环境如下：</p><pre><code>Python: 3.6.5 |Anaconda, Inc.| (default, Apr 26 2018, 08:42:37)[GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)]numpy: 1.15.0scipy: 1.1.0pandas: 0.23.3matplotlib: 2.2.2seaborn: 0.9.0sklearn: 0.19.2tensorflow: 1.9.0keras: 2.2.2</code></pre><h3 id="pyenv"><a href="#pyenv" class="headerlink" title="pyenv"></a>pyenv</h3><p>Simple Python version management.</p><p>pyenv lets you easily switch between multiple versions of Python. It’s simple, unobtrusive, and follows the UNIX tradition of single-purpose tools that do one thing well.</p><p><a href="https://github.com/pyenv/pyenv" target="_blank" rel="noopener">官网：https://github.com/pyenv/pyenv</a></p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol><li><a href="http://ipython.readthedocs.io/en/stable/index.html" target="_blank" rel="noopener">IPython Documentation</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Machine Learning </tag>
            
            <tag> Python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Notes-深度学习入门之总览</title>
      <link href="/2018/11/04/23-dl-introduction-notes/"/>
      <url>/2018/11/04/23-dl-introduction-notes/</url>
      
        <content type="html"><![CDATA[<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>深度学习入门：基于Python的理论和实现<br><img src="https://note.youdao.com/yws/api/personal/file/WEB1b21eedb4a3e78c5e742ffa97d9c15d5?method=download&amp;shareKey=b4dea69cb97bc9518be0886de84013c7" alt=""></p><ul><li>日本深度学习入门经典畅销书，原版上市不足2年印刷已达100000册。长期位列日亚“人工智能”类图书榜首，众多五星好评。</li><li>使用Python 3，尽量不依赖外部库或工具，从零创建一个深度学习模型。</li><li>示例代码清晰，源代码可下载，需要的运行环境非常简单。读者可以一边读书一边执行程序，简单易上手。</li><li>使用平实的语言，结合直观的插图和具体的例子，将深度学习的原理掰开揉碎讲解，简明易懂。</li><li>使用计算图介绍复杂的误差反向传播法，非常直观。</li><li>相比“花书”，本书更合适入门。</li></ul><p>从零开始编写可实际运行的程序，一边看源代码，一边思考。</p><h2 id="Main-Content"><a href="#Main-Content" class="headerlink" title="Main Content"></a>Main Content</h2><p><img src="https://ws4.sinaimg.cn/large/006tKfTcly1g0fbgzlyk3j30kd2t57bb.jpg" alt="深度学习入门_Outline"></p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol><li><a href="https://book.douban.com/subject/30270959/" target="_blank" rel="noopener">深度学习入门</a></li><li><a href="https://gitbook.cn/gitchat/geekbook/5be1048a665e8a7d734a2c3f/topic/5be91e802c33167c317c6f04" target="_blank" rel="noopener">深度学习入门：基于 Python 的理论与实现</a></li><li><a href="http://www.ituring.com.cn/book/1921" target="_blank" rel="noopener">深度学习入门：基于Python的理论与实现 图灵社区</a></li><li><a href="https://github.com/oreilly-japan/deep-learning-from-scratch" target="_blank" rel="noopener">配套代码：deep-learning-from-scratch</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> Deep Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Notes </tag>
            
            <tag> Deep Learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>统计学习方法概论</title>
      <link href="/2018/11/03/21-statistics-learning-methods/"/>
      <url>/2018/11/03/21-statistics-learning-methods/</url>
      
        <content type="html"><![CDATA[<p>统计学习(statistical learning)是关于计算机基于数据构建概率统计模型并运用模型对数据进行预测与分析的一门学科。<br>统计学习也称为统计机器学习(statistical machine learning)。<br>赫尔伯特·西蒙(Herbert A. Simon)曾对“学习”以下定义：如果一个系统能够通过执行某个过程改进它的性能，这就是学习。<br>按照这一观点，统计学习就是计算机系统通过运用数据及统计方法提供系统性能的机器学习。</p><a id="more"></a><h2 id="统计学习"><a href="#统计学习" class="headerlink" title="统计学习"></a>统计学习</h2><h3 id="统计学习的特点"><a href="#统计学习的特点" class="headerlink" title="统计学习的特点"></a>统计学习的特点</h3><p>统计学习的主要特点：</p><ol><li><p>统计学习以计算机及网络为平台，是建立在计算机及网络之上的；</p></li><li><p>统计学习以数据为研究对象，是数据驱动的学科；</p></li><li><p>统计学习的目的是对数据进行预测与分析；</p></li><li><p>统计学习以方法为中心，统计学习方法构建并应用模型进行预测与分析</p></li><li><p>统计学习是概率论、统计学、信息论、计算理论、最优化理论及计算机科学等多个领域的交叉学科，并且在发展中逐步形成独立的理论体系与方法论。</p></li></ol><h3 id="统计学习的对象"><a href="#统计学习的对象" class="headerlink" title="统计学习的对象"></a>统计学习的对象</h3><p>统计学习的对象是数据(data)，它从数据出发，提取数据的特征，抽象出数据的模型，发现数据中的知识，又回到对数据的分析与预测中去。</p><p>统计学习关于数据的假设是同类数据具有一定的统计规律性，这是统计学习的前提。</p><h3 id="统计学习的目的"><a href="#统计学习的目的" class="headerlink" title="统计学习的目的"></a>统计学习的目的</h3><p>统计学习用于对数据进行预测与分析，特别是对未知新数据进行预测与分析。</p><p>对数据的预测与分析是通过构建概率统计模型实现的。统计学习总的目标就是考虑学习什么样的模型和如何学习模型，以使模型能对数据进行准确的预测与分析，同时也要考虑尽可能地提高学习效率。</p><h3 id="统计学习的方法"><a href="#统计学习的方法" class="headerlink" title="统计学习的方法"></a>统计学习的方法</h3><p>统计学习由监督学习(supervised learning)、非监督学习(unsupervised learning)、半监督学习(semi-supervised learning)和强化学习(reinforcement learning)等组成。</p><p>统计学习方法包括模型的假设空间、模型选择的准则以及模型学习的算法，称其为统计学习方法的三要素，简称模型(model)、策略(strategy)和算法(algorithm)。</p><p>实现统计学习方法的步骤如下：</p><ol><li><p>得到一个有限的训练数据集合；</p></li><li><p>确定包含所有可能的模型的假设空间，即学习模型的集合；</p></li><li><p>确定模型选择的准则，即学习的策略；</p></li><li><p>实现求解最优模型的算法，及学习的算法；</p></li><li><p>通过学习方法选择最优模型；</p></li><li><p>利用学习的最优模型对新数据进行预测或分析。</p></li></ol><h3 id="统计学习的研究"><a href="#统计学习的研究" class="headerlink" title="统计学习的研究"></a>统计学习的研究</h3><p>统计学习研究一般包括统计学习方法(statistical learning medthod)、统计学习理论(statistical learning theory)及统计学习应用(application of statistical learning)三个方面。</p><h3 id="统计学习的重要性"><a href="#统计学习的重要性" class="headerlink" title="统计学习的重要性"></a>统计学习的重要性</h3><ol><li><p>统计学习是处理海量手的有效方法</p></li><li><p>统计学习是计算机智能化的有效手段</p></li><li><p>统计学习是计算机科学发展的一个重要组成部分</p></li></ol><h2 id="监督学习"><a href="#监督学习" class="headerlink" title="监督学习"></a>监督学习</h2><p>统计学习包括监督学习、非监督学习、半监督学习和强化学习。</p><h3 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h3><h3 id="问题的形式化"><a href="#问题的形式化" class="headerlink" title="问题的形式化"></a>问题的形式化</h3><h2 id="统计学习三要素"><a href="#统计学习三要素" class="headerlink" title="统计学习三要素"></a>统计学习三要素</h2><h3 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h3><h3 id="策略"><a href="#策略" class="headerlink" title="策略"></a>策略</h3><h3 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h3><h2 id="模型评估与模型选择"><a href="#模型评估与模型选择" class="headerlink" title="模型评估与模型选择"></a>模型评估与模型选择</h2><h3 id="训练误差与测试误差"><a href="#训练误差与测试误差" class="headerlink" title="训练误差与测试误差"></a>训练误差与测试误差</h3><h3 id="过拟合与模型选择"><a href="#过拟合与模型选择" class="headerlink" title="过拟合与模型选择"></a>过拟合与模型选择</h3><h2 id="正则化与交叉验证"><a href="#正则化与交叉验证" class="headerlink" title="正则化与交叉验证"></a>正则化与交叉验证</h2><h3 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a>正则化</h3><h3 id="交叉验证"><a href="#交叉验证" class="headerlink" title="交叉验证"></a>交叉验证</h3><h2 id="泛化能力"><a href="#泛化能力" class="headerlink" title="泛化能力"></a>泛化能力</h2><h3 id="泛化误差"><a href="#泛化误差" class="headerlink" title="泛化误差"></a>泛化误差</h3><h3 id="泛化误差上届"><a href="#泛化误差上届" class="headerlink" title="泛化误差上届"></a>泛化误差上届</h3><h2 id="生成模型与判别模型"><a href="#生成模型与判别模型" class="headerlink" title="生成模型与判别模型"></a>生成模型与判别模型</h2><h2 id="分类问题"><a href="#分类问题" class="headerlink" title="分类问题"></a>分类问题</h2><h2 id="标注问题"><a href="#标注问题" class="headerlink" title="标注问题"></a>标注问题</h2><h2 id="回归问题"><a href="#回归问题" class="headerlink" title="回归问题"></a>回归问题</h2><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ol><li><a href="https://book.douban.com/subject/10590856/" target="_blank" rel="noopener">统计学习方法 李航 清华大学出版社</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> Machine Learning </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Machine Learning </tag>
            
            <tag> Statistics </tag>
            
            <tag> Data Mining </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>C/C++编译流程</title>
      <link href="/2018/11/02/20-c-c-compile/"/>
      <url>/2018/11/02/20-c-c-compile/</url>
      
        <content type="html"><![CDATA[<h2 id="编译的基本流程"><a href="#编译的基本流程" class="headerlink" title="编译的基本流程"></a>编译的基本流程</h2><p><img src="https://ws3.sinaimg.cn/large/006tKfTcly1g0ffqrtuqwj30qw04qaa1.jpg" alt="C/C++编译流程"></p><h2 id="C详细过程"><a href="#C详细过程" class="headerlink" title="C详细过程"></a>C详细过程</h2><pre class=" language-c"><code class="language-c"><span class="token macro property">#<span class="token directive keyword">include</span> <span class="token string">&lt;stdio.h></span></span><span class="token keyword">int</span> <span class="token function">main</span><span class="token punctuation">(</span><span class="token keyword">int</span> argc<span class="token punctuation">,</span> <span class="token keyword">char</span> <span class="token operator">*</span>argv<span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">{</span>  <span class="token function">printf</span><span class="token punctuation">(</span><span class="token string">"Hello World!\n"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>  <span class="token keyword">return</span> <span class="token number">0</span><span class="token punctuation">;</span><span class="token punctuation">}</span></code></pre><h3 id="预处理-cpp"><a href="#预处理-cpp" class="headerlink" title="预处理(cpp)"></a>预处理(cpp)</h3><p><code>gcc -E hello.c -o hello.i</code></p><p>预处理，主要处理以下指令：宏定义指令，条件编译指令，头文件包含指令。 预处理所完成的基本上是对源程序的“替代”工作。经过此种替代，生成一个没有宏定义、没有条件编译指令，头文件都被展开（递归展开）的文件。</p><h3 id="编译-ccl"><a href="#编译-ccl" class="headerlink" title="编译(ccl)"></a>编译(ccl)</h3><p><code>gcc -S hello.i -o hello.s</code></p><p>编译，就是把C/C++代码“翻译”成汇编代码。</p><h3 id="汇编-as"><a href="#汇编-as" class="headerlink" title="汇编(as)"></a>汇编(as)</h3><p><code>gcc -c hello.s -o hello.o</code></p><p>汇编，就是将生成的汇编代码翻译成符合一定格式的机器代码，在Linux上一般表现为ELF目标文件。</p><h3 id="链接-ld"><a href="#链接-ld" class="headerlink" title="链接(ld)"></a>链接(ld)</h3><p><code>gcc hello.o -o hello</code></p><p>链接，将生成的目标文件和系统库文件进行链接，最终生成了可以在特定平台运行的可执行文件。为什么还要链接系统库中的某些目标文件（crt1.o, crti.o等）呢？这些目标文件都是用来初始化或者回收C运行时环境的，比如说堆内存分配上下文环境的初始化等，实际上crt也正是C RunTime的缩写。这也暗示了另外一点：程序并不是从main函数开始执行的，而是从crt中的某个入口开始的，在Linux上此入口是_start。而且默认情况下，ld是将这些系统库文件（本身也是动态库）都是以动态链接方式加入应用程序的，如果要以静态连接的方式进行，需要显示的指定ld命令的参数-static。</p><h3 id="其它"><a href="#其它" class="headerlink" title="其它"></a>其它</h3><p>此外，还有一个优化阶段。优化处理是编译系统中一项比较艰深的技术。它涉及到的问题不仅同编译技术本身有关，而且同机器的硬件环境也有很大的关系。优化一部分是对中间代码的优化。 这种优化不依赖于具体的计算机。另一种优化则主要针对目标代码的生成而进行的。对于前一种优化，主要的工作是删除公共表达式、循环优化（代码外提、强度削弱、变换循环控制条件、已知量的合并等）、复写传播，以及无用赋值的删除，等等。 后 一种类型的优化同机器的硬件结构密切相关，最主要的是考虑是如何充分利用机器的各个硬件寄存器存放的有关变量的值，以减少对于内存的访问次数。另外，如何 根据机器硬件执行指令的特点（如流水线、RISC、CISC、VLIW等）而对指令进行一些调整使目标代码比较短，执行的效率比较高。</p><h2 id="C"><a href="#C" class="headerlink" title="C++"></a>C++</h2><pre class=" language-c++"><code class="language-c++">g++ hello.cpp -o hello</code></pre><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ol><li><a href="http://www.cnblogs.com/Lynn-Zhang/p/5377024.html" target="_blank" rel="noopener">C/C++程序编译流程（预处理-&gt;编译-&gt;汇编-&gt;链接）</a></li><li><a href="http://blog.csdn.net/edisonlg/article/details/7081357" target="_blank" rel="noopener">C++编译链接过程</a></li><li><a href="http://www.cnblogs.com/ucas/p/5778664.html" target="_blank" rel="noopener">在Linux下编译C++程序</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> C++ </category>
          
      </categories>
      
      
        <tags>
            
            <tag> C++ </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>DCL DML DQL DDL</title>
      <link href="/2018/11/02/22-sql/"/>
      <url>/2018/11/02/22-sql/</url>
      
        <content type="html"><![CDATA[<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>SQL(Structure Query Language)语言是数据库的核心语言。</p><p>SQL的发展是从1974年开始的，其发展过程如下： </p><p>1974年—–由Boyce和Chamberlin提出，当时称SEQUEL。 </p><p>1976年—–IBM公司的Sanjase研究所在研制RDBMS SYSTEM R 时改为SQL。 </p><p>1979年—–ORACLE公司发表第一个基于SQL的商业化RDBMS产品。 </p><p>1982年—–IBM公司出版第一个RDBMS语言SQL/DS。 </p><p>1985年—–IBM公司出版第一个RDBMS语言DB2。 </p><p>1986年—–美国国家标准化组织ANSI宣布SQL作为数据库工业标准。 </p><p>SQL是一个标准的数据库语言，是面向集合的描述性非过程化语言。 它功能强，效率高，简单易学易维护（迄今为止，我还没见过比它还好 学的语言）。然而SQL语言由于以上优点，同时也出现了这样一个问题： 它是非过程性语言，即大多数语句都是独立执行的，与上下文无关，而 绝大部分应用都是一个完整的过程，显然用SQL完全实现这些功能是很困 难的。所以大多数数据库公司为了解决此问题，作了如下两方面的工作：</p><p>(1)扩充SQL，在SQL中引入过程性结构；</p><p>(2)把SQL嵌入到高级语言中， 以便一起完成一个完整的应用。</p><h2 id="SQL语言的分类"><a href="#SQL语言的分类" class="headerlink" title="SQL语言的分类"></a>SQL语言的分类</h2><p>SQL语言共分为四大类：数据查询语言DQL，数据操纵语言DML，数据定义语言DDL，数据控制语言DCL。</p><h3 id="DQL"><a href="#DQL" class="headerlink" title="DQL"></a>DQL</h3><p>DQL（Data Query Language SELECT ）数据查询语言。数据查询语言DQL 数据查询语言DQL基本结构是由SELECT子句，FROM子句，WHERE 子句组成的查询块： SELECT &lt;字段名表&gt; FROM &lt;表或视图名&gt; WHERE &lt;查询条件&gt;</p><h3 id="DML"><a href="#DML" class="headerlink" title="DML"></a>DML</h3><p>DML（ Data Manipulation Language）数据操纵语言。数据操纵语言DML 数据操纵语言DML主要有三种形式： 1) 插入：INSERT 2) 更新：UPDATE 3) 删除：DELETE</p><h3 id="DDL"><a href="#DDL" class="headerlink" title="DDL"></a>DDL</h3><p>DDL（data definition language）数据定义语言。数据定义语言DDL 数据定义语言DDL用来创建数据库中的各种对象—–表、视图、 索引、同义词、聚簇等如： CREATE TABLE/VIEW/INDEX/SYN/CLUSTER | | | | | 表 视图 索引 同义词 簇<br>DDL操作是隐性提交的！不能rollback </p><h3 id="DCL"><a href="#DCL" class="headerlink" title="DCL"></a>DCL</h3><p>DCL（Data Control Language）是数据库控制语言。数据控制语言DCL 数据控制语言DCL用来授予或回收访问数据库的某种特权，并控制 数据库操纵事务发生的时间及效果，对数据库实行监视等。如：<br>1) GRANT：授权。<br>2) ROLLBACK [WORK] TO [SAVEPOINT]：回退到某一点。 回滚—ROLLBACK 回滚命令使数据库状态回到上次最后提交的状态。其格式为： SQL&gt;ROLLBACK;<br>3) COMMIT [WORK]：提交。<br>    在数据库的插入、删除和修改操作时，只有当事务在提交到数据 库时才算完成。在事务提交前，只有操作数据库的这个人才能有权看 到所做的事情，别人只有在最后提交完成后才可以看到。 提交数据有三种类型：显式提交、隐式提交及自动提交。下面分 别说明这三种类型。<br>(1) 显式提交 用COMMIT命令直接完成的提交为显式提交。其格式为： SQL&gt;COMMIT；<br>(2) 隐式提交 用SQL命令间接完成的提交为隐式提交。这些命令是： ALTER，AUDIT，COMMENT，CONNECT，CREATE，DISCONNECT，DROP， EXIT，GRANT，NOAUDIT，QUIT，REVOKE，RENAME。<br>(3) 自动提交 若把AUTOCOMMIT设置为ON，则在插入、修改、删除语句执行后， 系统将自动进行提交，这就是自动提交。其格式为： SQL&gt;SET AUTOCOMMIT ON；</p><h2 id="另外一种解释"><a href="#另外一种解释" class="headerlink" title="另外一种解释"></a>另外一种解释</h2><h3 id="DCL-1"><a href="#DCL-1" class="headerlink" title="DCL"></a>DCL</h3><p>DCL（Data Control Language）是数据库控制语言，是用来设置或更改数据库用户或角色权限的语句，包括（grant,deny,revoke等）语句。</p><p>在默认状态下，只有sysadmin,dbcreator,db_owner或db_securityadmin等人员才有权力执行DCL<br>DCL数据库控制语言不同于程序设计语言，SQL语言（结构化程序设计语言）的组成部分包括了DCL数据库控制语言。</p><p>SQL语言包括三种主要程序设计语言类别的语句：数据定义语言(DDL)，数据操作语言(DML)及数据控制语言(DCL)。</p><h3 id="DML-1"><a href="#DML-1" class="headerlink" title="DML"></a>DML</h3><p>DML (Data Manipulation Language) 数据操纵语言，命令使用户能够查询数据库以及操作已有数据库中的数据的计算机语言。</p><p>具体是指是UPDATE更新、INSERT插入、DELETE删除。</p><p>DML包括：INSERT、UPDATE、DELETE。注意，select语句属于DQL(Data Query Language)。在oracle中需要进行事务提交，否则不能操作成功。</p><h3 id="DDL-1"><a href="#DDL-1" class="headerlink" title="DDL"></a>DDL</h3><p>数据库模式定义语言DDL(Data Definition Language)，是用于描述数据库中要存储的现实世界实体的语言。</p><p>一个数据库模式包含该数据库中所有实体的描述定义。这些定义包括结构定义、操作方法定义等。</p><h3 id="DQL-1"><a href="#DQL-1" class="headerlink" title="DQL"></a>DQL</h3><p>DQL（Data Query Language SELECT ）数据查询语言，select语句。<br>select具体用法:</p><pre><code>　　SELECT select_list　　[ INTO new_table ]　　FROM table_source　　[ WHERE search_condition ]　　[ GROUP BY group_by_expression ]　　[ HAVING search_condition ]　　[ ORDER BY order_expression [ ASC | DESC ] ]</code></pre>]]></content>
      
      
      <categories>
          
          <category> DataBase </category>
          
      </categories>
      
      
        <tags>
            
            <tag> SQL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Linux下的C语言开发</title>
      <link href="/2018/11/01/19-linux-c-development/"/>
      <url>/2018/11/01/19-linux-c-development/</url>
      
        <content type="html"><![CDATA[<h2 id="工具"><a href="#工具" class="headerlink" title="工具"></a>工具</h2><ol><li>学会使用vim/emacs，vim/emacs是linux下最常用的源码编辑具，不光要学会用它们编辑源码，还要学会用它们进行查找、定位、替换等。</li><li>学会makefile文件的编写规则，并结合使用工具aclocal、autoconf和automake生成makefile文件。</li><li>掌握gcc和gdb的基本用法。掌握gcc的用法对于构建一个软件包很有益处，当软件包包含的文件比较多的时候，你还能用gcc把它手动编译出来，你就会对软件包中各个文件间的依赖关系有一个清晰的了解。</li><li>掌握svn/cvs的基本用法。这是linux，也是开源社区最常用的版本管理系统。可以去试着参加sourceforge上的一些开源项目。</li></ol><h2 id="linux-unix系统调用与标准C库"><a href="#linux-unix系统调用与标准C库" class="headerlink" title="linux/unix系统调用与标准C库"></a>linux/unix系统调用与标准C库</h2><p>推荐学习资料为steven先生的UNIX环境高级编程（简称APUE）。</p><h2 id="库的学习"><a href="#库的学习" class="headerlink" title="库的学习"></a>库的学习</h2><ol><li>glib库</li><li>libxml库</li><li>readline库</li><li>readline库</li><li>gtk+和KDE库</li></ol><h2 id="网络"><a href="#网络" class="headerlink" title="网络"></a>网络</h2><p>推荐学习资料steven先生的UNIX网络编程（简称UNP）和TCP/IP协议详解</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ol><li><a href="http://os.51cto.com/art/200710/57820.htm" target="_blank" rel="noopener">Linux系统下的C语言开发都需要学些什么</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> C++ </category>
          
      </categories>
      
      
        <tags>
            
            <tag> C++ </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Java GC 分析和调优</title>
      <link href="/2018/10/29/18-java-gc/"/>
      <url>/2018/10/29/18-java-gc/</url>
      
        <content type="html"><![CDATA[<h2 id="dump"><a href="#dump" class="headerlink" title="dump"></a>dump</h2><p>在故障定位(尤其是out of memory)和性能分析的时候，经常会用到一些文件来帮助我们排除代码问题。这些文件记录了JVM运行期间的内存占用、线程执行等情况，这就是我们常说的dump文件。常用的有heap dump和thread dump（也叫javacore，或java dump）。我们可以这么理解：heap dump记录内存信息的，thread dump是记录CPU信息的。</p><h3 id="heap-dump"><a href="#heap-dump" class="headerlink" title="heap dump"></a>heap dump</h3><p>heap dump文件是一个二进制文件，它保存了某一时刻JVM堆中对象使用情况。HeapDump文件是指定时刻的Java堆栈的快照，是一种镜像文件。Heap Analyzer工具通过分析HeapDump文件，哪些对象占用了太多的堆栈空间，来发现导致内存泄露或者可能引起内存泄露的对象。</p><h3 id="thread-dump"><a href="#thread-dump" class="headerlink" title="thread dump"></a>thread dump</h3><p>hread dump文件主要保存的是java应用中各线程在某一时刻的运行的位置，即执行到哪一个类的哪一个方法哪一个行上。thread dump是一个文本文件，打开后可以看到每一个线程的执行栈，以stacktrace的方式显示。通过对thread dump的分析可以得到应用是否“卡”在某一点上，即在某一点运行的时间太长，如数据库查询，长期得不到响应，最终导致系统崩溃。单个的thread dump文件一般来说是没有什么用处的，因为它只是记录了某一个绝对时间点的情况。比较有用的是，线程在一个时间段内的执行情况。</p><p>两个thread dump文件在分析时特别有效，困为它可以看出在先后两个时间点上，线程执行的位置，如果发现先后两组数据中同一线程都执行在同一位置，则说明此处可能有问题，因为程序运行是极快的，如果两次均在某一点上，说明这一点的耗时是很大的。通过对这两个文件进行分析，查出原因，进而解决问题。</p><h2 id="JDK-常用分析工具"><a href="#JDK-常用分析工具" class="headerlink" title="JDK 常用分析工具"></a>JDK 常用分析工具</h2><h3 id="jps"><a href="#jps" class="headerlink" title="jps"></a>jps</h3><p>JPS, JVM Process Status Tool</p><p>显示指定系统内所有的HotSpot虚拟机进程</p><h3 id="jstack"><a href="#jstack" class="headerlink" title="jstack"></a>jstack</h3><p>jstack, Stack Trace for Java</p><p>显示虚拟机的线程快照</p><p>获取thread dump文件</p><pre class=" language-java"><code class="language-java">jstack <span class="token number">2576</span> <span class="token operator">></span> thread<span class="token punctuation">.</span>txt</code></pre><h3 id="jmap"><a href="#jmap" class="headerlink" title="jmap"></a>jmap</h3><p>jmap, Memory Map for Java</p><p>生成虚拟机的内存转储快照(heap dump文件)</p><p>获取heap dump文件</p><pre class=" language-java"><code class="language-java">jmap <span class="token operator">-</span>dump<span class="token operator">:</span>format<span class="token operator">=</span>b<span class="token punctuation">,</span>file<span class="token operator">=</span>heap<span class="token punctuation">.</span>hprof <span class="token operator">&lt;</span>pid<span class="token operator">></span></code></pre><p>format=b的含义是，dump出来的文件时二进制格式。</p><p>file-heap.bin的含义是，dump出来的文件名是heap.bin。</p><p><pid>就是JVM的进程号。</pid></p><p>导出后，使用 Eclipse 插件 Memory Analyzer 分析。</p><p>或者使用JDK自带的jhat命令。</p><h3 id="jstat"><a href="#jstat" class="headerlink" title="jstat"></a>jstat</h3><p>jstat, JVM Statistics Monitoring Tool</p><p>用于收集HotSpot虚拟机各方面的运行数据</p><h3 id="jhat"><a href="#jhat" class="headerlink" title="jhat"></a>jhat</h3><p>jhat：JVM Heap Dump Browser，用于分析heap dump文件，会建立一个HTTP/HTML服务器，让用户可以在浏览器查看分析结果</p><p> jhat是用来分析java堆的命令，可以将堆中的对象以html的形式显示出来，包括对象的数量，大小等等，并支持对象查询语言。</p><pre class=" language-java"><code class="language-java">jhat <span class="token operator">-</span>port <span class="token number">5000</span> heap<span class="token punctuation">.</span>hprof</code></pre><p>比如：</p><pre><code>jhat -port 8099 heap_2018010702.binReading from heap_2018010702.bin...Dump file created Mon Jan 07 16:52:26 CST 2019Snapshot read, resolving...Resolving 505190 objects...Chasing references, expect 101 dots.....................................................................................................Eliminating duplicate references.....................................................................................................Snapshot resolved.Started HTTP server on port 8099Server is ready.</code></pre><h3 id="jinfo"><a href="#jinfo" class="headerlink" title="jinfo"></a>jinfo</h3><p>jinfo, Configuration Info for Java</p><p>显示虚拟机配置信息</p><h3 id="jcmd"><a href="#jcmd" class="headerlink" title="jcmd"></a>jcmd</h3><h2 id="内存泄漏分析流程"><a href="#内存泄漏分析流程" class="headerlink" title="内存泄漏分析流程"></a>内存泄漏分析流程</h2><p>一般采用以下步骤分析：</p><ul><li>把 Java 应用程序使用的 heap dump 下来</li><li>使用 Java heap 分析工具，找出内存占用超过预期（一般是因为数量太多）的嫌疑对象</li><li>必要时，需要分析嫌疑对象和其他对象的引用关系</li><li>查看程序的源代码，找出嫌疑对象数量过多的原因</li></ul><h2 id="第三方调优工具"><a href="#第三方调优工具" class="headerlink" title="第三方调优工具"></a>第三方调优工具</h2><h3 id="MAT"><a href="#MAT" class="headerlink" title="MAT"></a>MAT</h3><p>MAT(Memory Analyzer Tool)，一个基于Eclipse的内存分析工具，是一个快速、功能丰富的Java heap分析工具，它可以帮助我们查找内存泄漏和减少内存消耗。使用内存分析工具从众多的对象中进行分析，快速的计算出在内存中对象的占用大小，看看是谁阻止了垃圾收集器的回收工作，并可以通过报表直观的查看到可能造成这种结果的对象。</p><p>通常内存泄露分析被认为是一件很有难度的工作，一般由团队中的资深人士进行。不过要介绍的 MAT（Eclipse Memory Analyzer）被认为是一个“傻瓜式“的堆转储文件分析工具，你只需要轻轻点击一下鼠标就可以生成一个专业的分析报告。和其他内存泄露分析工具相比，MAT 的使用非常容易，基本可以实现一键到位，即使是新手也能够很快上手使用。</p><p>MAT以eclipse 插件的形式来安装，具体的安装过程就不在描述了，可以利用visualvm或者是 jmap命令生产堆文件，导入eclipse mat中生成分析报告。</p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol><li><a href="https://www.cnblogs.com/z-sm/p/6745375.html" target="_blank" rel="noopener">JDK工具（查看JVM参数、内存使用情况及分析等）</a></li><li><a href="https://blog.csdn.net/qq_27376871/article/details/78492908" target="_blank" rel="noopener">分析和解决JAVA 内存泄露的实战例子</a></li><li><a href="https://help.eclipse.org/2018-12/index.jsp?topic=/org.eclipse.mat.ui.help/welcome.html" target="_blank" rel="noopener">Memory Analyzer Docs</a></li><li><a href="https://bijian1013.iteye.com/blog/2221240" target="_blank" rel="noopener">java程序性能分析之thread dump和heap dump</a></li><li><a href="http://howiefh.github.io/2015/04/09/jvm-note-3/" target="_blank" rel="noopener">深入理解Java虚拟机笔记三（JVM性能监控与故障处理工具）</a></li><li><a href="https://www.cnblogs.com/ityouknow/p/6437037.html" target="_blank" rel="noopener">jvm系列(七):jvm调优-工具篇</a></li><li><a href="https://www.cnblogs.com/lizhonghua34/p/7307139.html" target="_blank" rel="noopener">jstat 命令使用详解</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> Java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Java </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Java JVM</title>
      <link href="/2018/10/28/17-java-jvm/"/>
      <url>/2018/10/28/17-java-jvm/</url>
      
        <content type="html"><![CDATA[<h2 id="主要内容"><a href="#主要内容" class="headerlink" title="主要内容"></a>主要内容</h2><p>jvm体系总体分四大块：</p><ul><li>类的加载机制</li><li>jvm内存结构</li><li>GC算法 垃圾回收</li><li>GC分析 命令调优</li></ul><h2 id="JVM-简介"><a href="#JVM-简介" class="headerlink" title="JVM 简介"></a>JVM 简介</h2><p>JVM（Java虚拟机）是一个抽象的计算模型。就如同一台真实的机器，它有自己的指令集和执行引擎，可以在运行时操控内存区域。目的是为构建在其上运行的应用程序提供一个运行环境。JVM可以解读指令代码并与底层进行交互：包括操作系统平台和执行指令并管理资源的硬件体系结构。</p><h2 id="JVM-结构"><a href="#JVM-结构" class="headerlink" title="JVM 结构"></a>JVM 结构</h2><h3 id="Java-程序执行过程"><a href="#Java-程序执行过程" class="headerlink" title="Java 程序执行过程"></a>Java 程序执行过程</h3><p><img src="http://incdn1.b0.upaiyun.com/2017/07/818ee258f575128118ce05bdcf0cec87.jpg" alt="JVM"></p><h3 id="JVM-内存模型"><a href="#JVM-内存模型" class="headerlink" title="JVM 内存模型"></a>JVM 内存模型</h3><p><img src="https://note.youdao.com/yws/api/personal/file/WEB8e38c5ffd7e83145a36a32d69ea15738?method=download&amp;shareKey=9bdec18d0d609e18b43ef2ff85ec97a6" alt="JVM Model"></p><p><img src="https://note.youdao.com/yws/api/personal/file/WEB1e71cf566f75c01183500cdfe429b786?method=download&amp;shareKey=7ea95443ebe478873357ca6a306d86cf" alt="JVM Model cn"></p><p>Java 内存结构 (HotSpot 虚拟机)<br><img src="https://note.youdao.com/yws/api/personal/file/WEB1a31a18d3c9af69015ddbaa7ff3fb266?method=download&amp;shareKey=253057b12e35605167c5d3ac295bfbd9" alt=""></p><p>JVM内存结构主要有三大块：堆内存、方法区和栈。堆内存是JVM中最大的一块由年轻代和老年代组成，而年轻代内存又被分成三部分，Eden空间、From Survivor空间、To Survivor空间,默认情况下年轻代按照8:1:1的比例来分配；</p><p>方法区存储类信息、常量、静态变量等数据，是线程共享的区域，为与Java堆区分，方法区还有一个别名Non-Heap(非堆)；栈又分为java虚拟机栈和本地方法栈主要用于方法的执行。</p><p>JVM 控制参数<br><img src="https://note.youdao.com/yws/api/personal/file/WEBa2943e7d093f15bc682224fa72095a7f?method=download&amp;shareKey=f2405311f663a0787b7fc8c84d1325a9" alt=""></p><p>控制参数</p><pre><code>-Xms设置堆的最小空间大小。-Xmx设置堆的最大空间大小。-XX:NewSize设置新生代最小空间大小。-XX:MaxNewSize设置新生代最大空间大小。-XX:PermSize设置永久代最小空间大小。-XX:MaxPermSize设置永久代最大空间大小。-Xss设置每个线程的堆栈大小。</code></pre><p>没有直接设置老年代的参数，但是可以设置堆空间大小和新生代空间大小两个参数来间接控制。</p><p>老年代空间大小=堆空间大小-年轻代大空间大小</p><p>Java 中的堆是 JVM 所管理的最大的一块内存空间，主要用于存放各种类的实例对象，如下图所示：<br><img src="https://note.youdao.com/yws/api/personal/file/WEB8b40d5fa6939c1761a303d5b881b0e2d?method=download&amp;shareKey=424b0e63e16887369ee3ba626bd372d9" alt=""></p><p>在 Java 中，堆被划分成两个不同的区域：新生代 ( Young )、老年代 ( Old)。新生代 ( Young ) 又被划分为三个区域：Eden、S0、S1。 这样划分的目的是为了使 JVM 能够更好的管理堆内存中的对象，包括内存的分配以及回收。</p><p>Java 中的堆也是 GC 收集垃圾的主要区域。GC 分为两种：Minor GC、Full GC ( 或称为 Major GC )。</p><h2 id="JDK-JRE和JVM"><a href="#JDK-JRE和JVM" class="headerlink" title="JDK, JRE和JVM"></a>JDK, JRE和JVM</h2><p>JDK(Java Development Kit) 是 Java 语言的软件开发工具包（SDK）。JDK 物理存在，是 programming tools、JRE 和 JVM 的一个集合</p><p>JRE（Java Runtime Environment）Java 运行时环境，JRE 物理存在，主要由Java API 和 JVM 组成，提供了用于执行 java 应用程序最低要求的环境。</p><p>JVM(Java Virtual Machine) 是一种软件实现，执行像物理机程序的机器（即电脑）。<br>本来，Java被设计基于从物理机器分离实现WORA（ 写一次，随处运行 ）的虚拟机上运行，虽然这个目标已经几乎被遗忘。<br>JVM 并不是专为 Java 所实现的运行时，实际上只要有其他编程语言的编译器能生成正确 Java bytecode 文件，则这个语言也能实现在JVM上运行。</p><p><img src="https://note.youdao.com/yws/api/personal/file/WEBa7f2977701f58df053fe8f5f9dbefcf5?method=download&amp;shareKey=0a21d7d21db9129efdde20be1199d55a" alt="jdk"></p><h2 id="在JVM上执行Java程序"><a href="#在JVM上执行Java程序" class="headerlink" title="在JVM上执行Java程序"></a>在JVM上执行Java程序</h2><p>每一个在JRE上运行的Java程序都会创建一个JVM实例。编译后的Java类文件和其他被依赖的类文件会被加载到运行环境中。这一步由类加载器协助完成。<br><img src="https://ws1.sinaimg.cn/large/006tKfTcly1g0g7nm2umvj30bz0b774j.jpg" alt="类加载模块和其功能"></p><p>JVM和系统调用之间的关系</p><p><img src="https://ws1.sinaimg.cn/large/006tKfTcly1g0g7oc7r4fj30gz0ga77e.jpg" alt="JVM和系统调用之间的关系"></p><h4 id="Java堆-Heap"><a href="#Java堆-Heap" class="headerlink" title="Java堆 (Heap)"></a>Java堆 (Heap)</h4><p>对于大多数应用来说，Java堆（Java Heap）是Java虚拟机所管理的内存中最大的一块。Java堆是被所有线程共享的一块内存区域，在虚拟机启动时创建。此内存区域的唯一目的就是存放对象实例，几乎所有的对象实例都在这里分配内存。</p><p>Java堆是垃圾收集器管理的主要区域，因此很多时候也被称做“GC堆”。如果从内存回收的角度看，由于现在收集器基本都是采用的分代收集算法，所以Java堆中还可以细分为：新生代和老年代；再细致一点的有Eden空间、From Survivor空间、To Survivor空间等。</p><p>根据Java虚拟机规范的规定，Java堆可以处于物理上不连续的内存空间中，只要逻辑上是连续的即可，就像我们的磁盘空间一样。在实现时，既可以实现成固定大小的，也可以是可扩展的，不过当前主流的虚拟机都是按照可扩展来实现的（通过-Xmx和-Xms控制）。</p><p>如果在堆中没有内存完成实例分配，并且堆也无法再扩展时，将会抛出OutOfMemoryError异常。</p><h4 id="方法区-Method-Area"><a href="#方法区-Method-Area" class="headerlink" title="方法区 (Method Area)"></a>方法区 (Method Area)</h4><p>方法区（Method Area）与Java堆一样，是各个线程共享的内存区域，它用于存储已被虚拟机加载的类信息、常量、静态变量、即时编译器编译后的代码等数据。虽然Java虚拟机规范把方法区描述为堆的一个逻辑部分，但是它却有一个别名叫做Non-Heap（非堆），目的应该是与Java堆区分开来。</p><p>对于习惯在HotSpot虚拟机上开发和部署程序的开发者来说，很多人愿意把方法区称为“永久代”（Permanent Generation），本质上两者并不等价，仅仅是因为HotSpot虚拟机的设计团队选择把GC分代收集扩展至方法区，或者说使用永久代来实现方法区而已。</p><p>Java虚拟机规范对这个区域的限制非常宽松，除了和Java堆一样不需要连续的内存和可以选择固定大小或者可扩展外，还可以选择不实现垃圾收集。相对而言，垃圾收集行为在这个区域是比较少出现的，但并非数据进入了方法区就如永久代的名字一样“永久”存在了。这个区域的内存回收目标主要是针对常量池的回收和对类型的卸载，一般来说这个区域的回收“成绩”比较难以令人满意，尤其是类型的卸载，条件相当苛刻，但是这部分区域的回收确实是有必要的。</p><p>根据Java虚拟机规范的规定，当方法区无法满足内存分配需求时，将抛出OutOfMemoryError异常。</p><h4 id="程序计数器-Program-Counter-Register"><a href="#程序计数器-Program-Counter-Register" class="headerlink" title="程序计数器 (Program Counter Register)"></a>程序计数器 (Program Counter Register)</h4><p>程序计数器（Program Counter Register）是一块较小的内存空间，它的作用可以看做是当前线程所执行的字节码的行号指示器。在虚拟机的概念模型里（仅是概念模型，各种虚拟机可能会通过一些更高效的方式去实现），字节码解释器工作时就是通过改变这个计数器的值来选取下一条需要执行的字节码指令，分支、循环、跳转、异常处理、线程恢复等基础功能都需要依赖这个计数器来完成。</p><p>由于Java虚拟机的多线程是通过线程轮流切换并分配处理器执行时间的方式来实现的，在任何一个确定的时刻，一个处理器（对于多核处理器来说是一个内核）只会执行一条线程中的指令。因此，为了线程切换后能恢复到正确的执行位置，每条线程都需要有一个独立的程序计数器，各条线程之间的计数器互不影响，独立存储，我们称这类内存区域为“线程私有”的内存。</p><p>如果线程正在执行的是一个Java方法，这个计数器记录的是正在执行的虚拟机字节码指令的地址；如果正在执行的是Natvie方法，这个计数器值则为空（Undefined）。</p><p>此内存区域是唯一一个在Java虚拟机规范中没有规定任何OutOfMemoryError情况的区域。</p><h4 id="JVM栈-JVM-Stacks"><a href="#JVM栈-JVM-Stacks" class="headerlink" title="JVM栈 (JVM Stacks)"></a>JVM栈 (JVM Stacks)</h4><p>与程序计数器一样，Java虚拟机栈（Java Virtual Machine Stacks）也是线程私有的，它的生命周期与线程相同。虚拟机栈描述的是Java方法执行的内存模型：每个方法被执行的时候都会同时创建一个栈帧（Stack Frame）用于存储局部变量表、操作栈、动态链接、方法出口等信息。每一个方法被调用直至执行完成的过程，就对应着一个栈帧在虚拟机栈中从入栈到出栈的过程。</p><p>局部变量表存放了编译期可知的各种基本数据类型（boolean、byte、char、short、int、float、long、double）、对象引用（reference类型，它不等同于对象本身，根据不同的虚拟机实现，它可能是一个指向对象起始地址的引用指针，也可能指向一个代表对象的句柄或者其他与此对象相关的位置）和returnAddress类型（指向了一条字节码指令的地址）。</p><p>其中64位长度的long和double类型的数据会占用2个局部变量空间（Slot），其余的数据类型只占用1个。局部变量表所需的内存空间在编译期间完成分配，当进入一个方法时，这个方法需要在帧中分配多大的局部变量空间是完全确定的，在方法运行期间不会改变局部变量表的大小。</p><p>在Java虚拟机规范中，对这个区域规定了两种异常状况：如果线程请求的栈深度大于虚拟机所允许的深度，将抛出StackOverflowError异常；如果虚拟机栈可以动态扩展（当前大部分的Java虚拟机都可动态扩展，只不过Java虚拟机规范中也允许固定长度的虚拟机栈），当扩展时无法申请到足够的内存时会抛出OutOfMemoryError异常。</p><h4 id="本地方法栈-Native-Method-Stacks"><a href="#本地方法栈-Native-Method-Stacks" class="headerlink" title="本地方法栈 (Native Method Stacks)"></a>本地方法栈 (Native Method Stacks)</h4><p>本地方法栈（Native Method Stacks）与虚拟机栈所发挥的作用是非常相似的，其区别不过是虚拟机栈为虚拟机执行Java方法（也就是字节码）服务，而本地方法栈则是为虚拟机使用到的Native方法服务。虚拟机规范中对本地方法栈中的方法使用的语言、使用方式与数据结构并没有强制规定，因此具体的虚拟机可以自由实现它。甚至有的虚拟机（譬如Sun HotSpot虚拟机）直接就把本地方法栈和虚拟机栈合二为一。与虚拟机栈一样，本地方法栈区域也会抛出StackOverflowError和OutOfMemoryError异常。</p><p>java 代码执行过程</p><p><img src="http://upload-images.jianshu.io/upload_images/634730-089a64921220b40d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="java 代码执行过程"></p><h3 id="Class-Loader"><a href="#Class-Loader" class="headerlink" title="Class Loader"></a>Class Loader</h3><p>类加载器负责加载程序中的类型（类和接口），并赋予唯一的名字。</p><p>JDK 默认提供了三种 ClassLoader<br><img src="http://upload-images.jianshu.io/upload_images/634730-ffd9553bc18ea213.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="JDK三种 ClassLoader"></p><ol><li>Bootstrp loader 是在Java虚拟机启动后初始化的。</li><li>Bootstrp loader 负责加载 ExtClassLoader,并且将 ExtClassLoade r的父加载器设置为 Bootstrp loader。</li><li>Bootstrp loader 加载完 ExtClassLoader 后，就会加载 AppClassLoader,并且将 AppClassLoader 的父加载器指定为 ExtClassLoader。</li></ol><p><strong>双亲委托模型</strong></p><p>Java中ClassLoader的加载采用了双亲委托机制，采用双亲委托机制加载类的时候采用如下的几个步骤：</p><ol><li>当前ClassLoader首先从自己已经加载的类中查询是否此类已经加载，如果已经加载则直接返回原来已经加载的类。</li><li>当前classLoader的缓存中没有找到被加载的类的时候，委托父类加载器去加载，父类加载器采用同样的策略，首先查看自己的缓存，然后委托父类的父类去加载，一直到bootstrp ClassLoader.</li><li>当所有的父类加载器都没有加载的时候，再由当前的类加载器加载，并将其放入它自己的缓存中，以便下次有加载请求的时候直接返回。</li></ol><p><strong>过程</strong><br><img src="http://upload-images.jianshu.io/upload_images/634730-34263b8587735b2d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="类加载过程"></p><h3 id="Execution-Engine"><a href="#Execution-Engine" class="headerlink" title="Execution Engine"></a>Execution Engine</h3><p>通过类装载器装载的，被分配到JVM的运行时数据区的字节码会被执行引擎执行。执行引擎以指令为单位读取 Java 字节码。它就像一个 CPU 一样，一条一条地执行机器指令。每个字节码指令都由一个1字节的操作码和附加的操作数组成。执行引擎取得一个操作码，然后根据操作数来执行任务，完成后就继续执行下一条操作码。<br>不过 Java 字节码是用一种人类可以读懂的语言编写的，而不是用机器可以直接执行的语言。因此，==执行引擎必须把字节码转换成可以直接被 JVM 执行的语言==。字节码可以通过以下两种方式转换成合适的语言。</p><p><strong>解释器</strong>：一条一条地读取，解释并且执行字节码指令。因为它一条一条地解释和执行指令，所以它可以很快地解释字节码，但是执行起来会比较慢。这是解释执行的语言的一个缺点。字节码这种“语言”基本来说是解释执行的。</p><p><strong>即时(Just-In-Time)编译器</strong>：即时编译器被引入用来弥补解释器的缺点。执行引擎首先按照解释执行的方式来执行，然后在合适的时候，即时编译器把整段字节码编译成本地代码。然后，执行引擎就没有必要再去解释执行方法了，它可以直接通过本地代码去执行它。执行本地代码比一条一条进行解释执行的速度快很多。编译后的代码可以执行的很快，因为本地代码是保存在缓存里的。</p><p>Java 字节码是解释执行的，但是没有直接在 JVM 宿主执行原生代码快。为了提高性能，Oracle Hotspot 虚拟机会找到执行最频繁的字节码片段并把它们编译成原生机器码。编译出的原生机器码被存储在非堆内存的代码缓存中。通过这种方法（JIT），Hotspot 虚拟机将权衡下面两种时间消耗：将字节码编译成本地代码需要的额外时间和解释执行字节码消耗更多的时间。</p><p><img src="http://upload-images.jianshu.io/upload_images/634730-05d712735a355e9d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="java_compiler_and_jit_compiler"></p><h3 id="运行时数据区"><a href="#运行时数据区" class="headerlink" title="运行时数据区"></a>运行时数据区</h3><p><img src="http://upload-images.jianshu.io/upload_images/634730-0ff7a46c4ca21d28.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="runtime-data-access-configuration"></p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ol><li><a href="https://docs.oracle.com/en/java/index.html" target="_blank" rel="noopener">Java Documentation</a></li><li><a href="http://www.cnblogs.com/ityouknow/p/5610232.html" target="_blank" rel="noopener">JVM 内存结构</a></li><li><a href="http://www.importnew.com/23658.html" target="_blank" rel="noopener">清蒸JVM(一)</a></li><li><a href="http://www.importnew.com/25438.html" target="_blank" rel="noopener">Java虚拟机（JVM）概述</a></li><li><a href="http://www.importnew.com/17770.html" target="_blank" rel="noopener">JVM内幕：Java虚拟机详解</a></li><li><a href="http://www.cnblogs.com/ityouknow/p/5603287.html" target="_blank" rel="noopener">jvm系列(一): Java 类的加载机制</a></li><li><a href="https://www.cnblogs.com/ityouknow/p/5610232.html" target="_blank" rel="noopener">jvm系列(二):JVM内存结构</a></li><li><a href="http://www.cnblogs.com/ityouknow/p/5614961.html" target="_blank" rel="noopener">GC 算法</a></li><li><a href="https://www.jianshu.com/p/97c52ee6f07b" target="_blank" rel="noopener">JVM 内存结构浅析</a></li><li><a href="https://www.cnblogs.com/smyhvae/p/4748392.html" target="_blank" rel="noopener">Java虚拟机详解02—-JVM内存结构</a></li><li><a href="https://docs.oracle.com/javase/specs/jvms/se8/html/index.html" target="_blank" rel="noopener">The Java® Virtual Machine Specification Java SE 8 Edition</a></li><li><a href="https://my.oschina.net/wangkang80/blog/1554684" target="_blank" rel="noopener">JVM 运行时内存结构</a></li><li><a href="http://www.importnew.com/30756.html" target="_blank" rel="noopener">《深入理解 Java 虚拟机 》学习笔记</a></li><li><a href="https://www.guru99.com/java-virtual-machine-jvm.html" target="_blank" rel="noopener">Java Virtual Machine (JVM) &amp; its Architecture</a></li><li><a href="http://howiefh.github.io/2015/04/07/jvm-note-1/" target="_blank" rel="noopener">深入理解Java虚拟机笔记一（Java内存区域与内存溢出异常）</a></li><li><a href="https://mp.weixin.qq.com/s/SMXeSMllqmeV8qdreHn8Jw" target="_blank" rel="noopener">面试系列-Jvm看这篇就够了</a></li><li><a href="https://www.cnblogs.com/ityouknow/p/6482464.html" target="_blank" rel="noopener">jvm系列(八): jvm知识点总览</a></li><li><a href="https://www.guru99.com/java-virtual-machine-jvm.html" target="_blank" rel="noopener">Java Virtual Machine (JVM) &amp; its Architecture</a></li><li><a href="https://javapapers.com/java/java-garbage-collection-introduction/" target="_blank" rel="noopener">Java Garbage Collection Introduction</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> Java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Java </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Java JDBC</title>
      <link href="/2018/10/27/16-java-jdbc/"/>
      <url>/2018/10/27/16-java-jdbc/</url>
      
        <content type="html"><![CDATA[<h2 id="JDBC-简介"><a href="#JDBC-简介" class="headerlink" title="JDBC 简介"></a>JDBC 简介</h2><ul><li><p>JDBC(==Java Database Connectivity==)是一个独立于特定数据库管理系统、通用的SQL数据库存取和操作的公共接口（一组API），定义了用来访问数据库的标准Java类库，使用这个类库可以以一种标准的方法、方便地访问数据库资源。</p></li><li><p>JDBC为访问不同的数据库提供了一种 统一的途径，为开发者屏蔽了一些细节问题。</p></li><li><p>JDBC的目标是使Java程序员 使用JDBC可以连接任何提供了JDBC驱动程序的数据库系统，这样就使得程序员无需对特定的数据库系统的特点有过多的了解，从而大大简化和加快了开发过程。</p></li></ul><p><img src="http://upload-images.jianshu.io/upload_images/1802116-137a9b744ae6eee9.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240&amp;_=6023098" alt="image"></p><ul><li><p>JDBC接口（API）包括两个层次：<br>面向应用的API：Java API，抽象接口，供应用程序开发人员使用（连接数据库，执行SQL语句，获得结果）。<br>面向数据库的API：Java Driver API，供开发商开发数据库驱动程序用。</p></li><li><p>JDBC驱动程序：各个数据库厂商根据JDBC的规范制作的 JDBC 实现类的类库<br>JDBC驱动程序总共有四种类型：</p><ul><li>第一类：JDBC-ODBC桥。(Open Database Connectivity, ODBC)</li><li>第二类：部分本地API部分Java的驱动程序。</li><li>第三类：JDBC网络纯Java驱动程序。</li><li>第四类：本地协议的纯 Java 驱动程序。</li></ul></li></ul><h2 id="JDBC-基本使用"><a href="#JDBC-基本使用" class="headerlink" title="JDBC 基本使用"></a>JDBC 基本使用</h2><pre><code>// 相关类DriverManagerConnectionStatementResultSetSQLException</code></pre><p>JDBC 涉及以下6个步骤：</p><ul><li>导入包：需要包含包含数据库编程所需的JDBC类的包。 大多数情况下，使用import java.sql.*就足够了。</li><li>注册JDBC驱动程序：需要初始化驱动程序，以便可以打开与数据库的通信通道。（现在已不需要，自动注册）</li><li>打开一个连接：需要使用DriverManager.getConnection()方法创建一个Connection对象，它表示与数据库的物理连接。</li><li>执行查询：需要使用类型为Statement的对象来构建和提交SQL语句到数据库。</li><li>从结果集中提取数据：需要使用相应的ResultSet.getXXX()方法从结果集中检索数据。</li><li>清理环境：需要明确地关闭所有数据库资源，而不依赖于JVM的垃圾收集。</li></ul><h2 id="JDBC-数据类型"><a href="#JDBC-数据类型" class="headerlink" title="JDBC 数据类型"></a>JDBC 数据类型</h2><h2 id="JDBC-驱动程序类型"><a href="#JDBC-驱动程序类型" class="headerlink" title="JDBC 驱动程序类型"></a>JDBC 驱动程序类型</h2><h3 id="JDBC-ODBC桥驱动程序"><a href="#JDBC-ODBC桥驱动程序" class="headerlink" title="JDBC-ODBC桥驱动程序"></a>JDBC-ODBC桥驱动程序</h3><p><img src="http://www.yiibai.com/uploads/images/201705/3005/696110505_18575.jpg" alt="JDBC-ODBC桥驱动程序"></p><p>在类型1驱动程序中，JDBC桥接器用于访问安装在每台客户机上的ODBC驱动程序。 使用ODBC需要在系统上配置表示目标数据库的数据源名称(DSN)。</p><p>当Java第一次出现时，这是一个驱动程序，因为大多数数据库仅支持ODBC访问，但现在这种类型的驱动程序仅推荐用于实验性使用或没有其他替代方案时使用。</p><p>JDK 1.2附带的JDBC-ODBC桥接是这种驱动程序的一个很好的例子。</p><h3 id="JDBC-本地API"><a href="#JDBC-本地API" class="headerlink" title="JDBC 本地API"></a>JDBC 本地API</h3><p><img src="http://www.yiibai.com/uploads/images/201705/3005/881110514_49239.jpg" alt="JDBC 本地API"></p><p>在类型2驱动程序中，JDBC API调用将转换为本地C/C++ API调用，这是数据库唯一的。 这些驱动程序通常由数据库供应商提供，并以与JDBC-ODBC桥接相同的方式使用。 必须在每个客户机上安装供应商特定的驱动程序。</p><p>如果要更改数据库，则必须更改原生API，因为它特定于数据库，并且现在大部分已经过时，但是使用类型2驱动程序实现了一些扩展功能的开发，它消除了ODBC的开销。</p><p>Oracle调用接口(OCI)驱动程序是类型2驱动程序的示例。</p><h3 id="JDBC-Net-纯Java"><a href="#JDBC-Net-纯Java" class="headerlink" title="JDBC-Net 纯Java"></a>JDBC-Net 纯Java</h3><p><img src="http://www.yiibai.com/uploads/images/201705/3005/700110516_88885.jpg" alt="JDBC-Net 纯Java"></p><p>在类型3驱动程序中，使用三层方法访问数据库。 JDBC客户端使用标准网络套接字与中间件应用程序服务器进行通信。 套接字信息随后由中间件应用服务器转换成DBMS所需的调用格式，并转发到数据库服务器。</p><p>这种驱动程序是非常灵活的，因为它不需要在客户端上安装代码，一个驱动程序实际上可以提供多个数据库的访问。</p><p>可以将应用程序服务器视为JDBC“代理”，它会调用客户端应用程序。 因此，我们需要了解应用程序服务器的配置，才能有效地使用此驱动程序类型。</p><p>应用程序服务器可能会使用类型1,2或4驱动程序与数据库通信，了解细微差别对理解JDBC是有帮助的。</p><h3 id="100％纯Java"><a href="#100％纯Java" class="headerlink" title="100％纯Java"></a>100％纯Java</h3><p><img src="http://www.yiibai.com/uploads/images/201705/3005/770110529_65984.jpg" alt="100％纯Java"></p><p>在类型4驱动程序中，基于纯Java的驱动程序通过套接字连接与供应商的数据库直接通信。 这是数据库可用的最高性能驱动程序，通常由供应商自己提供。</p><p>这种驱动是非常灵活的，不需要在客户端或服务器上安装特殊的软件。 此外，这些驱动程序可以动态下载。</p><p>MySQL Connector/J驱动程序是类型4驱动程序。 由于其网络协议的专有性质，数据库供应商通常提供类型4驱动程序。</p><h3 id="应该使用哪个驱动程序"><a href="#应该使用哪个驱动程序" class="headerlink" title="应该使用哪个驱动程序"></a>应该使用哪个驱动程序</h3><p>如果您正在访问一种类型的数据库，例如Oracle，Sybase或IBM DB2，则首选驱动程序类型为类型4。</p><p>如果Java应用程序同时访问多种类型的数据库，则类型3是首选驱动程序。</p><p>类型2驱动程序在数据库不可用的类型3或类型4驱动程序的情况下使用。</p><p>类型1驱动程序不被视为部署级驱动程序，通常仅用于开发和测试目的。</p><h2 id="JDBC-Statement"><a href="#JDBC-Statement" class="headerlink" title="JDBC Statement"></a>JDBC Statement</h2><p>当获得了与数据库的连接后，就可以与数据库进行交互了。 JDBC Statement，CallableStatement和PreparedStatement接口定义了可用于发送SQL或PL/SQL命令，并从数据库接收数据的方法和属性。</p><p>它们还定义了有助于在Java和SQL数据类型的数据类型差异转换的方法。<br>下表提供了每个接口定义，以及使用这些接口的目的的总结。</p><table><thead><tr><th>接口</th><th>推荐使用</th></tr></thead><tbody><tr><td>Statement</td><td>用于对数据库进行通用访问，在运行时使用静态SQL语句时很有用。 Statement接口不能接受参数。</td></tr><tr><td>PreparedStatement</td><td>当计划要多次使用SQL语句时使用。PreparedStatement接口在运行时接受输入参数。</td></tr><tr><td>CallableStatement</td><td>当想要访问数据库存储过程时使用。CallableStatement接口也可以接受运行时输入参数。</td></tr></tbody></table><h2 id="示例代码"><a href="#示例代码" class="headerlink" title="示例代码"></a>示例代码</h2><pre class=" language-java"><code class="language-java"><span class="token keyword">package</span> com<span class="token punctuation">.</span>baidu<span class="token punctuation">.</span>jiaotong<span class="token punctuation">.</span>stone<span class="token punctuation">;</span><span class="token keyword">import</span> java<span class="token punctuation">.</span>sql<span class="token punctuation">.</span>Connection<span class="token punctuation">;</span><span class="token keyword">import</span> java<span class="token punctuation">.</span>sql<span class="token punctuation">.</span>DriverManager<span class="token punctuation">;</span><span class="token keyword">import</span> java<span class="token punctuation">.</span>sql<span class="token punctuation">.</span>ResultSet<span class="token punctuation">;</span><span class="token keyword">import</span> java<span class="token punctuation">.</span>sql<span class="token punctuation">.</span>SQLException<span class="token punctuation">;</span><span class="token keyword">import</span> java<span class="token punctuation">.</span>sql<span class="token punctuation">.</span>Statement<span class="token punctuation">;</span><span class="token keyword">public</span> <span class="token keyword">class</span> <span class="token class-name">TestJDBC1</span> <span class="token punctuation">{</span>    <span class="token comment" spellcheck="true">// JDBC driver name and database URL</span>    <span class="token comment" spellcheck="true">// static final String JDBC_DRIVER = "com.mysql.jdbc.Driver";</span>    <span class="token keyword">static</span> <span class="token keyword">final</span> String JDBC_DRIVER <span class="token operator">=</span> <span class="token string">"com.mysql.cj.jdbc.Driver"</span><span class="token punctuation">;</span>    <span class="token keyword">static</span> <span class="token keyword">final</span> String DB_URL <span class="token operator">=</span> <span class="token string">"jdbc:mysql://localhost/Emp"</span><span class="token punctuation">;</span>    <span class="token comment" spellcheck="true">// Database credentials</span>    <span class="token keyword">static</span> <span class="token keyword">final</span> String USER <span class="token operator">=</span> <span class="token string">"root"</span><span class="token punctuation">;</span>    <span class="token keyword">static</span> <span class="token keyword">final</span> String PASS <span class="token operator">=</span> <span class="token string">"123456"</span><span class="token punctuation">;</span>    <span class="token keyword">public</span> <span class="token keyword">static</span> <span class="token keyword">void</span> <span class="token function">main</span><span class="token punctuation">(</span>String<span class="token punctuation">[</span><span class="token punctuation">]</span> args<span class="token punctuation">)</span> <span class="token punctuation">{</span>        Connection conn <span class="token operator">=</span> null<span class="token punctuation">;</span>        Statement stmt <span class="token operator">=</span> null<span class="token punctuation">;</span>        <span class="token keyword">try</span> <span class="token punctuation">{</span>            <span class="token comment" spellcheck="true">// Register JDBC driver</span>            <span class="token comment" spellcheck="true">// Class.forName(JDBC_DRIVER);</span>            <span class="token comment" spellcheck="true">// 1. Get a jdbc connection</span>            System<span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span><span class="token string">"Connecting to database ..."</span><span class="token punctuation">)</span><span class="token punctuation">;</span>            conn <span class="token operator">=</span> DriverManager<span class="token punctuation">.</span><span class="token function">getConnection</span><span class="token punctuation">(</span>DB_URL<span class="token punctuation">,</span> USER<span class="token punctuation">,</span> PASS<span class="token punctuation">)</span><span class="token punctuation">;</span>            <span class="token comment" spellcheck="true">// 2.1 Declare a sql  </span>            <span class="token comment" spellcheck="true">// 2.2 Create a statement</span>            <span class="token comment" spellcheck="true">// 2.3 Execute a query and get the ResultSet</span>            System<span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span><span class="token string">"Create statement ..."</span><span class="token punctuation">)</span><span class="token punctuation">;</span>            String sql <span class="token operator">=</span> <span class="token string">"SELECT id, age, first, last FROM EMPLOYEES"</span><span class="token punctuation">;</span>            stmt <span class="token operator">=</span> conn<span class="token punctuation">.</span><span class="token function">createStatement</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>            ResultSet rs <span class="token operator">=</span> stmt<span class="token punctuation">.</span><span class="token function">executeQuery</span><span class="token punctuation">(</span>sql<span class="token punctuation">)</span><span class="token punctuation">;</span>            <span class="token comment" spellcheck="true">// 4.Extract data from ResultSet</span>            <span class="token keyword">while</span> <span class="token punctuation">(</span>rs<span class="token punctuation">.</span><span class="token function">next</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token punctuation">{</span>                <span class="token comment" spellcheck="true">// Retrieve by column name</span>                <span class="token keyword">int</span> id <span class="token operator">=</span> rs<span class="token punctuation">.</span><span class="token function">getInt</span><span class="token punctuation">(</span><span class="token string">"id"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>                <span class="token keyword">int</span> age <span class="token operator">=</span> rs<span class="token punctuation">.</span><span class="token function">getInt</span><span class="token punctuation">(</span><span class="token string">"age"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>                String first <span class="token operator">=</span> rs<span class="token punctuation">.</span><span class="token function">getString</span><span class="token punctuation">(</span><span class="token string">"first"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>                String last <span class="token operator">=</span> rs<span class="token punctuation">.</span><span class="token function">getString</span><span class="token punctuation">(</span><span class="token string">"last"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>                <span class="token comment" spellcheck="true">// Display values</span>                System<span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">print</span><span class="token punctuation">(</span><span class="token string">"ID: "</span> <span class="token operator">+</span> id<span class="token punctuation">)</span><span class="token punctuation">;</span>                System<span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">print</span><span class="token punctuation">(</span><span class="token string">", Age: "</span> <span class="token operator">+</span> age<span class="token punctuation">)</span><span class="token punctuation">;</span>                System<span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">print</span><span class="token punctuation">(</span><span class="token string">", First: "</span> <span class="token operator">+</span> first<span class="token punctuation">)</span><span class="token punctuation">;</span>                System<span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span><span class="token string">", Last: "</span> <span class="token operator">+</span> last<span class="token punctuation">)</span><span class="token punctuation">;</span>            <span class="token punctuation">}</span>            <span class="token comment" spellcheck="true">// 5. Clean-up environment</span>            rs<span class="token punctuation">.</span><span class="token function">close</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>            stmt<span class="token punctuation">.</span><span class="token function">close</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>            conn<span class="token punctuation">.</span><span class="token function">close</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token punctuation">}</span> <span class="token keyword">catch</span> <span class="token punctuation">(</span><span class="token class-name">SQLException</span> e<span class="token punctuation">)</span> <span class="token punctuation">{</span>            e<span class="token punctuation">.</span><span class="token function">printStackTrace</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>        <span class="token punctuation">}</span> <span class="token keyword">finally</span> <span class="token punctuation">{</span>            <span class="token comment" spellcheck="true">// 6. Finally block used to close resources</span>            <span class="token keyword">if</span> <span class="token punctuation">(</span>stmt <span class="token operator">!=</span> null<span class="token punctuation">)</span> <span class="token punctuation">{</span>                <span class="token keyword">try</span> <span class="token punctuation">{</span>                    stmt<span class="token punctuation">.</span><span class="token function">close</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>                <span class="token punctuation">}</span> <span class="token keyword">catch</span> <span class="token punctuation">(</span><span class="token class-name">SQLException</span> e<span class="token punctuation">)</span> <span class="token punctuation">{</span>                    e<span class="token punctuation">.</span><span class="token function">printStackTrace</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>                <span class="token punctuation">}</span>            <span class="token punctuation">}</span>            <span class="token keyword">if</span> <span class="token punctuation">(</span>conn <span class="token operator">!=</span> null<span class="token punctuation">)</span> <span class="token punctuation">{</span>                <span class="token keyword">try</span> <span class="token punctuation">{</span>                    conn<span class="token punctuation">.</span><span class="token function">close</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>                <span class="token punctuation">}</span> <span class="token keyword">catch</span> <span class="token punctuation">(</span><span class="token class-name">SQLException</span> e<span class="token punctuation">)</span> <span class="token punctuation">{</span>                    e<span class="token punctuation">.</span><span class="token function">printStackTrace</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>                <span class="token punctuation">}</span>            <span class="token punctuation">}</span>        <span class="token punctuation">}</span>        System<span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span><span class="token string">"Goodbye!"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    <span class="token punctuation">}</span><span class="token punctuation">}</span></code></pre><h2 id="JDBC-处理流程"><a href="#JDBC-处理流程" class="headerlink" title="JDBC 处理流程"></a>JDBC 处理流程</h2><p>JDBC处理流程和Spring JDBC 处理流程</p><p>Spring JDBC提供了一套JDBC抽象框架，用于简化JDBC开发</p><p><img src="https://ws3.sinaimg.cn/large/006tKfTcly1g0g7gxnnjdj30kh0d4juv.jpg" alt="JDBC对比"></p><p>Spring JDBC请参见另一篇文章。</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ol><li><a href="http://docs.oracle.com/javase/tutorial/jdbc/basics/index.html" target="_blank" rel="noopener">Java SE JDBC Tutorial</a></li><li><a href="http://www.cnblogs.com/jianshu/p/6023098.html" target="_blank" rel="noopener">JDBC 简介</a></li><li><a href="http://www.yiibai.com/jdbc/jdbc_quick_guide.html" target="_blank" rel="noopener">JDBC 快速入门</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> Java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Java </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Java 泛型</title>
      <link href="/2018/10/26/15-java-generics/"/>
      <url>/2018/10/26/15-java-generics/</url>
      
        <content type="html"><![CDATA[<h2 id="lt-extends-E-gt"><a href="#lt-extends-E-gt" class="headerlink" title="&lt;? extends E&gt;"></a>&lt;? extends E&gt;</h2><pre><code>&lt;? extends E&gt; 是 Upper Bound（上限）的通配符，用来限制元素的类型的上限&lt;?&gt;是&lt;? extends Object&gt;的简写</code></pre><h2 id="lt-super-E-gt"><a href="#lt-super-E-gt" class="headerlink" title="&lt;? super E&gt;"></a>&lt;? super E&gt;</h2><pre><code>&lt;? super E&gt; 是 Lower Bound（下限） 的通配符 ，用来限制元素的类型下限。</code></pre><h2 id="PECS法则"><a href="#PECS法则" class="headerlink" title="PECS法则"></a>PECS法则</h2><p>PECS法则：生产者（Producer）使用extends，消费者（Consumer）使用super</p><ol><li>生产者<br> 如果你需要一个提供E类型元素的集合，使用泛型通配符&lt;? extends E&gt;。它好比一个生产者，可以提供数据。</li><li>消费者<br> 如果你需要一个只能装入E类型元素的集合，使用泛型通配符&lt;? super E&gt;。它好比一个消费者，可以消费你提供的数据。</li><li>既是生产者也是消费者<br> 既要存储又要读取，那就别使用泛型通配符。</li></ol><p>为什么要引入泛型通配符？一句话：为了保证类型安全。</p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol><li><a href="https://www.cnblogs.com/lwbqqyumidi/p/3837629.html" target="_blank" rel="noopener">Java总结篇系列：Java泛型</a></li><li><a href="https://blog.csdn.net/u014513883/article/details/49820569" target="_blank" rel="noopener">Java泛型中&lt;？ extends E&gt;和&lt;？ super E&gt;的区别</a></li><li><a href="http://www.importnew.com/24029.html" target="_blank" rel="noopener">Java泛型详解</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> Java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Java </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Java 异常处理</title>
      <link href="/2018/10/25/14-java-exception/"/>
      <url>/2018/10/25/14-java-exception/</url>
      
        <content type="html"><![CDATA[<h2 id="异常简介"><a href="#异常简介" class="headerlink" title="异常简介"></a>异常简介</h2><p>异常是程序中的一些错误，但并不是所有的错误都是异常，并且错误有时候是可以避免的。</p><p>Java异常是一个描述在代码段中发生异常的对象，当发生异常情况时，一个代表该异常的对象被创建并且在导致该异常的方法中被抛出，而该方法可以选择自己处理异常或者传递该异常。</p><p>Java异常处理涉及到五个关键字，分别是：==try==、==catch==、==finally==、==throw==、==throws==。</p><h2 id="异常类"><a href="#异常类" class="headerlink" title="异常类"></a>异常类</h2><p><img src="https://note.youdao.com/yws/api/personal/file/WEB848198d077968b4a72f0e21771015895?method=download&amp;shareKey=af2821bd38155aec6d00e04e095fb12d" alt="Java 异常类继承树"><br><img src="https://ws1.sinaimg.cn/large/006tKfTcly1g0g7bdkafrj30go0ah0tv.jpg" alt="Java 异常类层次结构图"><br><img src="https://ws3.sinaimg.cn/large/006tKfTcly1g0g7bycaidj30ls0h40w7.jpg" alt="Java 异常总结图"></p><h3 id="Throwable"><a href="#Throwable" class="headerlink" title="Throwable"></a>Throwable</h3><p>The Throwable class is the superclass of all errors and exceptions in the Java language. Only objects that are instances of this class (or one of its subclasses) are thrown by the Java Virtual Machine or can be thrown by the Java throw statement. Similarly, only this class or one of its subclasses can be the argument type in a catch clause. For the purposes of compile-time checking of exceptions, Throwable and any subclass of Throwable that is not also a subclass of either RuntimeException or Error are regarded as checked exceptions.</p><h3 id="Error"><a href="#Error" class="headerlink" title="Error"></a>Error</h3><p>An Error is a subclass of Throwable that indicates serious problems that a reasonable application should not try to catch. Most such errors are abnormal conditions. The ThreadDeath error, though a “normal” condition, is also a subclass of Error because most applications should not try to catch it.</p><h3 id="Exception"><a href="#Exception" class="headerlink" title="Exception"></a>Exception</h3><p>The class Exception and its subclasses are a form of Throwable that indicates conditions that a reasonable application might want to catch.<br>The class Exception and any subclasses that are not also subclasses of RuntimeException are checked exceptions. Checked exceptions need to be declared in a method or constructor’s throws clause if they can be thrown by the execution of the method or constructor and propagate outside the method or constructor boundary.</p><h3 id="RuntimeException"><a href="#RuntimeException" class="headerlink" title="RuntimeException"></a>RuntimeException</h3><p>RuntimeException is the superclass of those exceptions that can be thrown during the normal operation of the Java Virtual Machine.<br>RuntimeException and its subclasses are unchecked exceptions. Unchecked exceptions do not need to be declared in a method or constructor’s throws clause if they can be thrown by the execution of the method or constructor and propagate outside the method or constructor boundary.</p><p>在 Java 中，所有的异常都有一个共同的祖先 <strong>Throwable</strong>。Throwable 指定代码中可用异常传播机制通过 Java 应用程序传输的任何问题的共性。</p><h2 id="名词解释"><a href="#名词解释" class="headerlink" title="名词解释"></a>名词解释</h2><ul><li>检查性异常: 不处理编译不能通过</li><li>非检查性异常:不处理编译可以通过，如果有抛出直接抛到控制台</li><li>运行时异常: 就是非检查性异常</li><li>非运行时异常: 就是检查性异常</li></ul><h2 id="异常处理机制"><a href="#异常处理机制" class="headerlink" title="异常处理机制"></a>异常处理机制</h2><p>在 Java 应用程序中，异常处理机制为：抛出异常，捕捉异常。</p><h2 id="异常处理原则"><a href="#异常处理原则" class="headerlink" title="异常处理原则"></a>异常处理原则</h2><ul><li>具体明确</li><li>提早抛出</li><li>延迟捕捉</li></ul><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ol><li><a href="https://www.tianmaying.com/tutorial/Java-Exception" target="_blank" rel="noopener">Java入门之异常处理</a></li><li><a href="http://www.cnblogs.com/Qian123/p/5715402.html" target="_blank" rel="noopener">Java异常处理提高篇</a></li><li><a href="http://www.importnew.com/1701.html" target="_blank" rel="noopener">有效处理异常3原则</a></li><li><a href="http://www.importnew.com/14688.html" target="_blank" rel="noopener">深入理解Java异常处理机制</a></li><li><a href="http://www.runoob.com/java/java-exceptions.html" target="_blank" rel="noopener">Java异常处理</a></li><li><a href="http://www.importnew.com/20139.html" target="_blank" rel="noopener">Java异常处理的10个最佳实践</a></li><li><a href="https://my.oschina.net/c5ms/blog/1827907" target="_blank" rel="noopener">如何优雅的处理你的Java异常</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> Java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Java </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Java 注解</title>
      <link href="/2018/10/24/13-java-annotation/"/>
      <url>/2018/10/24/13-java-annotation/</url>
      
        <content type="html"><![CDATA[<h2 id="什么是注解（Annotation）"><a href="#什么是注解（Annotation）" class="headerlink" title="什么是注解（Annotation）"></a>什么是注解（Annotation）</h2><p>Annotation（注解）就是Java提供了一种元程序中的元素关联任何信息和着任何元数据（metadata）的途径和方法。Annotion(注解)是一个接口，程序可以通过反射来获取指定程序元素的Annotion对象，然后通过Annotion对象来获取注解里面的元数据。</p><p>　　Annotation(注解)是JDK5.0及以后版本引入的。它可以用于创建文档，跟踪代码中的依赖性，甚至执行基本编译时检查。从某些方面看，annotation就像修饰符一样被使用，并应用于包、类 型、构造方法、方法、成员变量、参数、本地变量的声明中。这些信息被存储在Annotation的“name=value”结构对中。</p><p>　　Annotation的成员在Annotation类型中以无参数的方法的形式被声明。其方法名和返回值定义了该成员的名字和类型。在此有一个特定的默认语法：允许声明任何Annotation成员的默认值：一个Annotation可以将name=value对作为没有定义默认值的Annotation成员的值，当然也可以使用name=value对来覆盖其它成员默认值。这一点有些近似类的继承特性，父类的构造函数可以作为子类的默认构造函数，但是也可以被子类覆盖。</p><p>　　Annotation能被用来为某个程序元素（类、方法、成员变量等）关联任何的信息。需要注意的是，这里存在着一个基本的规则：Annotation不能影响程序代码的执行，无论增加、删除 Annotation，代码都始终如一的执行。另外，尽管一些annotation通过java的反射api方法在运行时被访问，而java语言解释器在工作时忽略了这些annotation。正是由于java虚拟机忽略了Annotation，导致了annotation类型在代码中是“不起作用”的； 只有通过某种配套的工具才会对annotation类型中的信息进行访问和处理。本文中将涵盖标准的Annotation和meta-annotation类型，陪伴这些annotation类型的工具是java编译器（当然要以某种特殊的方式处理它们）。</p><h3 id="Annotation类型"><a href="#Annotation类型" class="headerlink" title="Annotation类型"></a>Annotation类型</h3><p>Annotation类型定义了Annotation的名字、类型、成员默认值。一个Annotation类型可以说是一个特殊的java接口，它的成员变量是受限制的，而声明Annotation类型时需要使用新语法。当我们通过java反射api访问Annotation时，返回值将是一个实现了该 annotation类型接口的对象，通过访问这个对象我们能方便的访问到其Annotation成员。</p><h3 id="注解的分类"><a href="#注解的分类" class="headerlink" title="注解的分类"></a>注解的分类</h3><p>根据<strong>注解参数的个数</strong>，我们可以将注解分为三类：</p><ol><li>标记注解:一个没有成员定义的Annotation类型被称为标记注解。这种Annotation类型仅使用自身的存在与否来为我们提供信息。比如后面的系统注解@Override;</li><li>单值注解</li><li>完整注解　　</li></ol><p>根据<strong>注解使用方法和用途</strong>，我们可以将Annotation分为三类：</p><ol><li>JDK内置系统注解</li><li>元注解</li><li>自定义注解</li></ol><p>JavaSE中内置三个<strong>标准注解</strong>，定义在java.lang中：</p><ul><li>@Override：用于修饰此方法覆盖了父类的方法;</li><li>@Deprecated：用于修饰已经过时的方法;</li><li>@SuppressWarnnings:用于通知java编译器禁止特定的编译警告。</li></ul><h3 id="元注解"><a href="#元注解" class="headerlink" title="元注解"></a>元注解</h3><p>元注解的作用就是负责注解其他注解。Java5.0定义了4个标准的meta-annotation类型，它们被用来提供对其它 annotation类型作说明。Java5.0定义的元注解：</p><ul><li>@Target</li><li>@Retention</li><li>@Documented</li><li>@Inherited<br>这些类型和它们所支持的类在java.lang.annotation包中可以找到。</li></ul><ol><li><strong>@Target</strong></li></ol><p>@Target说明了Annotation所修饰的对象范围：Annotation可被用于 packages、types（类、接口、枚举、Annotation类型）、类型成员（方法、构造方法、成员变量、枚举值）、方法参数和本地变量（如循环变量、catch参数）。在Annotation类型的声明中使用了target可更加明晰其修饰的目标。</p><p><strong>作用</strong>：用于描述注解的使用范围（即：被描述的注解可以用在什么地方）</p><ol start="2"><li><strong>@Retention</strong></li></ol><p>@Retention定义了该Annotation被保留的时间长短：某些Annotation仅出现在源代码中，而被编译器丢弃；而另一些却被编译在class文件中；编译在class文件中的Annotation可能会被虚拟机忽略，而另一些在class被装载时将被读取（请注意并不影响class的执行，因为Annotation与class在使用上是被分离的）。使用这个meta-Annotation可以对 Annotation的“生命周期”限制。</p><p><strong>作用</strong>：表示需要在什么级别保存该注释信息，用于描述注解的生命周期（即：被描述的注解在什么范围内有效）</p><ol start="3"><li><p><strong>@Documented</strong><br>@Documented用于描述其它类型的annotation应该被作为被标注的程序成员的公共API，因此可以被例如javadoc此类的工具文档化。Documented是一个标记注解，没有成员。</p></li><li><p><strong>@Inherited</strong><br>@Inherited 元注解是一个标记注解，@Inherited阐述了某个被标注的类型是被继承的。如果一个使用了@Inherited修饰的annotation类型被用于一个class，则这个annotation将被用于该class的子类。</p></li></ol><h2 id="自定义注解"><a href="#自定义注解" class="headerlink" title="自定义注解"></a>自定义注解</h2><p>使用@interface自定义注解时，<strong>自动继承了java.lang.annotation.Annotation接口</strong>，由编译程序自动完成其他细节。在定义注解时，不能继承其他的注解或接口。@interface用来声明一个注解，其中的每一个方法实际上是声明了一个配置参数。方法的名称就是参数的名称，返回值类型就是参数的类型（返回值类型只能是基本类型、Class、String、enum）。可以通过default来声明参数的默认值。</p><p>定义注解格式：</p><pre class=" language-java"><code class="language-java">　　<span class="token keyword">public</span> @<span class="token keyword">interface</span> 注解名 <span class="token punctuation">{</span>定义体<span class="token punctuation">}</span></code></pre><p>注解参数的可支持数据类型：</p><ol><li>所有基本数据类型（int,float,boolean,byte,double,char,long,short)</li><li>String类型</li><li>Class类型</li><li>enum类型</li><li>Annotation类型</li><li>以上所有类型的数组</li></ol><p>Annotation类型里面的参数该怎么设定:</p><ol><li>只能用public或默认(default)这两个访问权修饰.例如,String value();这里把方法设为defaul默认类型；　 　</li><li>参数成员只能用基本类型byte,short,char,int,long,float,double,boolean八种基本数据类型和 String,Enum,Class,annotations等数据类型,以及这一些类型的数组.例如,String value();这里的参数成员就为String;　　</li><li>如果只有一个参数成员,最好把参数名称设为”value”,后加小括号.　　</li></ol><h3 id="注解元素的默认值"><a href="#注解元素的默认值" class="headerlink" title="注解元素的默认值"></a>注解元素的默认值</h3><p>注解元素必须有确定的值，要么在定义注解的默认值中指定，要么在使用注解时指定，非基本类型的注解元素的值不可为null。因此, 使用空字符串或0作为默认值是一种常用的做法。这个约束使得处理器很难表现一个元素的存在或缺失的状态，因为每个注解的声明中，所有元素都存在，并且都具有相应的值，为了绕开这个约束，我们只能定义一些特殊的值，例如空字符串或者负数，一次表示某个元素不存在，在定义注解时，这已经成为一个习惯用法。</p><h2 id="注解处理器"><a href="#注解处理器" class="headerlink" title="注解处理器"></a>注解处理器</h2><p>如果没有用来读取注解的方法和工作，那么注解也就不会比注释更有用处了。使用注解的过程中，很重要的一部分就是创建于使用注解处理器。Java SE5扩展了反射机制的API，以帮助程序员快速的构造自定义注解处理器。</p><h3 id="注解处理器类库-java-lang-reflect-AnnotatedElement"><a href="#注解处理器类库-java-lang-reflect-AnnotatedElement" class="headerlink" title="注解处理器类库(java.lang.reflect.AnnotatedElement)"></a>注解处理器类库(java.lang.reflect.AnnotatedElement)</h3><p><strong>Java使用Annotation接口来代表程序元素前面的注解，该接口是所有Annotation类型的父接口。</strong> 除此之外，Java在java.lang.reflect 包下新增了AnnotatedElement接口，该接口代表程序中可以接受注解的程序元素，该接口主要有如下几个实现类：</p><ul><li>Class：类定义</li><li>Constructor：构造器定义</li><li>Field：累的成员变量定义</li><li>Method：类的方法定义</li><li>Package：类的包定义</li></ul><p><img src="https://note.youdao.com/yws/api/personal/file/WEB3e72f264dcf3813004bf86ceefa9ae10?method=download&amp;shareKey=df0ddce169feff7b3912b1718979a791" alt="Java Annotation"></p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ol><li><a href="https://www.cnblogs.com/happyPawpaw/archive/2013/04/09/3009553.html" target="_blank" rel="noopener">Java enum的用法详解</a></li><li><a href="https://www.ibm.com/developerworks/cn/java/j-lo-enum/" target="_blank" rel="noopener">Java 语言中 Enum 类型的使用介绍</a></li><li><a href="http://www.cnblogs.com/peida/archive/2013/04/23/3036035.html" target="_blank" rel="noopener">深入理解Java：注解（Annotation）基本概念</a></li><li><a href="http://www.importnew.com/10294.html" target="_blank" rel="noopener">Java中的注解是如何工作的</a></li><li><a href="https://docs.oracle.com/javase/tutorial/java/annotations/index.html" target="_blank" rel="noopener">The Java™ Tutorials: Annotations</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> Java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Java </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Java 反射</title>
      <link href="/2018/10/23/12-java-reflect/"/>
      <url>/2018/10/23/12-java-reflect/</url>
      
        <content type="html"><![CDATA[<h2 id="反射机制是什么"><a href="#反射机制是什么" class="headerlink" title="反射机制是什么"></a>反射机制是什么</h2><p>反射机制是在运行状态中，对于任意一个类，都能够知道这个类的所有属性和方法；对于任意一个对象，都能够调用它的任意一个方法和属性；这种动态获取的信息以及动态调用对象的方法的功能称为java语言的反射机制。</p><h2 id="反射机制能做什么"><a href="#反射机制能做什么" class="headerlink" title="反射机制能做什么"></a>反射机制能做什么</h2><p>反射机制主要提供了以下功能：</p><ul><li>在运行时判断任意一个对象所属的类；</li><li>在运行时构造任意一个类的对象；</li><li>在运行时判断任意一个类所具有的成员变量和方法；</li><li>在运行时调用任意一个对象的方法；</li><li>生成动态代理。</li></ul><h2 id="反射使用方式"><a href="#反射使用方式" class="headerlink" title="反射使用方式"></a>反射使用方式</h2><pre class=" language-java"><code class="language-java"><span class="token comment" spellcheck="true">// forName</span><span class="token keyword">try</span> <span class="token punctuation">{</span>    Class<span class="token operator">&lt;</span><span class="token operator">?</span><span class="token operator">></span> clazz1 <span class="token operator">=</span> Class<span class="token punctuation">.</span><span class="token function">forName</span><span class="token punctuation">(</span><span class="token string">"com.baidu.jiaotong.stone.reflection1.Person"</span><span class="token punctuation">)</span><span class="token punctuation">;</span>    System<span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span>clazz1<span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token punctuation">}</span> <span class="token keyword">catch</span> <span class="token punctuation">(</span><span class="token class-name">ClassNotFoundException</span> e<span class="token punctuation">)</span> <span class="token punctuation">{</span>    e<span class="token punctuation">.</span><span class="token function">printStackTrace</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token punctuation">}</span><span class="token comment" spellcheck="true">// class</span>Class<span class="token operator">&lt;</span><span class="token operator">?</span><span class="token operator">></span> clazz2 <span class="token operator">=</span> Person<span class="token punctuation">.</span><span class="token keyword">class</span><span class="token punctuation">;</span>System<span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span>clazz2<span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token comment" spellcheck="true">// getClass</span>Person person <span class="token operator">=</span> <span class="token keyword">new</span> <span class="token class-name">Person</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>Class<span class="token operator">&lt;</span><span class="token operator">?</span><span class="token operator">></span> clazz3 <span class="token operator">=</span> person<span class="token punctuation">.</span><span class="token function">getClass</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span>System<span class="token punctuation">.</span>out<span class="token punctuation">.</span><span class="token function">println</span><span class="token punctuation">(</span>clazz3<span class="token punctuation">)</span><span class="token punctuation">;</span></code></pre><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ol><li><a href="http://www.cnblogs.com/zhaoyanjun/p/6074887.html" target="_blank" rel="noopener">Java 反射总结</a></li><li><a href="https://docs.oracle.com/javase/tutorial/reflect/index.html" target="_blank" rel="noopener">The Java™ Tutorials: Reflection</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> Java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Java </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Java 集合类 - java.util</title>
      <link href="/2018/10/22/11-java-collections/"/>
      <url>/2018/10/22/11-java-collections/</url>
      
        <content type="html"><![CDATA[<h2 id="集合类框架图"><a href="#集合类框架图" class="headerlink" title="集合类框架图"></a>集合类框架图</h2><p><img src="https://note.youdao.com/yws/api/personal/file/WEB80befac9e2577ed5dc7f7b0d08dc3412?method=download&amp;shareKey=dd94762edb9b6ff7b44827ea9f3465c2" alt="Java集合工具包框架图"></p><h2 id="类继承关系"><a href="#类继承关系" class="headerlink" title="类继承关系"></a>类继承关系</h2><p><img src="https://note.youdao.com/yws/api/personal/file/WEB527bfb6d4fc936a7436e5c248e06884e?method=download&amp;shareKey=075d753bfca73d0ea1df38a1736b5f96" alt="Java集合类接口继承关系"></p><p><img src="https://note.youdao.com/yws/api/personal/file/WEBdc643897f9fc5c55b8dac15c83fb81e4?method=download&amp;shareKey=4949d7e7752fe62663eaeaefb65d4b4f" alt="Java集合类继承关系"></p><h2 id="常用集合类"><a href="#常用集合类" class="headerlink" title="常用集合类"></a>常用集合类</h2><h3 id="List-ArrayList-Vector-Stack-LinkedList"><a href="#List-ArrayList-Vector-Stack-LinkedList" class="headerlink" title="List, ArrayList, Vector, Stack, LinkedList"></a>List, ArrayList, Vector, Stack, LinkedList</h3><h4 id="ArrayList-and-Vector"><a href="#ArrayList-and-Vector" class="headerlink" title="ArrayList and Vector"></a>ArrayList and Vector</h4><p>ArrayList和Vector在用法上几乎完全相同，但由于Vector是一个古老的集合(从JDK1.0就有了)，最开始的时候，Java没有提供系统的集合框架，所以Vector里面提供一些方法名很长的方法：例如addElement(Object obj);实际上这个方法与add(Object obj)没有任何区别。从JDK1.2之后，Java提供了系统的集合框架，就将Vector改为实现List接口，做为List的实现之一，从而导致Vector里面有一些功能重复的方法。</p><p>除此之外，ArrayList和Vector的显著区别是；ArrayList是线程不安全的，当多条线程访问同一个ArrayList集合时，如果有超过一条线程修改了ArrayList集合，则程序必须受到保证该集合的同步性。但Vector是线程安全的，无须程序保证该集合的同步性。因为Vector是线程安全的，所以Vector的性能要比ArrayList的效率要低。实际上即使需要保证list集合线程安全，同样不推荐使用Vector实现类，而是使用Collections工具类，它可以将一个ArrayList变的线程安全。</p><h2 id="Set-HashSet-LinkedHashSet-EnumSet-TreeSet"><a href="#Set-HashSet-LinkedHashSet-EnumSet-TreeSet" class="headerlink" title="Set, HashSet, LinkedHashSet, EnumSet, TreeSet"></a>Set, HashSet, LinkedHashSet, EnumSet, TreeSet</h2><h3 id="Queue-PriorityQueue"><a href="#Queue-PriorityQueue" class="headerlink" title="Queue, PriorityQueue"></a>Queue, PriorityQueue</h3><h3 id="Map-HashMap-HashTable-EnumMap-TreeMap"><a href="#Map-HashMap-HashTable-EnumMap-TreeMap" class="headerlink" title="Map, HashMap, HashTable, EnumMap, TreeMap"></a>Map, HashMap, HashTable, EnumMap, TreeMap</h3><h4 id="HashMap-and-HashTable"><a href="#HashMap-and-HashTable" class="headerlink" title="HashMap and HashTable"></a>HashMap and HashTable</h4><p>HashMap和Hashtable的两点典型区别：</p><p>Hashtable是一个线程安全的Map实现，但HashMap是线程不安全的实现，所以HashMap比Hashtable性能要高一点；但如果有多条线程访问同一个Map对象时，使用Hashtable实现类会更好。</p><p>Hashtable不容许使用null作为key和value，如果试图把null放进Hashtable中，将会引发NullPointerException异常；但HashMap可以使用null做为key和value。</p><p>注意：与Vector类似，尽量少用Hashtable实现类，即使需要创建线程安全的Map实现类，也可以通过Collections工具类把HashMap变成线程安全的，无须使用Hashtable实现类。</p><h2 id="工具类"><a href="#工具类" class="headerlink" title="工具类"></a>工具类</h2><h3 id="Collections"><a href="#Collections" class="headerlink" title="Collections"></a>Collections</h3><p>Java提供了一个操作Set、List和Map等集合的工具类：Collections，该工具类里提供了大量方法对集合元素进行排序、查询和修改等操作，还提供了将集合对象设置为不可变、对集合对象实现同步控制等方法。</p><h3 id="Arrays"><a href="#Arrays" class="headerlink" title="Arrays"></a>Arrays</h3><p>sort排序和parallelSort并行排序</p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol><li><a href="https://docs.oracle.com/javase/8/docs/technotes/guides/collections/" target="_blank" rel="noopener">JDK 8: The Collections Framework</a></li><li><a href="http://www.cnblogs.com/skywang12345/p/3308498.html" target="_blank" rel="noopener">Java 集合类总体框架</a></li><li><a href="https://www.cnblogs.com/be-forward-to-help-others/p/6708130.html" target="_blank" rel="noopener">Java中的集合HashSet、LinkedHashSet、TreeSet和EnumSet(二)</a></li><li><a href="https://docs.oracle.com/javase/tutorial/collections/index.html" target="_blank" rel="noopener">The Java™ Tutorials: Collections</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> Java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Java </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Java IO</title>
      <link href="/2018/10/21/9-java-io/"/>
      <url>/2018/10/21/9-java-io/</url>
      
        <content type="html"><![CDATA[<h2 id="基本"><a href="#基本" class="headerlink" title="基本"></a>基本</h2><h3 id="总述"><a href="#总述" class="headerlink" title="总述"></a>总述</h3><p>Java 的 I/O 操作类在包 java.io 下，大概有将近 80 个类，但是这些类大概可以分成四组，分别是：</p><ol><li>字节流： InputStream, OutputStream</li><li>字符流： Reader, Writer</li><li>磁盘操作IO接口： File</li><li>网络操作IO接口： Socket</li><li>随机存储文件类： RandomAccessFile</li></ol><h3 id="分类一：功能上分"><a href="#分类一：功能上分" class="headerlink" title="分类一：功能上分"></a>分类一：功能上分</h3><ul><li>输入流</li><li>输出流</li></ul><h3 id="分类二：结构上分"><a href="#分类二：结构上分" class="headerlink" title="分类二：结构上分"></a>分类二：结构上分</h3><ul><li>字节流</li><li>字符流</li></ul><h3 id="分类三："><a href="#分类三：" class="headerlink" title="分类三："></a>分类三：</h3><h4 id="节点流"><a href="#节点流" class="headerlink" title="节点流"></a>节点流</h4><p>从特定的地方读写的流类，比如：磁盘或一块内存区域</p><h4 id="过滤流"><a href="#过滤流" class="headerlink" title="过滤流"></a>过滤流</h4><p>使用节点流作为输入或输出，过滤流是使用一个已经存在的输入流或输出流连接创造的。</p><h2 id="详细"><a href="#详细" class="headerlink" title="详细"></a>详细</h2><ol><li><p>File类</p></li><li><p>RandomAccessFile类</p></li><li><p>字节流和字符流</p></li><li><p>字节-字符转换流<br>在整个IO包中，实际上就是分为字节流和字符流，但是除了这两个流之外，还存在了一组字节流-字符流的转换类。<br><strong>OutputStreamWriter</strong>: 是Writer的子类，将输出的字符流变为字节流，即：将一个字符流的输出对象变成字节流的输出对象。<br><strong>InputStreamReader</strong>: 是Reader的子类，将输入的字节流变为字符流，即：将一个字节流的输入对象变成字符流的输入对象。<br>一般在操作输入输出内容就需要使用字节或字符流，但是有些时候需要将字符流变成字节流的形式，或者将字节流变为字符流的形式，所以，就需要另外一组转换流的操作类。</p></li><li><p>内存操作流<br>之前所讲解的程序中，输出和输入都是从文件中来得，当然，也可以将输出的位置设置在内存之上，此时就要使用ByteArrayInputStream、ByteArrayOutputStream来完成输入输出功能了<br><strong>ByteArrayInputStream</strong>的主要功能将内容输入到内存之中<br><strong>ByteArrayOutputStream</strong>的主要功能是将内存中的数据输出<br>此时应该把内存作为操作点。</p></li><li><p>管道流(线程通信流)<br>管道流的主要作用是可以进行两个线程间的通讯，分为管道输出流(PipedOutputStream)、管道输入流（PipedInputStream），如果想要进行管道输出，则必须要把输出流连在输入流之上，在PipedOutputStream类上有如下的一个方法用于连接管道：<br>public void connect(PipedInputStream snk)throws IOException</p></li></ol><p><img src="https://ws1.sinaimg.cn/large/006tKfTcly1g0g6z2zc5pj30rj07174r.jpg" alt="管道流"></p><ol start="7"><li><p>打印流<br>在整个IO包中，打印流是输出信息最方便的类，主要包含字节打印流（PrintStream）和字符打印流（PrintWrite）。打印流提供了非常方便的打印功能，可以打印任何的数据类型，例如：小数、整数、字符串等等。</p></li><li><p>System类对IO的支持<br>System类的常量<br>System表示系统类，此类也对IO给予了一定的支持。<br>public static final PrintStream out  对应系统标准输出，一般是显示器<br>public static final PrintStream err   错误信息输出<br>public static final InputStream in   对应着标准输入，一般是键盘<br>又是由于历史遗留问题 全局变量没有大写~</p></li></ol><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ol><li><a href="http://blog.csdn.net/zhangerqing/article/details/8466532" target="_blank" rel="noopener">Java中的IO</a></li><li><a href="http://www.cnblogs.com/oubo/archive/2012/01/06/2394638.html" target="_blank" rel="noopener">IO流学习总结</a></li><li><a href="http://www.cnblogs.com/lovebread/archive/2009/11/23/1609122.html" target="_blank" rel="noopener">Java读文件</a></li><li><a href="http://www.cnblogs.com/lich/archive/2011/12/10/2283445.html" target="_blank" rel="noopener"><strong>Java IO 学习笔记</strong></a></li><li><a href="https://docs.oracle.com/javase/tutorial/essential/io/index.html" target="_blank" rel="noopener">Java Basic I/O</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> Java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Java </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Log4j2</title>
      <link href="/2018/10/20/10-log4j2/"/>
      <url>/2018/10/20/10-log4j2/</url>
      
        <content type="html"><![CDATA[<h2 id="Main-Componments"><a href="#Main-Componments" class="headerlink" title="Main Componments"></a>Main Componments</h2><h3 id="Log4j-Classes"><a href="#Log4j-Classes" class="headerlink" title="Log4j Classes"></a>Log4j Classes</h3><p><img src="https://logging.apache.org/log4j/2.x/images/Log4jClasses.jpg" alt="MacDown logo"></p><ul><li><strong>LoggerContext</strong></li><li><strong>Configuration</strong></li><li><strong>Logger</strong></li><li><strong>LoggerConfig</strong></li><li><strong>Filter</strong></li><li><strong>Appender</strong></li><li><strong>Layout</strong></li><li><strong>StrSubstitutor and StrLookup</strong></li></ul><h3 id="Using-Log4j-in-your-Apache-Maven-build"><a href="#Using-Log4j-in-your-Apache-Maven-build" class="headerlink" title="Using Log4j in your Apache Maven build"></a>Using Log4j in your Apache Maven build</h3><pre><code>&lt;dependencies&gt;  &lt;dependency&gt;    &lt;groupId&gt;org.apache.logging.log4j&lt;/groupId&gt;    &lt;artifactId&gt;log4j-api&lt;/artifactId&gt;    &lt;version&gt;2.8.1&lt;/version&gt;  &lt;/dependency&gt;  &lt;dependency&gt;    &lt;groupId&gt;org.apache.logging.log4j&lt;/groupId&gt;    &lt;artifactId&gt;log4j-core&lt;/artifactId&gt;    &lt;version&gt;2.8.1&lt;/version&gt;  &lt;/dependency&gt;&lt;/dependencies&gt;</code></pre><h3 id="SLF4J-Bridge"><a href="#SLF4J-Bridge" class="headerlink" title="SLF4J Bridge"></a>SLF4J Bridge</h3><p>If existing components use SLF4J and you want to have this logging routed to Log4j 2, then add the following but do not remove any SLF4J dependencies.</p><pre><code>&lt;dependencies&gt;  &lt;dependency&gt;    &lt;groupId&gt;org.apache.logging.log4j&lt;/groupId&gt;    &lt;artifactId&gt;log4j-slf4j-impl&lt;/artifactId&gt;    &lt;version&gt;2.8.1&lt;/version&gt;  &lt;/dependency&gt;&lt;/dependencies&gt;</code></pre><h3 id="基本用法"><a href="#基本用法" class="headerlink" title="基本用法"></a>基本用法</h3><blockquote><p>注：参考中的4详细讲述了log4j2的基本用法，非常适合新手入门。</p></blockquote><h4 id="基本配置"><a href="#基本配置" class="headerlink" title="基本配置"></a>基本配置</h4><p>示例1：<br><strong>log4j2.xml</strong></p><pre class=" language-xml"><code class="language-xml"><span class="token prolog">&lt;?xml version="1.0" encoding="UTF-8" ?></span><span class="token doctype">&lt;!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/2002/xmlspec/dtd/2.10/xmlspec.dtd"></span><span class="token comment" spellcheck="true">&lt;!-- status : 这个用于设置log4j2自身内部的信息输出,可以不设置,当设置成trace时,会看到log4j2内部各种详细输出 monitorInterval    : Log4j能够自动检测修改配置文件和重新配置本身, 设置间隔秒数。 注：本配置文件的目标是将不同级别的日志输出到不同文件，最大2MB一个文件，    文件数据达到最大值时，旧数据会被压缩并放进指定文件夹 --></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>Configuration</span> <span class="token attr-name">status</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>WARN<span class="token punctuation">"</span></span> <span class="token attr-name">monitorInterval</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>600<span class="token punctuation">"</span></span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>Properties</span><span class="token punctuation">></span></span> <span class="token comment" spellcheck="true">&lt;!-- 配置日志文件输出目录，此配置将日志输出到tomcat根目录下的指定文件夹 --></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>Property</span> <span class="token attr-name">name</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>LOG_HOME<span class="token punctuation">"</span></span><span class="token punctuation">></span></span>${sys:catalina.home}/WebAppLogs/HHServices        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>Property</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>Properties</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>Appenders</span><span class="token punctuation">></span></span>        <span class="token comment" spellcheck="true">&lt;!-- 优先级从高到低分别是 OFF、FATAL、ERROR、WARN、INFO、DEBUG、ALL --></span>        <span class="token comment" spellcheck="true">&lt;!-- 单词解释： Match：匹配 DENY：拒绝 Mismatch：不匹配 ACCEPT：接受 --></span>        <span class="token comment" spellcheck="true">&lt;!-- DENY，日志将立即被抛弃不再经过其他过滤器； NEUTRAL，有序列表里的下个过滤器过接着处理日志； ACCEPT，日志会被立即处理，不再经过剩余过滤器。 --></span>        <span class="token comment" spellcheck="true">&lt;!--输出日志的格式         %d{yyyy-MM-dd HH:mm:ss, SSS} : 日志生产时间         %p : 日志输出格式         %c : logger的名称         %m : 日志内容，即 logger.info("message")         %n : 换行符         %C : Java类名         %L : 日志输出所在行数         %M : 日志输出所在方法名         hostName : 本地机器名         hostAddress : 本地ip地址 --></span>        <span class="token comment" spellcheck="true">&lt;!--这个输出控制台的配置，这里输出除了warn和error级别的信息到System.out --></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>Console</span> <span class="token attr-name">name</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>console_out_appender<span class="token punctuation">"</span></span> <span class="token attr-name">target</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>SYSTEM_OUT<span class="token punctuation">"</span></span><span class="token punctuation">></span></span>            <span class="token comment" spellcheck="true">&lt;!-- 控制台只输出level及以上级别的信息(onMatch),其他的直接拒绝(onMismatch) . --></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>ThresholdFilter</span> <span class="token attr-name">level</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>DEBUG<span class="token punctuation">"</span></span> <span class="token attr-name">onMatch</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>ACCEPT<span class="token punctuation">"</span></span>                <span class="token attr-name">onMismatch</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>DENY<span class="token punctuation">"</span></span> <span class="token punctuation">/></span></span>            <span class="token comment" spellcheck="true">&lt;!-- 输出日志的格式 --></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>PatternLayout</span> <span class="token attr-name">pattern</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>%5p [%t] %d{yyyy-MM-dd HH:mm:ss} (%F:%L) %m%n<span class="token punctuation">"</span></span> <span class="token punctuation">/></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>Console</span><span class="token punctuation">></span></span>        <span class="token comment" spellcheck="true">&lt;!-- 这个输出控制台的配置，这里输出error级别的信息到System.err，在eclipse控制台上看到的是红色文字 --></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>Console</span> <span class="token attr-name">name</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>console_err_appender<span class="token punctuation">"</span></span> <span class="token attr-name">target</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>SYSTEM_ERR<span class="token punctuation">"</span></span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>ThresholdFilter</span> <span class="token attr-name">level</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>ERROR<span class="token punctuation">"</span></span> <span class="token attr-name">onMatch</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>ACCEPT<span class="token punctuation">"</span></span>                <span class="token attr-name">onMismatch</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>DENY<span class="token punctuation">"</span></span> <span class="token punctuation">/></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>PatternLayout</span> <span class="token attr-name">pattern</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>%5p [%t] %d{yyyy-MM-dd HH:mm:ss} (%F:%L) %m%n<span class="token punctuation">"</span></span> <span class="token punctuation">/></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>Console</span><span class="token punctuation">></span></span>        <span class="token comment" spellcheck="true">&lt;!-- TRACE级别日志 ; 设置日志格式并配置日志压缩格式，压缩文件独立放在一个文件夹内， 日期格式不能为冒号，否则无法生成，因为文件名不允许有冒号，此appender只输出trace级别的数据到trace.log --></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>RollingFile</span> <span class="token attr-name">name</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>trace_appender<span class="token punctuation">"</span></span> <span class="token attr-name">immediateFlush</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>true<span class="token punctuation">"</span></span>            <span class="token attr-name">fileName</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>${LOG_HOME}/trace.log<span class="token punctuation">"</span></span> <span class="token attr-name">filePattern</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>${LOG_HOME}/trace/trace - %d{yyyy-MM-dd HH_mm_ss}.log.gz<span class="token punctuation">"</span></span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>PatternLayout</span><span class="token punctuation">></span></span>                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>pattern</span><span class="token punctuation">></span></span>%5p [%t] %d{yyyy-MM-dd HH:mm:ss} (%F:%L) %m%n<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>pattern</span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>PatternLayout</span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>Policies</span><span class="token punctuation">></span></span>                <span class="token comment" spellcheck="true">&lt;!-- 每个日志文件最大2MB --></span>                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>SizeBasedTriggeringPolicy</span> <span class="token attr-name">size</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>2MB<span class="token punctuation">"</span></span> <span class="token punctuation">/></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>Policies</span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>Filters</span><span class="token punctuation">></span></span>                <span class="token comment" spellcheck="true">&lt;!-- 此Filter意思是，只输出TRACE级别的数据 DENY，日志将立即被抛弃不再经过其他过滤器； NEUTRAL，有序列表里的下个过滤器过接着处理日志；                    ACCEPT，日志会被立即处理，不再经过剩余过滤器。 --></span>                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>ThresholdFilter</span> <span class="token attr-name">level</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>debug<span class="token punctuation">"</span></span> <span class="token attr-name">onMatch</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>DENY<span class="token punctuation">"</span></span>                    <span class="token attr-name">onMismatch</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>NEUTRAL<span class="token punctuation">"</span></span> <span class="token punctuation">/></span></span>                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>ThresholdFilter</span> <span class="token attr-name">level</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>trace<span class="token punctuation">"</span></span> <span class="token attr-name">onMatch</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>ACCEPT<span class="token punctuation">"</span></span>                    <span class="token attr-name">onMismatch</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>DENY<span class="token punctuation">"</span></span> <span class="token punctuation">/></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>Filters</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>RollingFile</span><span class="token punctuation">></span></span>        <span class="token comment" spellcheck="true">&lt;!-- DEBUG级别日志 设置日志格式并配置日志压缩格式，压缩文件独立放在一个文件夹内， 日期格式不能为冒号，否则无法生成，因为文件名不允许有冒号，此appender只输出debug级别的数据到debug.log; --></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>RollingFile</span> <span class="token attr-name">name</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>debug_appender<span class="token punctuation">"</span></span> <span class="token attr-name">immediateFlush</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>true<span class="token punctuation">"</span></span>            <span class="token attr-name">fileName</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>${LOG_HOME}/debug.log<span class="token punctuation">"</span></span> <span class="token attr-name">filePattern</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>${LOG_HOME}/debug/debug - %d{yyyy-MM-dd HH_mm_ss}.log.gz<span class="token punctuation">"</span></span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>PatternLayout</span><span class="token punctuation">></span></span>                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>pattern</span><span class="token punctuation">></span></span>%5p [%t] %d{yyyy-MM-dd HH:mm:ss} (%F:%L) %m%n<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>pattern</span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>PatternLayout</span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>Policies</span><span class="token punctuation">></span></span><span class="token comment" spellcheck="true">&lt;!-- 每个日志文件最大2MB ; --></span>                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>SizeBasedTriggeringPolicy</span> <span class="token attr-name">size</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>2MB<span class="token punctuation">"</span></span> <span class="token punctuation">/></span></span>                <span class="token comment" spellcheck="true">&lt;!-- 如果启用此配置，则日志会按文件名生成新压缩文件， 即如果filePattern配置的日期格式为 %d{yyyy-MM-dd HH}                    ，则每小时生成一个压缩文件， 如果filePattern配置的日期格式为 %d{yyyy-MM-dd} ，则天生成一个压缩文件 --></span>                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>TimeBasedTriggeringPolicy</span> <span class="token attr-name">interval</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>1<span class="token punctuation">"</span></span>                    <span class="token attr-name">modulate</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>true<span class="token punctuation">"</span></span> <span class="token punctuation">/></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>Policies</span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>Filters</span><span class="token punctuation">></span></span><span class="token comment" spellcheck="true">&lt;!-- 此Filter意思是，只输出debug级别的数据 --></span>                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>ThresholdFilter</span> <span class="token attr-name">level</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>info<span class="token punctuation">"</span></span> <span class="token attr-name">onMatch</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>DENY<span class="token punctuation">"</span></span>                    <span class="token attr-name">onMismatch</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>NEUTRAL<span class="token punctuation">"</span></span> <span class="token punctuation">/></span></span>                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>ThresholdFilter</span> <span class="token attr-name">level</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>debug<span class="token punctuation">"</span></span> <span class="token attr-name">onMatch</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>ACCEPT<span class="token punctuation">"</span></span>                    <span class="token attr-name">onMismatch</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>DENY<span class="token punctuation">"</span></span> <span class="token punctuation">/></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>Filters</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>RollingFile</span><span class="token punctuation">></span></span>        <span class="token comment" spellcheck="true">&lt;!-- INFO级别日志 --></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>RollingFile</span> <span class="token attr-name">name</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>info_appender<span class="token punctuation">"</span></span> <span class="token attr-name">immediateFlush</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>true<span class="token punctuation">"</span></span>            <span class="token attr-name">fileName</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>${LOG_HOME}/info.log<span class="token punctuation">"</span></span> <span class="token attr-name">filePattern</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>${LOG_HOME}/info/info - %d{yyyy-MM-dd HH_mm_ss}.log.gz<span class="token punctuation">"</span></span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>PatternLayout</span><span class="token punctuation">></span></span>                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>pattern</span><span class="token punctuation">></span></span>%5p [%t] %d{yyyy-MM-dd HH:mm:ss} (%F:%L) %m%n<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>pattern</span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>PatternLayout</span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>Policies</span><span class="token punctuation">></span></span>                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>SizeBasedTriggeringPolicy</span> <span class="token attr-name">size</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>2MB<span class="token punctuation">"</span></span> <span class="token punctuation">/></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>Policies</span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>Filters</span><span class="token punctuation">></span></span>                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>ThresholdFilter</span> <span class="token attr-name">level</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>warn<span class="token punctuation">"</span></span> <span class="token attr-name">onMatch</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>DENY<span class="token punctuation">"</span></span>                    <span class="token attr-name">onMismatch</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>NEUTRAL<span class="token punctuation">"</span></span> <span class="token punctuation">/></span></span>                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>ThresholdFilter</span> <span class="token attr-name">level</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>info<span class="token punctuation">"</span></span> <span class="token attr-name">onMatch</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>ACCEPT<span class="token punctuation">"</span></span>                    <span class="token attr-name">onMismatch</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>DENY<span class="token punctuation">"</span></span> <span class="token punctuation">/></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>Filters</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>RollingFile</span><span class="token punctuation">></span></span>        <span class="token comment" spellcheck="true">&lt;!-- WARN级别日志 --></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>RollingFile</span> <span class="token attr-name">name</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>warn_appender<span class="token punctuation">"</span></span> <span class="token attr-name">immediateFlush</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>true<span class="token punctuation">"</span></span>            <span class="token attr-name">fileName</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>${LOG_HOME}/warn.log<span class="token punctuation">"</span></span> <span class="token attr-name">filePattern</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>${LOG_HOME}/warn/warn - %d{yyyy-MM-dd HH_mm_ss}.log.gz<span class="token punctuation">"</span></span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>PatternLayout</span><span class="token punctuation">></span></span>                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>pattern</span><span class="token punctuation">></span></span>%5p [%t] %d{yyyy-MM-dd HH:mm:ss} (%F:%L) %m%n<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>pattern</span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>PatternLayout</span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>Policies</span><span class="token punctuation">></span></span>                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>SizeBasedTriggeringPolicy</span> <span class="token attr-name">size</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>2MB<span class="token punctuation">"</span></span> <span class="token punctuation">/></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>Policies</span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>Filters</span><span class="token punctuation">></span></span>                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>ThresholdFilter</span> <span class="token attr-name">level</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>error<span class="token punctuation">"</span></span> <span class="token attr-name">onMatch</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>DENY<span class="token punctuation">"</span></span>                    <span class="token attr-name">onMismatch</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>NEUTRAL<span class="token punctuation">"</span></span> <span class="token punctuation">/></span></span>                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>ThresholdFilter</span> <span class="token attr-name">level</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>warn<span class="token punctuation">"</span></span> <span class="token attr-name">onMatch</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>ACCEPT<span class="token punctuation">"</span></span>                    <span class="token attr-name">onMismatch</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>DENY<span class="token punctuation">"</span></span> <span class="token punctuation">/></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>Filters</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>RollingFile</span><span class="token punctuation">></span></span>        <span class="token comment" spellcheck="true">&lt;!-- ERROR级别日志 --></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>RollingFile</span> <span class="token attr-name">name</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>error_appender<span class="token punctuation">"</span></span> <span class="token attr-name">immediateFlush</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>true<span class="token punctuation">"</span></span>            <span class="token attr-name">fileName</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>${LOG_HOME}/error.log<span class="token punctuation">"</span></span> <span class="token attr-name">filePattern</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>${LOG_HOME}/error/error - %d{yyyy-MM-dd HH_mm_ss}.log.gz<span class="token punctuation">"</span></span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>PatternLayout</span><span class="token punctuation">></span></span>                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>pattern</span><span class="token punctuation">></span></span>%5p [%t] %d{yyyy-MM-dd HH:mm:ss} (%F:%L) %m%n<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>pattern</span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>PatternLayout</span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>Policies</span><span class="token punctuation">></span></span>                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>SizeBasedTriggeringPolicy</span> <span class="token attr-name">size</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>2MB<span class="token punctuation">"</span></span> <span class="token punctuation">/></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>Policies</span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>Filters</span><span class="token punctuation">></span></span>                <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>ThresholdFilter</span> <span class="token attr-name">level</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>error<span class="token punctuation">"</span></span> <span class="token attr-name">onMatch</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>ACCEPT<span class="token punctuation">"</span></span>                    <span class="token attr-name">onMismatch</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>DENY<span class="token punctuation">"</span></span> <span class="token punctuation">/></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>Filters</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>RollingFile</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>Appenders</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>Loggers</span><span class="token punctuation">></span></span>        <span class="token comment" spellcheck="true">&lt;!-- 配置日志的根节点 --></span>        <span class="token comment" spellcheck="true">&lt;!-- 定义logger，只有定义了logger并引入了appender，appender才会生效 --></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>root</span> <span class="token attr-name">level</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>trace<span class="token punctuation">"</span></span><span class="token punctuation">></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>appender-ref</span> <span class="token attr-name">ref</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>console_out_appender<span class="token punctuation">"</span></span> <span class="token punctuation">/></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>appender-ref</span> <span class="token attr-name">ref</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>console_err_appender<span class="token punctuation">"</span></span> <span class="token punctuation">/></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>appender-ref</span> <span class="token attr-name">ref</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>trace_appender<span class="token punctuation">"</span></span> <span class="token punctuation">/></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>appender-ref</span> <span class="token attr-name">ref</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>debug_appender<span class="token punctuation">"</span></span> <span class="token punctuation">/></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>appender-ref</span> <span class="token attr-name">ref</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>info_appender<span class="token punctuation">"</span></span> <span class="token punctuation">/></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>appender-ref</span> <span class="token attr-name">ref</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>warn_appender<span class="token punctuation">"</span></span> <span class="token punctuation">/></span></span>            <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>appender-ref</span> <span class="token attr-name">ref</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>error_appender<span class="token punctuation">"</span></span> <span class="token punctuation">/></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>root</span><span class="token punctuation">></span></span>        <span class="token comment" spellcheck="true">&lt;!-- 第三方日志系统 --></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>logger</span> <span class="token attr-name">name</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>org.springframework.core<span class="token punctuation">"</span></span> <span class="token attr-name">level</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>info<span class="token punctuation">"</span></span> <span class="token punctuation">/></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>logger</span> <span class="token attr-name">name</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>org.springframework.beans<span class="token punctuation">"</span></span> <span class="token attr-name">level</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>info<span class="token punctuation">"</span></span> <span class="token punctuation">/></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>logger</span> <span class="token attr-name">name</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>org.springframework.context<span class="token punctuation">"</span></span> <span class="token attr-name">level</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>info<span class="token punctuation">"</span></span> <span class="token punctuation">/></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>logger</span> <span class="token attr-name">name</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>org.springframework.web<span class="token punctuation">"</span></span> <span class="token attr-name">level</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>info<span class="token punctuation">"</span></span> <span class="token punctuation">/></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>logger</span> <span class="token attr-name">name</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>org.jboss.netty<span class="token punctuation">"</span></span> <span class="token attr-name">level</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>warn<span class="token punctuation">"</span></span> <span class="token punctuation">/></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>logger</span> <span class="token attr-name">name</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>org.apache.http<span class="token punctuation">"</span></span> <span class="token attr-name">level</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>warn<span class="token punctuation">"</span></span> <span class="token punctuation">/></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>Loggers</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>Configuration</span><span class="token punctuation">></span></span></code></pre><h4 id="按日志级别区分文件输出"><a href="#按日志级别区分文件输出" class="headerlink" title="按日志级别区分文件输出"></a>按日志级别区分文件输出</h4><blockquote><p>使用Filter实现</p></blockquote><p>ALL &lt; DEBUG &lt; INFO &lt; WARN &lt; ERROR &lt; FATAL &lt; OFF</p><p>示例2：<br><strong>log4j2.xml</strong></p><pre><code>&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;Configuration status=&quot;WARN&quot; monitorInterval=&quot;300&quot;&gt;    &lt;properties&gt;        &lt;property name=&quot;LOG_HOME&quot;&gt;logs&lt;/property&gt;        &lt;property name=&quot;FILE_NAME&quot;&gt;mylog&lt;/property&gt;    &lt;/properties&gt;    &lt;Appenders&gt;        &lt;Console name=&quot;Console&quot; target=&quot;SYSTEM_OUT&quot;&gt;            &lt;PatternLayout pattern=&quot;%d{HH:mm:ss.SSS} [%t] %-5level %logger{36} - %msg%n&quot; /&gt;        &lt;/Console&gt;        &lt;!-- &lt;File name=&quot;MyFile&quot; fileName=&quot;logs/app.log&quot; append=&quot;true&quot;&gt; &lt;PatternLayout            pattern=&quot;%d{HH:mm:ss.SSS} [%t] %-5level %logger{36} - %msg%n&quot; /&gt; &lt;/File&gt; --&gt;        &lt;RollingRandomAccessFile name=&quot;InfoFile&quot;            fileName=&quot;${LOG_HOME}/info.log&quot;            filePattern=&quot;${LOG_HOME}/$${date:yyyy-MM}/info-%d{yyyy-MM-dd HH-mm}-%i.log&quot;&gt;            &lt;Filters&gt;                &lt;ThresholdFilter level=&quot;warn&quot; onMatch=&quot;DENY&quot;                    onMismatch=&quot;NEUTRAL&quot; /&gt;                &lt;ThresholdFilter level=&quot;info&quot; onMatch=&quot;ACCEPT&quot;                    onMisMatch=&quot;DENY&quot; /&gt;            &lt;/Filters&gt;            &lt;PatternLayout                pattern=&quot;%date{yyyy-MM-dd HH:mm:ss.SSS} %level [%thread][%file:%line] - %msg%n&quot; /&gt;            &lt;Policies&gt;                &lt;TimeBasedTriggeringPolicy /&gt;                &lt;SizeBasedTriggeringPolicy size=&quot;10 MB&quot; /&gt;            &lt;/Policies&gt;            &lt;DefaultRolloverStrategy max=&quot;20&quot; /&gt;        &lt;/RollingRandomAccessFile&gt;        &lt;RollingRandomAccessFile name=&quot;WarnFile&quot;            fileName=&quot;${LOG_HOME}/warn.log&quot;            filePattern=&quot;${LOG_HOME}/$${date:yyyy-MM}/warn-%d{yyyy-MM-dd}-%i.log&quot;&gt;            &lt;Filters&gt;                &lt;ThresholdFilter level=&quot;error&quot; onMatch=&quot;DENY&quot;                    onMismatch=&quot;NEUTRAL&quot; /&gt;                &lt;ThresholdFilter level=&quot;warn&quot; onMatch=&quot;ACCEPT&quot;                    onMismatch=&quot;DENY&quot; /&gt;            &lt;/Filters&gt;            &lt;PatternLayout                pattern=&quot;%date{yyyy-MM-dd HH:mm:ss.SSS} %level [%thread][%file:%line] - %msg%n&quot; /&gt;            &lt;Policies&gt;                &lt;TimeBasedTriggeringPolicy /&gt;                &lt;SizeBasedTriggeringPolicy size=&quot;10 MB&quot; /&gt;            &lt;/Policies&gt;            &lt;DefaultRolloverStrategy max=&quot;20&quot; /&gt;        &lt;/RollingRandomAccessFile&gt;        &lt;RollingRandomAccessFile name=&quot;ErrorFile&quot;            fileName=&quot;${LOG_HOME}/error.log&quot;            filePattern=&quot;${LOG_HOME}/$${date:yyyy-MM}/error-%d{yyyy-MM-dd}-%i.log&quot;&gt;            &lt;Filters&gt;                &lt;ThresholdFilter level=&quot;fatal&quot; onMatch=&quot;DENY&quot;                    onMismatch=&quot;NEUTRAL&quot; /&gt;                &lt;ThresholdFilter level=&quot;error&quot; onMatch=&quot;ACCEPT&quot;                    onMismatch=&quot;DENY&quot; /&gt;            &lt;/Filters&gt;            &lt;PatternLayout                pattern=&quot;%date{yyyy-MM-dd HH:mm:ss.SSS} %level [%thread][%file:%line] - %msg%n&quot; /&gt;            &lt;Policies&gt;                &lt;TimeBasedTriggeringPolicy /&gt;                &lt;SizeBasedTriggeringPolicy size=&quot;10 MB&quot; /&gt;            &lt;/Policies&gt;            &lt;DefaultRolloverStrategy max=&quot;20&quot; /&gt;        &lt;/RollingRandomAccessFile&gt;        &lt;RollingRandomAccessFile name=&quot;FatalFile&quot;            fileName=&quot;${LOG_HOME}/fatal.log&quot;            filePattern=&quot;${LOG_HOME}/$${date:yyyy-MM}/fatal-%d{yyyy-MM-dd}-%i.log&quot;&gt;            &lt;Filters&gt;                &lt;ThresholdFilter level=&quot;fatal&quot; onMatch=&quot;ACCEPT&quot;                    onMismatch=&quot;DENY&quot; /&gt;            &lt;/Filters&gt;            &lt;PatternLayout                pattern=&quot;%date{yyyy-MM-dd HH:mm:ss.SSS} %level [%thread][%file:%line] - %msg%n&quot; /&gt;            &lt;Policies&gt;                &lt;TimeBasedTriggeringPolicy /&gt;                &lt;SizeBasedTriggeringPolicy size=&quot;10 MB&quot; /&gt;            &lt;/Policies&gt;            &lt;DefaultRolloverStrategy max=&quot;20&quot; /&gt;        &lt;/RollingRandomAccessFile&gt;        &lt;!-- &lt;RollingRandomAccessFile name=&quot;MyRollingFile&quot; fileName=&quot;${LOG_HOME}/${FILE_NAME}.log&quot;            filePattern=&quot;${LOG_HOME}/$${date:yyyy-MM}/${FILE_NAME}-%d{yyyy-MM-dd HH-mm}-%i.log&quot;&gt;            &lt;PatternLayout pattern=&quot;%d{HH:mm:ss.SSS} [%t] %-5level %logger{36} - %msg%n&quot;            /&gt; &lt;Policies&gt; &lt;TimeBasedTriggeringPolicy interval=&quot;1&quot; /&gt; &lt;SizeBasedTriggeringPolicy            size=&quot;10 MB&quot; /&gt; &lt;/Policies&gt; &lt;DefaultRolloverStrategy max=&quot;20&quot; /&gt; &lt;/RollingRandomAccessFile&gt; --&gt;    &lt;/Appenders&gt;    &lt;Loggers&gt;        &lt;!-- &lt;Logger name=&quot;mylog&quot; level=&quot;trace&quot; additivity=&quot;false&quot;&gt;            &lt;AppenderRef ref=&quot;Console&quot; /&gt;            &lt;AppenderRef ref=&quot;MyFile&quot; /&gt;            &lt;AppenderRef ref=&quot;MyRollingFile&quot; /&gt;        &lt;/Logger&gt; --&gt;        &lt;Root level=&quot;trace&quot;&gt;            &lt;AppenderRef ref=&quot;Console&quot; /&gt;            &lt;AppenderRef ref=&quot;InfoFile&quot; /&gt;            &lt;AppenderRef ref=&quot;WarnFile&quot; /&gt;            &lt;AppenderRef ref=&quot;ErrorFile&quot; /&gt;            &lt;AppenderRef ref=&quot;FatalFile&quot; /&gt;        &lt;/Root&gt;    &lt;/Loggers&gt;&lt;/Configuration&gt;</code></pre><h2 id="使用过程中遇到的问题"><a href="#使用过程中遇到的问题" class="headerlink" title="使用过程中遇到的问题"></a>使用过程中遇到的问题</h2><ol><li>日志中文乱码<br>原因：日志是ASII编码的，如下：</li></ol><p><code>fetchdata.log: ASCII text</code></p><p>解决方法：<br>设置PatternLayout 属性 charset=”UTF-8”, 如下:</p><pre><code>&lt;PatternLayout charset=&quot;UTF-8&quot; pattern=&quot;%d{yyyy-MM-dd HH:mm:ss,SSS}:%4p %t (%F:%L) - %m%n&quot; /&gt;</code></pre><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ol><li><a href="https://logging.apache.org/log4j/2.x/" target="_blank" rel="noopener">Log4j 2 官网</a>: <a href="https://logging.apache.org/log4j/2.x/" target="_blank" rel="noopener">https://logging.apache.org/log4j/2.x/</a></li><li><a href="http://logging.apache.org/log4j/1.2/" target="_blank" rel="noopener">Lo4j 1 </a>: <a href="http://logging.apache.org/log4j/1.2/" target="_blank" rel="noopener">http://logging.apache.org/log4j/1.2/</a></li><li><a href="https://logging.apache.org/log4j/2.x/log4j-api/apidocs/index.html" target="_blank" rel="noopener">Log4j 2 api</a>: <a href="https://logging.apache.org/log4j/2.x/log4j-api/apidocs/index.html" target="_blank" rel="noopener">https://logging.apache.org/log4j/2.x/log4j-api/apidocs/index.html</a></li><li><a href="http://www.cnblogs.com/hafiz/p/6170702.html" target="_blank" rel="noopener">log4j2配置文件log4j2.xml</a>: <a href="http://www.cnblogs.com/hafiz/p/6170702.html" target="_blank" rel="noopener">http://www.cnblogs.com/hafiz/p/6170702.html</a></li><li><a href="http://blog.csdn.net/autfish/article/details/51203709" target="_blank" rel="noopener"><strong>详解log4j2(上)-从基础到实战</strong></a></li><li><a href="http://blog.csdn.net/autfish/article/details/51244787" target="_blank" rel="noopener"><strong>详解log4j2(下) - Async/MongoDB/Flume Appender 按日志级别区分文件输出</strong></a></li><li><a href="https://my.oschina.net/pingpangkuangmo/blog/406618" target="_blank" rel="noopener"><strong>jdk-logging、log4j、logback日志介绍及原理</strong></a></li><li><a href="https://www.cnblogs.com/hyyq/p/7171227.html" target="_blank" rel="noopener">Log4j2的日志配置文件</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> Java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Java </tag>
            
            <tag> Log4j2 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Slf4j</title>
      <link href="/2018/10/19/8-slf4j/"/>
      <url>/2018/10/19/8-slf4j/</url>
      
        <content type="html"><![CDATA[<h2 id="基本使用"><a href="#基本使用" class="headerlink" title="基本使用"></a>基本使用</h2><h3 id="使用maven创建Java工程"><a href="#使用maven创建Java工程" class="headerlink" title="使用maven创建Java工程"></a>使用maven创建Java工程</h3><p><strong>pom.xml 文件如下:</strong></p><pre><code>&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;    xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt;    &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;    &lt;groupId&gt;com.baidu.lbs&lt;/groupId&gt;    &lt;artifactId&gt;demo-slf4j&lt;/artifactId&gt;    &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt;    &lt;packaging&gt;jar&lt;/packaging&gt;    &lt;name&gt;demo-slf4j&lt;/name&gt;    &lt;url&gt;http://maven.apache.org&lt;/url&gt;    &lt;properties&gt;        &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;    &lt;/properties&gt;    &lt;dependencies&gt;        &lt;dependency&gt;            &lt;groupId&gt;junit&lt;/groupId&gt;            &lt;artifactId&gt;junit&lt;/artifactId&gt;            &lt;version&gt;4.12&lt;/version&gt;            &lt;scope&gt;test&lt;/scope&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.slf4j&lt;/groupId&gt;            &lt;artifactId&gt;slf4j-api&lt;/artifactId&gt;            &lt;version&gt;1.7.24&lt;/version&gt;        &lt;/dependency&gt;    &lt;/dependencies&gt;    &lt;build&gt;        &lt;resources&gt;            &lt;resource&gt;                &lt;directory&gt;src/main/java&lt;/directory&gt;                &lt;includes&gt;                    &lt;include&gt;log4j.properties&lt;/include&gt;                &lt;/includes&gt;            &lt;/resource&gt;        &lt;/resources&gt;        &lt;plugins&gt;            &lt;plugin&gt;                &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;                &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt;                &lt;version&gt;3.6.1&lt;/version&gt;                &lt;configuration&gt;                    &lt;source&gt;1.7&lt;/source&gt;                    &lt;target&gt;1.7&lt;/target&gt;                    &lt;encoding&gt;${project.build.sourceEncoding}&lt;/encoding&gt;                &lt;/configuration&gt;            &lt;/plugin&gt;            &lt;plugin&gt;                &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;                &lt;artifactId&gt;maven-shade-plugin&lt;/artifactId&gt;                &lt;version&gt;3.0.0&lt;/version&gt;                &lt;executions&gt;                    &lt;execution&gt;                        &lt;phase&gt;package&lt;/phase&gt;                        &lt;goals&gt;                            &lt;goal&gt;shade&lt;/goal&gt;                        &lt;/goals&gt;                        &lt;configuration&gt;                            &lt;transformers&gt;                                &lt;transformer                                    implementation=&quot;org.apache.maven.plugins.shade.resource.ManifestResourceTransformer&quot;&gt;                                    &lt;mainClass&gt;com.baidu.lbs.demo_slf4j.App&lt;/mainClass&gt;                                &lt;/transformer&gt;                            &lt;/transformers&gt;                        &lt;/configuration&gt;                    &lt;/execution&gt;                &lt;/executions&gt;            &lt;/plugin&gt;        &lt;/plugins&gt;    &lt;/build&gt;&lt;/project&gt;</code></pre><p><strong>App.java代码如下：</strong></p><pre><code>import org.slf4j.Logger;import org.slf4j.LoggerFactory;/** * Hello world! * */public class App {    public static void main(String[] args) {        Logger logger = LoggerFactory.getLogger(App.class);        logger.info(&quot;Hello, world!&quot;);    }}</code></pre><p><strong>Maven build, 然后运行结果如下：</strong></p><pre><code>SLF4J: Failed to load class &quot;org.slf4j.impl.StaticLoggerBinder&quot;.SLF4J: Defaulting to no-operation (NOP) logger implementationSLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.</code></pre><p><strong>在pom.xml中添加如下代码：</strong></p><pre><code>&lt;dependency&gt;    &lt;groupId&gt;org.slf4j&lt;/groupId&gt;    &lt;artifactId&gt;slf4j-simple&lt;/artifactId&gt;    &lt;version&gt;1.7.24&lt;/version&gt;&lt;/dependency&gt;</code></pre><p><strong>再次Maven build, 运行结果如下：</strong></p><pre><code>[main] INFO com.baidu.lbs.demo_slf4j.App - Hello, world!</code></pre><h2 id="部署时绑定日志框架"><a href="#部署时绑定日志框架" class="headerlink" title="部署时绑定日志框架"></a>部署时绑定日志框架</h2><h3 id="常用SLF4J日志框架"><a href="#常用SLF4J日志框架" class="headerlink" title="常用SLF4J日志框架"></a>常用SLF4J日志框架</h3><hr><p><strong>slf4j-log4j12-1.7.24.jar</strong></p><blockquote><p>Binding for log4j version 1.2, a widely used logging framework. You also need to place log4j.jar on your class path.</p></blockquote><hr><p><strong>slf4j-jdk14-1.7.24.jar</strong></p><blockquote><p>Binding for java.util.logging, also referred to as JDK 1.4 logging</p></blockquote><p>===<br><strong>slf4j-nop-1.7.24.jar</strong></p><blockquote><p>Binding for NOP, silently discarding all logging.</p></blockquote><p>===<br><strong>slf4j-simple-1.7.24.jar</strong></p><blockquote><p>Binding for Simple implementation, which outputs all events to System.err. Only messages of level INFO and higher are printed. This binding may be useful in the context of small applications.</p></blockquote><p>===<br><strong>slf4j-jcl-1.7.24.jar</strong></p><blockquote><p>Binding for Jakarta Commons Logging. This binding will delegate all SLF4J logging to JCL.</p></blockquote><p>===</p><p><strong>logback-classic-1.0.13.jar (requires logback-core-1.0.13.jar)</strong></p><blockquote><p><strong>NATIVE IMPLEMENTATION</strong> There are also SLF4J bindings external to the SLF4J project, e.g. logback which implements SLF4J natively. Logback’s ch.qos.logback.classic.Logger class is a direct implementation of SLF4J’s org.slf4j.Logger interface. Thus, using SLF4J in conjunction with logback involves strictly zero memory and computational overhead.</p></blockquote><h3 id="SLF4J-绑定图"><a href="#SLF4J-绑定图" class="headerlink" title="SLF4J 绑定图"></a>SLF4J 绑定图</h3><p><img src="https://www.slf4j.org/images/concrete-bindings.png" alt="slf4j 绑定图"></p><blockquote><p><strong>注：新手强烈推荐看参考3</strong>.</p></blockquote><h2 id="SLF4j-与jcl-log4j1-log4j2-logback的集成"><a href="#SLF4j-与jcl-log4j1-log4j2-logback的集成" class="headerlink" title="SLF4j 与jcl, log4j1, log4j2, logback的集成"></a>SLF4j 与jcl, log4j1, log4j2, logback的集成</h2><blockquote><p>注: 详细可以参看参考4，作者详细的介绍了如何集成以及集成的原理。</p></blockquote><p>##参考</p><ol><li><a href="https://www.slf4j.org/index.html" target="_blank" rel="noopener">官网</a>: <a href="https://www.slf4j.org/index.html" target="_blank" rel="noopener">https://www.slf4j.org/index.html</a></li><li><a href="https://www.slf4j.org/docs.html" target="_blank" rel="noopener">官网文档</a>: <a href="https://www.slf4j.org/docs.html" target="_blank" rel="noopener">https://www.slf4j.org/docs.html</a></li><li><a href="https://my.oschina.net/pingpangkuangmo/blog/408382" target="_blank" rel="noopener"><strong>slf4j与jul、log4j1、log4j2、logback的集成原理</strong></a></li><li><a href="https://my.oschina.net/pingpangkuangmo/blog/410224" target="_blank" rel="noopener"><strong>slf4j、jcl、jul、log4j1、log4j2、logback大总结</strong></a></li></ol>]]></content>
      
      
      <categories>
          
          <category> Java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Java </tag>
            
            <tag> Slf4j </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MongoDB (三) 常用操作</title>
      <link href="/2018/10/18/7-mongodb-common-operations/"/>
      <url>/2018/10/18/7-mongodb-common-operations/</url>
      
        <content type="html"><![CDATA[<h2 id="开启MongoDB查询日志"><a href="#开启MongoDB查询日志" class="headerlink" title="开启MongoDB查询日志"></a>开启MongoDB查询日志</h2><ol><li>指定配置文件启动</li></ol><p>/home/map/.jumbo/etc/mongo.conf</p><pre><code>port=8017dbpath=/home/map/.jumbo/var/lib/mongo/datalogpath=/home/map/.jumbo/var/log/mongodb.loglogappend=trueprofile=2slowms=1</code></pre><p>启动：/home/map/.jumbo/opt/mongodb/bin/mongod -f /home/map/.jumbo/etc/mongo.conf &amp;</p><ol start="2"><li>指定启动参数</li></ol><p>mongod –profile=2 –slowms=1</p><p>大于5ms的慢查询都会打印出来</p><p>在Mongo命令行中输入插叙：</p><pre><code>&gt; db.getProfilingStatus();{ &quot;was&quot; : 2, &quot;slowms&quot; : 5 }</code></pre><pre><code># 查看索引是否存在&gt; db.boundary.ensureIndex({ &quot;geometry&quot; : &quot;2dsphere&quot; } ){    &quot;createdCollectionAutomatically&quot; : false,    &quot;numIndexesBefore&quot; : 2,    &quot;numIndexesAfter&quot; : 2,    &quot;note&quot; : &quot;all indexes already exist&quot;,    &quot;ok&quot; : 1}# 删除索引&gt; db.boundary.dropIndex({ &quot;geometry&quot; : &quot;2dsphere&quot; }){ &quot;nIndexesWas&quot; : 2, &quot;ok&quot; : 1 }# 查询某一字段是否存在&gt; db.boundary.find( { &quot;geometry&quot;: { $exists: true } } ).count()54# 删除表中的某一字段&gt; db.boundary.update( {}, {$unset: {&quot;geometry&quot;:&quot;&quot;} }, {multi: true} )WriteResult({ &quot;nMatched&quot; : 1971, &quot;nUpserted&quot; : 0, &quot;nModified&quot; : 54 })# 创建索引db.boundary.createIndex({ &quot;geometry&quot;: &quot;2dsphere&quot; })</code></pre>]]></content>
      
      
      <categories>
          
          <category> DataBase </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NoSQL </tag>
            
            <tag> MongoDB </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Java 日志框架</title>
      <link href="/2018/10/18/6-java-log/"/>
      <url>/2018/10/18/6-java-log/</url>
      
        <content type="html"><![CDATA[<h2 id="常用日志框架"><a href="#常用日志框架" class="headerlink" title="常用日志框架"></a>常用日志框架</h2><ul><li>logging</li><li>log4j1</li><li>log4j2</li><li>logback</li></ul><blockquote><p>注：重点参看参考1.</p></blockquote><ol><li><a href="https://my.oschina.net/pingpangkuangmo/blog/406618" target="_blank" rel="noopener">jdk-logging、log4j、logback日志介绍及原理</a></li><li><a href="http://blog.csdn.net/qingkangxu/article/details/7514770" target="_blank" rel="noopener">JDK Logging深入分析</a></li></ol><h2 id="各种jar包总结"><a href="#各种jar包总结" class="headerlink" title="各种jar包总结"></a>各种jar包总结</h2><h3 id="log4j1"><a href="#log4j1" class="headerlink" title="log4j1"></a>log4j1</h3><ul><li>log4j：log4j1的全部内容</li></ul><h3 id="log4j2"><a href="#log4j2" class="headerlink" title="log4j2"></a>log4j2</h3><ul><li>log4j-api:log4j2定义的API</li><li>log4j-core:log4j2上述API的实现</li></ul><h3 id="logback"><a href="#logback" class="headerlink" title="logback"></a>logback</h3><ul><li>logback-core:logback的核心包</li><li>logback-classic：logback实现了slf4j的API</li></ul><h3 id="common-logging"><a href="#common-logging" class="headerlink" title="common-logging"></a>common-logging</h3><ul><li>commons-logging:commons-logging的原生全部内容</li><li>log4j-jcl:commons-logging到log4j2的桥梁</li><li>jcl-over-slf4j：commons-logging到slf4j的桥梁</li></ul><h3 id="slf4j转向某个实际的日志框架"><a href="#slf4j转向某个实际的日志框架" class="headerlink" title="slf4j转向某个实际的日志框架"></a>slf4j转向某个实际的日志框架</h3><p>场景介绍：如 使用slf4j的API进行编程，底层想使用log4j1来进行实际的日志输出，这就是slf4j-log4j12干的事。</p><ul><li>slf4j-jdk14：slf4j到jdk-logging的桥梁</li><li>slf4j-log4j12：slf4j到log4j1的桥梁</li><li>log4j-slf4j-impl：slf4j到log4j2的桥梁</li><li>logback-classic：slf4j到logback的桥梁</li><li>slf4j-jcl：slf4j到commons-logging的桥梁</li></ul><h3 id="某个实际的日志框架转向slf4j"><a href="#某个实际的日志框架转向slf4j" class="headerlink" title="某个实际的日志框架转向slf4j"></a>某个实际的日志框架转向slf4j</h3><p>场景介绍：如 使用log4j1的API进行编程，但是想最终通过logback来进行输出，所以就需要先将log4j1的日志输出转交给slf4j来输出，slf4j再交给logback来输出。将log4j1的输出转给slf4j，这就是log4j-over-slf4j做的事</p><p>这一部分主要用来进行实际的日志框架之间的切换</p><ul><li>jul-to-slf4j：jdk-logging到slf4j的桥梁</li><li>log4j-over-slf4j：log4j1到slf4j的桥梁</li><li>jcl-over-slf4j：commons-logging到slf4j的桥梁</li></ul><h2 id="集成总结"><a href="#集成总结" class="headerlink" title="集成总结"></a>集成总结</h2><h3 id="commons-logging与其他日志框架集成"><a href="#commons-logging与其他日志框架集成" class="headerlink" title="commons-logging与其他日志框架集成"></a>commons-logging与其他日志框架集成</h3><h3 id="slf4j与其他日志框架集成"><a href="#slf4j与其他日志框架集成" class="headerlink" title="slf4j与其他日志框架集成"></a>slf4j与其他日志框架集成</h3><h4 id="slf4j与jdk-logging集成"><a href="#slf4j与jdk-logging集成" class="headerlink" title="slf4j与jdk-logging集成"></a>slf4j与jdk-logging集成</h4><p>需要的jar包：</p><ul><li>slf4j-api</li><li>slf4j-jdk14(集成包)</li></ul><h4 id="slf4j与log4j1集成"><a href="#slf4j与log4j1集成" class="headerlink" title="slf4j与log4j1集成"></a>slf4j与log4j1集成</h4><p>需要的jar包：</p><ul><li>slf4j-api</li><li>log4j</li><li>slf4j-log4j12(集成包)</li></ul><h4 id="slf4j与log4j2集成："><a href="#slf4j与log4j2集成：" class="headerlink" title="slf4j与log4j2集成："></a>slf4j与log4j2集成：</h4><p>需要的jar包：</p><ul><li>slf4j-api</li><li>log4j-api</li><li>log4j-core</li><li>log4j-slf4j-impl(集成包)</li></ul><h5 id="slf4j与logback集成"><a href="#slf4j与logback集成" class="headerlink" title="slf4j与logback集成"></a>slf4j与logback集成</h5><p>需要的jar包：</p><ul><li>slf4j-api</li><li>logback-core</li><li>logback-classic(集成包)</li></ul><h4 id="slf4j与commons-logging集成"><a href="#slf4j与commons-logging集成" class="headerlink" title="slf4j与commons-logging集成"></a>slf4j与commons-logging集成</h4><p>需要的jar包：</p><ul><li>slf4j-api</li><li>commons-logging</li><li>slf4j-jcl(集成包)</li></ul><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ol><li><a href="https://my.oschina.net/pingpangkuangmo/blog/410224" target="_blank" rel="noopener">slf4j、jcl、jul、log4j1、log4j2、logback大总结</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> Java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Java </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MongoDB (二) Shell 命令</title>
      <link href="/2018/10/16/5-mongodb-shell/"/>
      <url>/2018/10/16/5-mongodb-shell/</url>
      
        <content type="html"><![CDATA[<h2 id="创建数据库"><a href="#创建数据库" class="headerlink" title="创建数据库"></a>创建数据库</h2><p>MongoDB 创建数据库的语法格式如下：</p><pre><code>use DATABASE_NAME</code></pre><p>如果数据库不存在，则创建数据库，否则切换到指定数据库。</p><p>如果你想查看所有数据库，可以使用 show dbs 命令：</p><pre><code>show dbs</code></pre><h2 id="删除数据库"><a href="#删除数据库" class="headerlink" title="删除数据库"></a>删除数据库</h2><p>MongoDB 删除数据库的语法格式如下：</p><pre><code>db.dropDatabase()</code></pre><p>删除当前数据库，默认为 test，你可以使用 db 命令查看当前数据库名。</p><h2 id="删除集合"><a href="#删除集合" class="headerlink" title="删除集合"></a>删除集合</h2><p>集合删除语法格式如下：</p><pre><code>db.collection.drop()</code></pre><h2 id="插入文档"><a href="#插入文档" class="headerlink" title="插入文档"></a>插入文档</h2><p>文档的数据结构和JSON基本一样。<br>所有存储在集合中的数据都是BSON格式。<br>BSON是一种类json的一种二进制形式的存储格式,简称Binary JSON。</p><p>MongoDB 使用 insert() 或 save() 方法向集合中插入文档，语法如下：</p><pre><code>db.COLLECTION_NAME.insert(document)</code></pre><p>示例：</p><pre><code>db.col.insert({&quot;title&quot;: &quot;个人主页&quot;,&quot;description&quot;: &quot;zhongchun的主页&quot;,&quot;by&quot;: &quot;zhongchun&quot;,&quot;url&quot;: &quot;http://yuzhongchun.com&quot;,&quot;tags&quot;: [&#39;mongodb&#39;, &#39;database&#39;, &#39;NoSQL&#39;],&quot;likes&quot;: 100})</code></pre><pre><code>document=({&quot;title&quot;: &quot;个人主页&quot;, &quot;description&quot;: &quot;zhongchun的主页&quot;, &quot;by&quot;: &quot;zhongchun&quot;, &quot;url&quot;: &quot;http://yuzhongchun.com&quot;, &quot;tags&quot;: [&#39;mongodb&#39;, &#39;database&#39;, &#39;NoSQL&#39;], likes: 100})// 执行后如下{    &quot;title&quot; : &quot;个人主页&quot;,    &quot;description&quot; : &quot;zhongchun的主页&quot;,    &quot;by&quot; : &quot;zhongchun&quot;,    &quot;url&quot; : &quot;http://yuzhongchun.com&quot;,    &quot;tags&quot; : [        &quot;mongodb&quot;,        &quot;database&quot;,        &quot;NoSQL&quot;    ],    &quot;likes&quot; : 100}// 执行插入操作db.col.insert(document)WriteResult({ &quot;nInserted&quot; : 1 })</code></pre><p>插入文档也可以使用 db.col.save(document) 命令。如果不指定 _id 字段 save() 方法类似于 insert() 方法。如果指定 _id 字段，则会更新该 _id 的数据。</p><h2 id="更新文档"><a href="#更新文档" class="headerlink" title="更新文档"></a>更新文档</h2><p>MongoDB 使用 update() 和 save() 方法来更新集合中的文档。接下来让我们详细来看下两个函数的应用及其区别。</p><h3 id="update-方法"><a href="#update-方法" class="headerlink" title="update() 方法"></a>update() 方法</h3><p>语法格式如下：</p><pre><code>db.collection.update(   &lt;query&gt;,   &lt;update&gt;,   {     upsert: &lt;boolean&gt;,     multi: &lt;boolean&gt;,     writeConcern: &lt;document&gt;   })</code></pre><p>参数说明：</p><pre><code>query : update的查询条件，类似sql update查询内where后面的。update : update的对象和一些更新的操作符（如$,$inc...）等，也可以理解为sql update查询内set后面的upsert : 可选，这个参数的意思是，如果不存在update的记录，是否插入objNew,true为插入，默认是false，不插入。multi : 可选，mongodb 默认是false,只更新找到的第一条记录，如果这个参数为true,就把按条件查出来多条记录全部更新。writeConcern :可选，抛出异常的级别。</code></pre><p>示例：</p><pre><code>db.col.update({&#39;title&#39;: &quot;个人主页&quot;}, {$set:{&#39;title&#39;: &quot;BerMaker&quot;}})</code></pre><h3 id="save-方法"><a href="#save-方法" class="headerlink" title="save() 方法"></a>save() 方法</h3><p>语法格式如下：</p><pre><code>db.collection.save(   &lt;document&gt;,   {     writeConcern: &lt;document&gt;   })</code></pre><p>参数说明：</p><pre><code>document : 文档数据。writeConcern :可选，抛出异常的级别。</code></pre><p>示例：</p><pre><code>db.col.save({&quot;_id&quot; : ObjectId(&quot;59a6a3d4963d665550f6e2d2&quot;), &quot;title&quot; : &quot;不涸&quot;, &quot;description&quot;: &quot;zhongchun的主页&quot;})</code></pre><p>运行结果：</p><pre><code>&gt; db.col.find().pretty(){    &quot;_id&quot; : ObjectId(&quot;59a6a3d4963d665550f6e2d2&quot;),    &quot;title&quot; : &quot;不涸&quot;,    &quot;description&quot; : &quot;zhongchun的主页&quot;}</code></pre><h2 id="删除文档"><a href="#删除文档" class="headerlink" title="删除文档"></a>删除文档</h2><h2 id="备份与还原"><a href="#备份与还原" class="headerlink" title="备份与还原"></a>备份与还原</h2><pre><code>mongodump -h 10.94.241.53 --port 8017 -d boundary_tool -o /home/map/data/mongo/201708301810</code></pre><pre><code>mongorestore -h 10.101.44.169 --port 27017 -d boundary_tool --drop /home/wuzhibin/yuzhongchun/data/mongo/boundary_tool</code></pre><p>drop 表示先删除所有的记录，然后恢复。</p><h2 id="单表导出与导入"><a href="#单表导出与导入" class="headerlink" title="单表导出与导入"></a>单表导出与导入</h2><p>导出</p><pre class=" language-sh"><code class="language-sh">mongoexport --host 10.206.210.30 --port 27019 --db boundary_tool --collection boundary --out boundary.json</code></pre><p>导入</p><pre class=" language-sh"><code class="language-sh">mongoimport --host 127.0.0.1 --port 8017 --db boundary_tool --collection boundary --file boundary.json</code></pre><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ol><li><a href="http://www.jb51.net/article/52498.htm" target="_blank" rel="noopener">mongodb 数据库操作–备份 还原 导出 导入</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> DataBase </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NoSQL </tag>
            
            <tag> MongoDB </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MongoDB (一) 基本</title>
      <link href="/2018/10/15/4-mongodb-basic/"/>
      <url>/2018/10/15/4-mongodb-basic/</url>
      
        <content type="html"><![CDATA[<h2 id="MongoDB是是什么"><a href="#MongoDB是是什么" class="headerlink" title="MongoDB是是什么"></a>MongoDB是是什么</h2><p>MongoDB 是由C++语言编写的，是一个基于分布式文件存储的开源数据库系统。<br>在高负载的情况下，添加更多的节点，可以保证服务器性能。<br>MongoDB 旨在为WEB应用提供可扩展的高性能数据存储解决方案。<br>MongoDB 将数据存储为一个文档，数据结构由键值(key=&gt;value)对组成。MongoDB 文档类似于 JSON 对象。字段值可以包含其他文档，数组及文档数组。<br><img src="http://www.runoob.com/wp-content/uploads/2013/10/crud-annotated-document.png" alt="mongodb"></p><h2 id="MongoDB-特点"><a href="#MongoDB-特点" class="headerlink" title="MongoDB 特点"></a>MongoDB 特点</h2><ul><li>MongoDB的提供了一个面向文档存储，操作起来比较简单和容易。</li><li>你可以在MongoDB记录中设置任何属性的索引 (如：FirstName=”Sameer”,Address=”8 Gandhi Road”)来实现更快的排序。</li><li>你可以通过本地或者网络创建数据镜像，这使得MongoDB有更强的扩展性。</li><li>如果负载的增加（需要更多的存储空间和更强的处理能力） ，它可以分布在计算机网络中的其他节点上这就是所谓的分片。</li><li>Mongo支持丰富的查询表达式。查询指令使用JSON形式的标记，可轻易查询文档中内嵌的对象及数组。</li><li>MongoDb 使用update()命令可以实现替换完成的文档（数据）或者一些指定的数据字段 。</li><li>Mongodb中的Map/reduce主要是用来对数据进行批量处理和聚合操作。</li><li>Map和Reduce。Map函数调用emit(key,value)遍历集合中所有的记录，将key与value传给Reduce函数进行处理。</li><li>Map函数和Reduce函数是使用Javascript编写的，并可以通过db.runCommand或mapreduce命令来执行MapReduce操作。</li><li>GridFS是MongoDB中的一个内置功能，可以用于存放大量小文件。</li><li>MongoDB允许在服务端执行脚本，可以用Javascript编写某个函数，直接在服务端执行，也可以把函数的定义存储在服务端，下次直接调用即可。</li><li>MongoDB支持各种编程语言:RUBY，PYTHON，JAVA，C++，PHP，C#等多种语言。</li><li>MongoDB安装简单。</li></ul><h2 id="MongoDB-概念解析"><a href="#MongoDB-概念解析" class="headerlink" title="MongoDB 概念解析"></a>MongoDB 概念解析</h2><table><thead><tr><th>RDBMS术语</th><th>MongoDB术语</th><th>解释</th></tr></thead><tbody><tr><td>database</td><td>database</td><td>数据库</td></tr><tr><td>table</td><td>collection</td><td>数据库表/集合</td></tr><tr><td>row</td><td>document</td><td>数据记录hang/文档</td></tr><tr><td>column</td><td>field</td><td>数据字段/域</td></tr><tr><td>index</td><td>index</td><td>索引</td></tr><tr><td>tables joins</td><td></td><td>表连接/MongoDB不支持</td></tr><tr><td>primary key</td><td>primary key</td><td>主键/MongoDB自动将_id字段设置为主键</td></tr></tbody></table><p><img src="http://www.runoob.com/wp-content/uploads/2013/10/Figure-1-Mapping-Table-to-Collection-1.png" alt="传统数据库表与MongoDB对比"></p><h2 id="数据库"><a href="#数据库" class="headerlink" title="数据库"></a>数据库</h2><pre><code>// 显示所有数据的列表show dbs// 显示当前数据库对象或集合db// 运行 use 命令，连接到一个指定数据库</code></pre><h2 id="文档"><a href="#文档" class="headerlink" title="文档"></a>文档</h2><p>文档是一组键值(key-value)对(即BSON)。MongoDB 的文档不需要设置相同的字段，并且相同的字段不需要相同的数据类型，这与关系型数据库有很大的区别，也是 MongoDB 非常突出的特点。</p><h2 id="集合"><a href="#集合" class="headerlink" title="集合"></a>集合</h2><p>集合就是 MongoDB 文档组，类似于 RDBMS （关系数据库管理系统：Relational Database Management System)中的表格。<br>集合存在于数据库中，集合没有固定的结构，这意味着你在对集合可以插入不同格式和类型的数据，但通常情况下我们插入集合的数据都会有一定的关联性。</p><h2 id="元数据"><a href="#元数据" class="headerlink" title="元数据"></a>元数据</h2><p>数据库的信息是存储在集合中。它们使用了系统的命名空间：</p><p><code>dbname.system.*</code></p><h2 id="MongoDB-数据类型"><a href="#MongoDB-数据类型" class="headerlink" title="MongoDB 数据类型"></a>MongoDB 数据类型</h2><table><thead><tr><th>数据类型</th><th>描述</th></tr></thead><tbody><tr><td>String</td><td>字符串。存储数据常用的数据类型。在 MongoDB 中，UTF-8 编码的字符串才是合法的。</td></tr><tr><td>Integer</td><td>整型数值。用于存储数值。根据你所采用的服务器，可分为 32 位或 64 位。</td></tr><tr><td>Boolean</td><td>布尔值。用于存储布尔值（真/假）。</td></tr><tr><td>Double</td><td>双精度浮点值。用于存储浮点值。</td></tr><tr><td>Min/Max keys</td><td>将一个值与 BSON（二进制的 JSON）元素的最低值和最高值相对比。</td></tr><tr><td>Arrays</td><td>用于将数组或列表或多个值存储为一个键。</td></tr><tr><td>Timestamp</td><td>时间戳。记录文档修改或添加的具体时间。</td></tr><tr><td>Object</td><td>用于内嵌文档。</td></tr><tr><td>Null</td><td>用于创建空值。</td></tr><tr><td>Symbol</td><td>符号。该数据类型基本上等同于字符串类型，但不同的是，它一般用于采用特殊符号类型的语言。</td></tr><tr><td>Date</td><td>日期时间。用 UNIX 时间格式来存储当前日期或时间。你可以指定自己的日期时间：创建 Date 对象，传入年月日信息。</td></tr><tr><td>Object ID</td><td>对象 ID。用于创建文档的 ID。</td></tr><tr><td>Binary Data</td><td>二进制数据。用于存储二进制数据。</td></tr><tr><td>Code</td><td>代码类型。用于在文档中存储 JavaScript 代码。</td></tr><tr><td>Regular expression</td><td>正则表达式类型。用于存储正则表达式。</td></tr></tbody></table>]]></content>
      
      
      <categories>
          
          <category> DataBase </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NoSQL </tag>
            
            <tag> MongoDB </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Kafka 使用</title>
      <link href="/2018/10/14/3-kafka-basic-use/"/>
      <url>/2018/10/14/3-kafka-basic-use/</url>
      
        <content type="html"><![CDATA[<p>Kafka是由LinkedIn开发的一个分布式的消息系统，使用Scala编写，它以可水平扩展和高吞吐率而被广泛使用。目前越来越多的开源分布式处理系统如Cloudera、Apache Storm、Spark都支持与Kafka集成。</p><h2 id="Kafka拓扑结构"><a href="#Kafka拓扑结构" class="headerlink" title="Kafka拓扑结构"></a>Kafka拓扑结构</h2><p><img src="https://ws4.sinaimg.cn/large/006tKfTcly1g0g5yg3pw3j310q0kptbd.jpg" alt="kafka拓扑结构"></p><h2 id="安装和启动"><a href="#安装和启动" class="headerlink" title="安装和启动"></a>安装和启动</h2><h3 id="Download"><a href="#Download" class="headerlink" title="Download"></a>Download</h3><p>Download the 0.10.2.1 release and un-tar it.</p><pre><code>tar -xzf kafka_2.11-0.10.2.1.tgzcd kafka_2.11-0.10.2.1</code></pre><h3 id="Start-the-server"><a href="#Start-the-server" class="headerlink" title="Start the server"></a>Start the server</h3><p>Kafka uses ZooKeeper so you need to first start a ZooKeeper server if you don’t already have one. You can use the convenience script packaged with kafka to get a quick-and-dirty single-node ZooKeeper instance.</p><pre><code>bin/zookeeper-server-start.sh config/zookeeper.properties</code></pre><p>Now start the Kafka server:</p><pre><code>bin/kafka-server-start.sh config/server.properties</code></pre><h3 id="Create-a-topic"><a href="#Create-a-topic" class="headerlink" title="Create a topic"></a>Create a topic</h3><pre><code>bin/kafka-topics.sh --create --zookeeper localhost:8181 --replication-factor 1 --partitions 1 --topic test</code></pre><p>查看topic</p><pre><code>bin/kafka-topics.sh --list --zookeeper localhost:8181test</code></pre><h3 id="Send-some-messages"><a href="#Send-some-messages" class="headerlink" title="Send some messages"></a>Send some messages</h3><pre><code>bin/kafka-console-producer.sh --broker-list localhost:8092 --topic test</code></pre><h3 id="Start-a-consumer"><a href="#Start-a-consumer" class="headerlink" title="Start a consumer"></a>Start a consumer</h3><pre><code>bin/kafka-console-consumer.sh --bootstrap-server localhost:8092 --topic test --from-beginning</code></pre><h3 id="common-commands"><a href="#common-commands" class="headerlink" title="common commands"></a>common commands</h3><ol><li><p>list topic command</p><pre><code>bin/kafka-topics.sh --list --zookeeper localhost:8181</code></pre></li><li><p>查询topic</p><pre><code>bin/kafka-topics.sh --describe --zookeeper localhost:8181 --topic my-replicated-topicTopic:my-replicated-topic    PartitionCount:1    ReplicationFactor:3    Configs: Topic: my-replicated-topic    Partition: 0    Leader: 0    Replicas: 0,2,1    Isr: 0</code></pre></li></ol><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ol><li><a href="http://www.cnblogs.com/cyfonly/p/5954614.html" target="_blank" rel="noopener">kafka学习笔记：知识点整理</a></li><li><a href="http://www.aboutyun.com/thread-12882-1-1.html" target="_blank" rel="noopener">Kafka入门经典教程</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> BigData </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Tools </tag>
            
            <tag> ELK </tag>
            
            <tag> Kafka </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Logstash (二) 使用</title>
      <link href="/2018/10/13/2-logstash-basic-use/"/>
      <url>/2018/10/13/2-logstash-basic-use/</url>
      
        <content type="html"><![CDATA[<h2 id="基本用法"><a href="#基本用法" class="headerlink" title="基本用法"></a>基本用法</h2><h3 id="测试配置文件是否在安装路径"><a href="#测试配置文件是否在安装路径" class="headerlink" title="测试配置文件是否在安装路径"></a>测试配置文件是否在安装路径</h3><pre><code>./filebeat -configtest -e</code></pre><p>如下输入：</p><pre><code>2017/06/23 10:04:00.515550 beat.go:285: INFO Home path: [/home/map/tools/filebeat-5.4.1-linux-x86_64] Config path: [/home/map/tools/filebeat-5.4.1-linux-x86_64] Data path: [/home/map/tools/filebeat-5.4.1-linux-x86_64/data] Logs path: [/home/map/tools/filebeat-5.4.1-linux-x86_64/logs]2017/06/23 10:04:00.515612 beat.go:186: INFO Setup Beat: filebeat; Version: 5.4.12017/06/23 10:04:00.515722 logstash.go:90: INFO Max Retries set to: 32017/06/23 10:04:00.515784 metrics.go:23: INFO Metrics logging every 10s2017/06/23 10:04:00.515871 outputs.go:108: INFO Activated logstash as output plugin.2017/06/23 10:04:00.516005 publish.go:295: INFO Publisher name: cp01-map-2016-52.epc.baidu.com2017/06/23 10:04:00.516164 async.go:63: INFO Flush Interval set to: 1s2017/06/23 10:04:00.516180 async.go:64: INFO Max Bulk Size set to: 2048Config OK</code></pre><h3 id="简单的例子"><a href="#简单的例子" class="headerlink" title="简单的例子"></a>简单的例子</h3><pre><code>cd logstash-5.4.2bin/logstash -e &#39;input { stdin { } } output { stdout {} }&#39;</code></pre><h3 id="从文件中读取数据"><a href="#从文件中读取数据" class="headerlink" title="从文件中读取数据"></a>从文件中读取数据</h3><p>1) 写 taxi-pipeline.conf, 如下</p><pre><code>input {    file {        path =&gt; &quot;/home/map/data/taxi.txt&quot;    }}filter {}output {    stdout {    }}</code></pre><p>2) 测试配置文件是否OK</p><pre><code>map@cp01-map-2016-52.epc.baidu.com ~/tools/logstash-5.4.1 16:34:42 $bin/logstash -f taxi-pipeline.conf --config.test_and_exitSending Logstash&#39;s logs to /home/map/tools/logstash-5.4.1/logs which is now configured via log4j2.propertiesConfiguration OK[2017-06-22T16:35:01,525][INFO ][logstash.runner          ] Using config.test_and_exit mode. Config Validation Result: OK. Exiting Logstash</code></pre><p>The –config.test_and_exit option parses your configuration file and reports any errors.</p><p>3) 运行</p><pre><code>bin/logstash -f taxi-pipeline.conf --config.reload.automatic</code></pre><h3 id="使用Filebeat搜集日志"><a href="#使用Filebeat搜集日志" class="headerlink" title="使用Filebeat搜集日志"></a>使用Filebeat搜集日志</h3><p>配置filebeat.yml文件</p><p>启动服务</p><pre><code>./filebeat -e -c filebeat.yml -d &quot;publish&quot;</code></pre><h3 id="修改Filebeat检测log的时间"><a href="#修改Filebeat检测log的时间" class="headerlink" title="修改Filebeat检测log的时间"></a>修改Filebeat检测log的时间</h3><pre><code>scan_frequency: 3s</code></pre><h2 id="Filebeat"><a href="#Filebeat" class="headerlink" title="Filebeat"></a>Filebeat</h2><h3 id="Command-Line-Options"><a href="#Command-Line-Options" class="headerlink" title="Command Line Options"></a>Command Line Options</h3><p><a href="https://www.elastic.co/guide/en/beats/filebeat/current/command-line-options.html" target="_blank" rel="noopener">https://www.elastic.co/guide/en/beats/filebeat/current/command-line-options.html</a></p><h3 id="How-Filebeat-Works"><a href="#How-Filebeat-Works" class="headerlink" title="How Filebeat Works"></a>How Filebeat Works</h3><p><a href="https://www.elastic.co/guide/en/beats/filebeat/current/how-filebeat-works.html" target="_blank" rel="noopener">https://www.elastic.co/guide/en/beats/filebeat/current/how-filebeat-works.html</a></p><p>Filebeat consists of two main components: <strong>==prospectors==</strong> and <strong>==harvesters==</strong>. These components work together to tail files and send event data to the output that you specify.</p><h3 id="Configuring-Filebeat"><a href="#Configuring-Filebeat" class="headerlink" title="Configuring Filebeat"></a>Configuring Filebeat</h3><p><a href="https://www.elastic.co/guide/en/beats/filebeat/current/configuring-howto-filebeat.html" target="_blank" rel="noopener">https://www.elastic.co/guide/en/beats/filebeat/current/configuring-howto-filebeat.html</a></p><p><a href="https://www.elastic.co/guide/en/beats/libbeat/5.4/config-file-format.html" target="_blank" rel="noopener">Configure File Format</a><br><a href="https://www.elastic.co/guide/en/beats/filebeat/current/filebeat-configuration-details.html" target="_blank" rel="noopener">Configuration Options (Reference)</a></p><h2 id="Logstash"><a href="#Logstash" class="headerlink" title="Logstash"></a>Logstash</h2><p><a href="https://www.elastic.co/guide/en/logstash/5.4/monitoring.html" target="_blank" rel="noopener">Monitoring APIs</a></p><h3 id="Retrieve-general-information-about-the-Logstash-instance"><a href="#Retrieve-general-information-about-the-Logstash-instance" class="headerlink" title="Retrieve general information about the Logstash instance"></a>Retrieve general information about the Logstash instance</h3><pre><code>curl -XGET &#39;localhost:9600/?pretty&#39;{  &quot;host&quot; : &quot;cp01-map-2016-52.epc.baidu.com&quot;,  &quot;version&quot; : &quot;5.4.1&quot;,  &quot;http_address&quot; : &quot;127.0.0.1:9600&quot;,  &quot;id&quot; : &quot;b5d32f3d-903c-42f8-a59c-0f0567db1c12&quot;,  &quot;name&quot; : &quot;cp01-map-2016-52.epc.baidu.com&quot;,  &quot;build_date&quot; : &quot;2017-05-29T16:40:20Z&quot;,  &quot;build_sha&quot; : &quot;cf39b7a82225994a0a3e716021c66f7a45fae46c&quot;,  &quot;build_snapshot&quot; : false}</code></pre><h3 id="Retrieve-information-about-the-node"><a href="#Retrieve-information-about-the-node" class="headerlink" title="Retrieve information about the node"></a>Retrieve information about the node</h3><pre><code>curl -XGET &#39;localhost:9600/_node?pretty&#39;{  &quot;host&quot; : &quot;cp01-map-2016-52.epc.baidu.com&quot;,  &quot;version&quot; : &quot;5.4.1&quot;,  &quot;http_address&quot; : &quot;127.0.0.1:9600&quot;,  &quot;id&quot; : &quot;b5d32f3d-903c-42f8-a59c-0f0567db1c12&quot;,  &quot;name&quot; : &quot;cp01-map-2016-52.epc.baidu.com&quot;,  &quot;pipeline&quot; : {    &quot;workers&quot; : 12,    &quot;batch_size&quot; : 125,    &quot;batch_delay&quot; : 5,    &quot;config_reload_automatic&quot; : true,    &quot;config_reload_interval&quot; : 3,    &quot;id&quot; : &quot;main&quot;  },  &quot;os&quot; : {    &quot;name&quot; : &quot;Linux&quot;,    &quot;arch&quot; : &quot;amd64&quot;,    &quot;version&quot; : &quot;2.6.32_1-16-0-0_virtio&quot;,    &quot;available_processors&quot; : 12  },  &quot;jvm&quot; : {    &quot;pid&quot; : 6909,    &quot;version&quot; : &quot;1.8.0_111&quot;,    &quot;vm_name&quot; : &quot;Java HotSpot(TM) 64-Bit Server VM&quot;,    &quot;vm_version&quot; : &quot;1.8.0_111&quot;,    &quot;vm_vendor&quot; : &quot;Oracle Corporation&quot;,    &quot;start_time_in_millis&quot; : 1498462270980,    &quot;mem&quot; : {      &quot;heap_init_in_bytes&quot; : 268435456,      &quot;heap_max_in_bytes&quot; : 1037959168,      &quot;non_heap_init_in_bytes&quot; : 2555904,      &quot;non_heap_max_in_bytes&quot; : 0    },    &quot;gc_collectors&quot; : [ &quot;ParNew&quot;, &quot;ConcurrentMarkSweep&quot; ]  }}</code></pre><h3 id="Plugins-Info-API"><a href="#Plugins-Info-API" class="headerlink" title="Plugins Info API"></a>Plugins Info API</h3><pre><code>curl -XGET &#39;localhost:9600/_node/plugins?pretty&#39;</code></pre><h3 id="Node-Stats-API"><a href="#Node-Stats-API" class="headerlink" title="Node Stats API"></a>Node Stats API</h3><pre><code>curl -XGET &#39;localhost:9600/_node/stats/&lt;types&gt;&#39;curl -XGET &#39;localhost:9600/_node/stats/jvm?pretty&#39;curl -XGET &#39;localhost:9600/_node/stats/pipeline?pretty&#39;curl -XGET &#39;localhost:9600/_node/stats/os?pretty&#39;</code></pre><h3 id="Hot-Threads-API"><a href="#Hot-Threads-API" class="headerlink" title="Hot Threads API"></a>Hot Threads API</h3><pre><code>curl -XGET &#39;localhost:9600/_node/hot_threads?pretty&#39;</code></pre><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ol><li><a href="https://www.elastic.co/guide/en/logstash/current/index.html" target="_blank" rel="noopener">Logstash Reference</a></li><li><a href="https://www.elastic.co/guide/en/beats/filebeat/current/index.html" target="_blank" rel="noopener">Filebeat Reference</a></li><li><a href="http://grokdebug.herokuapp.com" target="_blank" rel="noopener">Grok Debug</a></li><li><a href="https://github.com/elastic/logstash/blob/v1.4.2/patterns/grok-patterns" target="_blank" rel="noopener">Grok Patterns</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> BigData </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Tools </tag>
            
            <tag> ELK </tag>
            
            <tag> Logstash </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Logstash (-) 简介</title>
      <link href="/2018/10/12/1-logstash-introduction/"/>
      <url>/2018/10/12/1-logstash-introduction/</url>
      
        <content type="html"><![CDATA[<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>Logstash is an open source data collection engine with real-time pipelining capabilities. Logstash can dynamically unify data from disparate sources and normalize the data into destinations of your choice. Cleanse and democratize all your data for diverse advanced downstream analytics and visualization use cases.</p><p>While Logstash originally drove innovation in log collection, its capabilities extend well beyond that use case. Any type of event can be enriched and transformed with a broad array of input, filter, and output plugins, with many native codecs further simplifying the ingestion process. Logstash accelerates your insights by harnessing a greater volume and variety of data.</p><h3 id="The-Power-of-Logstash"><a href="#The-Power-of-Logstash" class="headerlink" title="The Power of Logstash"></a>The Power of Logstash</h3><p><img src="https://www.elastic.co/guide/en/logstash/current/static/images/logstash.png" alt="The Power of Logstash"></p><h3 id="Logstash-Loves-Data"><a href="#Logstash-Loves-Data" class="headerlink" title="Logstash Loves Data"></a>Logstash Loves Data</h3><ul><li>Logs and Metrics</li><li>The Web</li><li>Data Stores and Streams</li><li>Sensors and IoT</li></ul><h3 id="How-Logstash-Works"><a href="#How-Logstash-Works" class="headerlink" title="How Logstash Works"></a>How Logstash Works</h3><ul><li>Inputs</li><li>Filters</li><li>Outputs</li><li>Codecs</li></ul><p><img src="https://www.elastic.co/guide/en/logstash/current/static/images/basic_logstash_pipeline.png" alt="Logstash Flow"></p><h2 id="Futher-More"><a href="#Futher-More" class="headerlink" title="Futher More"></a>Futher More</h2><h3 id="Supported-Logstash-Plugins"><a href="#Supported-Logstash-Plugins" class="headerlink" title="Supported Logstash Plugins"></a>Supported Logstash Plugins</h3><p>Logstash 支持的plugins: <a href="https://www.elastic.co/support/matrix#show_logstash_plugins" target="_blank" rel="noopener">Support Matrix</a></p><p>These plugins are maintained and supported by Elastic. The full list of plugins in the Logstash ecosystem can be found in the product documentation:</p><ul><li>Input plugins</li><li>Codec plugins</li><li>Filter plugins</li><li>Output plugins</li></ul><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ol><li><a href="https://www.elastic.co/products/logstash" target="_blank" rel="noopener">Logstash 官网</a></li><li><a href="https://www.elastic.co/guide/en/logstash/current/index.html" target="_blank" rel="noopener">Logstash Reference</a></li><li><a href="https://www.gitbook.com/book/chenryn/elk-stack-guide-cn/details" target="_blank" rel="noopener">ELKstack 中文指南</a></li><li><a href="http://blog.csdn.net/u010454030/article/details/49659467" target="_blank" rel="noopener">分布式日志收集之Logstash 笔记（一）</a></li><li><a href="http://tchuairen.blog.51cto.com/3848118/1840596/" target="_blank" rel="noopener">Logstash 讲解与实战应用</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> BigData </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Tools </tag>
            
            <tag> ELK </tag>
            
            <tag> Logstash </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Computer Science Books</title>
      <link href="/2018/10/07/0-cs-books/"/>
      <url>/2018/10/07/0-cs-books/</url>
      
        <content type="html"><![CDATA[<h2 id="Books"><a href="#Books" class="headerlink" title="Books"></a>Books</h2><ol><li><p>Introduction to Algorithms(3rd Edition) 算法导论(第三版)</p><pre><code> 作者: Thomas H.Cormen / Charles E.Leiserson / Ronald L.Rivest / Clifford Stein 出版社: 机械工业出版社 原作名: Introduction to Algorithms, Third Edition 译者: 殷建平 / 徐云 / 王刚 / 刘晓光 / 苏明 / 邹恒明 / 王宏志 出版年: 2012-12 页数: 780 定价: 128.00元 装帧: 平装 丛书: 计算机科学丛书 ISBN: 9787111407010</code></pre></li><li><p>Computer Systems: A Programmer’s Perspective 深入理解计算机系统</p><pre><code> 作者: Randal E.Bryant / David O&#39;Hallaron 出版社: 机械工业出版社 原作名: Computer Systems: A Programmer&#39;s Perspective (3rd Edition) 译者: 龚奕利 / 贺莲 出版年: 2016-11 页数: 737 定价: 139.00元 装帧: 平装 丛书: 计算机科学丛书 ISBN: 9787111544937</code></pre></li><li><p>The Art of Computer Programming 计算机程序设计艺术</p><p> 第一卷：基本算法</p><pre><code> 作者: [美] 唐纳德·E. 克努特 出版社: 国防工业出版社 副标题: 基本算法 译者: 苏运霖 出版年: 2002-9 页数: 626 定价: 98.00元 装帧: 精装16开 丛书: 计算机程序设计艺术（中文版） ISBN: 9787118027990</code></pre><p> 第二卷：半数值算法</p><p> 第三卷：排序与查找</p><p> 第四卷：组合算法</p></li></ol><pre><code>Completed    Volume 1 – Fundamental Algorithms    Volume 2 – Seminumerical Algorithms    Volume 3 – Sorting and Searching    Volume 4A – Combinatorial Algorithms, Part 1Planned    Volume 4B, 4C, 4D – Combinatorial Algorithms    Volume 5 – Syntactic Algorithms    Volume 6 – The Theory of Context-free Languages[7]    Volume 7 – Compiler Techniques</code></pre><p><a href="https://en.wikipedia.org/wiki/The_Art_of_Computer_Programming" target="_blank" rel="noopener">TAOCP wikipedia</a></p><ol start="4"><li><p>Structure and Interpretation of Computer Programs 计算机程序的构造和解释</p><pre><code> 作者: Harold Abelson / Gerald Jay Sussman / Julie Sussman 出版社: 机械工业出版社 副标题: 原书第2版 原作名: Structure and Interpretation of Computer Programs 译者: 裘宗燕 出版年: 2004-2 页数: 473 定价: 45.00元 装帧: 平装 丛书: 计算机科学丛书 ISBN: 9787111135104</code></pre></li><li><p>TCP/IP详解</p><pre><code> 作者:  [美] W·Richard Stevens 出版社: 机械工业出版社 原作名: TCP/IP ILLustrated Volume 1: The Protocols 译者: 范建华 出版年: 2000-4-1 页数: 423 定价: 45.00元 装帧: 平装 丛书: TCP/IP详解（中文版） ISBN: 9787111075660</code></pre></li><li><p>计算机网络（第6版）</p><pre><code> 作者: [美] James F.Kurose / [美] Keith W.Ross 出版社: 机械工业出版社 副标题: 自顶向下方法 原作名: Computer Networking：A Top-Down Approach，Sixth Edition 译者: 陈鸣 出版年: 2014-10 页数: 548 定价: 79.00元 装帧: 平装 丛书: 计算机科学丛书 ISBN: 9787111453789</code></pre></li><li><p>计算机体系结构</p><pre><code> 作者: [美] John L. Hennessy / [美] David A. Patterson 出版社: 人民邮电出版社 副标题: 量化研究方法（第5版） 原作名: Computer Architecture:A Quantitative Approach,Fifth Edition 译者: 贾洪峰 出版年: 2012-12 页数: 595 定价: 109.00元 装帧: 平装 丛书: 图灵计算机科学丛书 ISBN: 9787115297655</code></pre></li><li><p>现代操作系统（第3版）</p><pre><code> 作者:  [美] Andrew S·Tanenbaum 出版社: 机械工业出版社 原作名: Modern Operating Systems 译者: 陈向群 / 马洪兵 出版年: 2009-7 页数: 582 定价: 75.00元 装帧: 平装 丛书: 计算机科学丛书 ISBN: 9787111255444</code></pre></li><li><p>数据库系统概念：第五版</p><pre><code> 作者: Abraham Silberschatz / Henry F. Korth / S. Sudarshan 出版社: 机械工业 副标题: 第五版 原作名: Datebase System Concepts(Fifth Edition) 译者: 杨冬青 / 马秀莉 / 唐世渭 出版年: 2006-10-01 页数: 775 定价: 69.50元 装帧: 平装 丛书: 计算机科学丛书 ISBN: 9787111196877</code></pre></li><li><p>设计模式</p><pre><code>作者: [美] Erich Gamma / Richard Helm / Ralph Johnson / John Vlissides出版社: 机械工业出版社副标题: 可复用面向对象软件的基础原作名: Design Patterns: Elements of Reusable Object-Oriented Software译者: 李英军 / 马晓星 / 蔡敏 / 刘建中 等出版年: 2000-9页数: 254定价: 35.00元装帧: 平装丛书: 计算机科学丛书ISBN: 9787111075752</code></pre></li><li><p>编译原理</p><pre><code>作者: Alfred V. Aho / Monica S.Lam / Ravi Sethi / Jeffrey D. Ullman出版社: 机械工业出版社副标题: 原理、技术与工具译者: 赵建华 / 郑滔 / 戴新宇出版年: 2008年12月页数: 631定价: 89.00元装帧: 平装丛书: 计算机科学丛书ISBN: 9787111251217</code></pre></li><li><p>C程序设计语言</p><pre><code>作者: （美）Brian W. Kernighan / （美）Dennis M. Ritchie出版社: 机械工业出版社副标题: 第 2 版·新版原作名: The C Programming Language译者: 徐宝文 / 李志译 / 尤晋元审校出版年: 2004-1页数: 258定价: 30.00元装帧: 平装丛书: 计算机科学丛书ISBN: 9787111128069</code></pre></li></ol><ol start="13"><li><p>C++ Primer</p><pre><code>作者: Stanley B.Lippman / Josée LaJoie / Barbara E.Moo出版社: 人民邮电出版社原作名: C++ Primer, 4th Edition译者: 李师贤 / 蒋爱军 / 梅晓勇 / 林瑛出版年: 2006页数: 745定价: 99.00元装帧: 16开丛书: 图灵程序设计丛书ISBN: 9787115145543</code></pre></li><li><p>Effective C++</p><pre><code>作者:  [美] Scott Meyers出版社: 电子工业出版社副标题: 改善程序与设计的55个具体做法原作名: Effective C++: 55 Specific Ways to Improve Your Programs and Designs译者:  侯捷出版年: 2006-7页数: 297定价: 58.00元装帧: 简裝本ISBN: 9787121029097</code></pre></li><li><p>Effective STL</p><pre><code>作者:  [美] Scott Meyers出版社: 清华大学出版社副标题: 50条有效使用STL的经验译者: 潘爱民 / 陈铭 / 邹开红出版年: 2006-1页数: 208定价: 30.00元装帧: 平装ISBN: 9787302126959</code></pre></li></ol><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><ol><li><a href="https://www.cnblogs.com/broglie/p/5554345.html" target="_blank" rel="noopener">学习计算机需要看哪些经典书？</a></li><li><a href="https://www.zhihu.com/question/273973062/answer/372714422" target="_blank" rel="noopener">计算机必读经典书籍</a></li><li><a href="https://zhuanlan.zhihu.com/p/25983004" target="_blank" rel="noopener">计算机经典书籍汇总</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> Book </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Notes </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
